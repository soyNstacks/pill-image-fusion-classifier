{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoUcudOdrAdp",
        "outputId": "f17f864b-1d48-4c1b-bc2c-3dea8a4a1e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m174.1/176.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install library for hyperparameter optimisation\n",
        "!pip install keras-tuner -q\n",
        "\n",
        "# Import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import itertools\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import multiprocessing\n",
        "import os, shutil, sys\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# packages for processing images\n",
        "# from PIL import Image\n",
        "import cv2\n",
        "import PIL\n",
        "from skimage import exposure\n",
        "from skimage.filters import threshold_otsu\n",
        "from sklearn.cluster import KMeans\n",
        "from skimage.exposure import is_low_contrast\n",
        "import skimage.filters as filters\n",
        "from skimage.feature import local_binary_pattern\n",
        "from scipy.stats import mode, norm\n",
        "\n",
        "from progressbar import ProgressBar, Bar, SimpleProgress\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from IPython.display import Image, clear_output\n",
        "from scipy import ndimage\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.feature import hog\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_score, GridSearchCV, validation_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import (confusion_matrix, r2_score, recall_score,\n",
        "                             precision_score, f1_score, accuracy_score)\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, validation_curve\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "\n",
        "# tensorflow packages and modules\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import (Dense, Conv2D, MaxPooling2D, BatchNormalization, Lambda,\n",
        "                                     Concatenate, Activation, Dropout, Flatten, Rescaling,\n",
        "                                     GlobalAveragePooling2D, GlobalMaxPooling2D)\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner\n",
        "from tensorflow.keras.applications import VGG16, InceptionV3, MobileNetV2\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "from tensorflow.keras.utils import (to_categorical, plot_model,\n",
        "                                    array_to_img, img_to_array, load_img)\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, apply_affine_transform\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau,\n",
        "                                        LearningRateScheduler, CSVLogger, ModelCheckpoint,\n",
        "                                        TensorBoard)\n",
        "from tensorboard import notebook\n",
        "\n",
        "# Import library for image augmentation\n",
        "import imgaug\n",
        "from imgaug import augmenters as iaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5xKle4-mJTE"
      },
      "outputs": [],
      "source": [
        "# Save requirements as text file for reproducibility\n",
        "# !pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88zlXOY1M83U",
        "outputId": "be8bdccc-bf41-417d-a847-bf30739e0051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n"
          ]
        }
      ],
      "source": [
        "# Find out TensorFlow version you are working on\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Seed"
      ],
      "metadata": {
        "id": "g3DVrQ_NqkE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility in different runtimes\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "XTzX6_tKqeqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Import Google Drive Directory if running this notebook on Google Colaboratory"
      ],
      "metadata": {
        "id": "pm2ck_dznMqk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvEMoL5yNBVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461fa881-cca3-4fb9-f4ce-de29a9845434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IFYiGnWMFw-",
        "outputId": "e372164f-9809-4bc1-ea4e-dfe6aa190603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/uploaded_datasets\n"
          ]
        }
      ],
      "source": [
        "# Replace with your designated working directory\n",
        "%cd /content/drive/MyDrive/uploaded_datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj2NcJuCG3n0"
      },
      "outputs": [],
      "source": [
        "def create_folder(base_dir, folder_name):\n",
        "  \"\"\"\n",
        "  Creates a folder in designated directory and returns relative path\n",
        "  \"\"\"\n",
        "  new_path = os.path.join(base_dir, folder_name)\n",
        "  try:\n",
        "    os.makedirs(new_path)\n",
        "  except FileExistsError:\n",
        "    print('Folder exists')\n",
        "\n",
        "  return new_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CEZ9dwarkNQ"
      },
      "outputs": [],
      "source": [
        "# Root directory containing project folders and files\n",
        "DATASET_DIR = os.path.join(os.getcwd(), \"pill-classification\")\n",
        "# Dataset Directory (original images)\n",
        "IMAGE_DIR = os.path.join(DATASET_DIR, \"image_dataset\")\n",
        "\n",
        "# Dataset Directory (.npz)\n",
        "NPZ_DIR = os.path.join(DATASET_DIR, \"npz\")\n",
        "# Directories of preprocessed dataset\n",
        "COLOUR_DIR = os.path.join(DATASET_DIR, \"colour_dataset\")\n",
        "TEXTURE_DIR = os.path.join(DATASET_DIR, \"texture_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfJ2426RW_8S",
        "outputId": "b61da4ab-a23e-4cb1-c1ad-a85192502828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder exists\n",
            "Folder exists\n",
            "Folder exists\n"
          ]
        }
      ],
      "source": [
        "# Root directory to store model related outcomes\n",
        "ROOT_RESULT_DIR = create_folder(DATASET_DIR, 'all-results')\n",
        "\n",
        "# Directories to store baseline results\n",
        "BASELINE_CNN_RESULT_DIR = create_folder(ROOT_RESULT_DIR, 'baseline-results-cnn')\n",
        "BASELINE_CNN_TB_DIR = create_folder(ROOT_RESULT_DIR, 'baseline-tb-cnn')\n",
        "\n",
        "# Directories to store single-input pre-trained results\n",
        "# PP_SINGLE_RESULT_DIR = create_folder(ROOT_RESULT_DIR, 'single-results-pp')\n",
        "# PP_SINGLE_TB_DIR = create_folder(ROOT_RESULT_DIR, 'single-tb-pp')\n",
        "\n",
        "# Directories to store multi-input pre-trained results\n",
        "PP_MULTI_RESULT_DIR = create_folder(ROOT_RESULT_DIR, 'multi-results-pp')\n",
        "PP_MULTI_TB_DIR = create_folder(ROOT_RESULT_DIR, 'multi-tb-pp')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A full dataset including preprocessed images and texture arrays were saved to be reused in this notebook due frequent running out of RAM and compute units.\n",
        "\n",
        "Hence, you will see instances of loading train, validation, and test sets using .csv and .npz files. As a result, the splitting you observe here does not hold true.\n",
        "\n",
        "You may download and refer to the following files if you wish to run this notebook containing iterative experimental phase:\n",
        "- 'train_directory.csv': for train and validation sets\n",
        "- 'test_directory.csv': for test set"
      ],
      "metadata": {
        "id": "HX6vxER-5pe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "8zKJ66yKnpGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XB2fPBrzO8z",
        "outputId": "a2d7804b-693f-4187-f581-65afb204c71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5449 entries, 0 to 5448\n",
            "Data columns (total 11 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   ndc9           5449 non-null   object\n",
            " 1   filename       5449 non-null   object\n",
            " 2   is_ref         5449 non-null   bool  \n",
            " 3   is_front       5449 non-null   bool  \n",
            " 4   ndc            5449 non-null   object\n",
            " 5   splshape       5449 non-null   object\n",
            " 6   splshape_text  5449 non-null   object\n",
            " 7   splimprint     5442 non-null   object\n",
            " 8   splcolor_text  5449 non-null   object\n",
            " 9   rxstring       4881 non-null   object\n",
            " 10  filepath       5449 non-null   object\n",
            "dtypes: bool(2), object(9)\n",
            "memory usage: 436.3+ KB\n"
          ]
        }
      ],
      "source": [
        "# Read directory of filepaths and relevant groundtruths from derived Pillbox CSV file\n",
        "directory_df = pd.read_csv(os.path.join(DATASET_DIR, 'original_directory.csv'),\n",
        "                           index_col=0,\n",
        "                           dtype={\n",
        "                              'ndc': str,\n",
        "                              'ndc9': str\n",
        "                              })\n",
        "directory_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sample Distribution"
      ],
      "metadata": {
        "id": "ngoZGhxFn4oY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y--W2i56zWPE"
      },
      "outputs": [],
      "source": [
        "# Get class distribution values\n",
        "class_distrib = directory_df.ndc9.value_counts().to_dict()\n",
        "class_names = class_distrib.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wqmBfdwL9XJC",
        "outputId": "5451b98e-a21e-47f0-beec-b08571153293"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"4968b251-7caa-4fb2-b231-81294f56490a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4968b251-7caa-4fb2-b231-81294f56490a\")) {                    Plotly.newPlot(                        \"4968b251-7caa-4fb2-b231-81294f56490a\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"ndc9=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"000711015\",\"000711015\",\"007815184\",\"007815184\",\"001431425\",\"001431425\",\"003783495\",\"003783495\",\"005550588\",\"005550588\",\"000937206\",\"000937206\",\"000934232\",\"000934232\",\"000932268\",\"000932268\",\"003782076\",\"003782076\",\"000937540\",\"000937540\",\"555130073\",\"555130073\",\"003787002\",\"003787002\",\"003781800\",\"003781800\",\"000935712\",\"000935712\",\"002282717\",\"002282717\",\"162520590\",\"162520590\",\"647640805\",\"647640805\",\"001724358\",\"001724358\",\"000937248\",\"000937248\",\"000780359\",\"000780359\",\"416160636\",\"416160636\",\"000935289\",\"000935289\",\"000931077\",\"000931077\",\"000938035\",\"000938035\",\"005550485\",\"005550485\",\"000937437\",\"000937437\",\"000930537\",\"000930537\",\"534890510\",\"534890510\",\"003781807\",\"003781807\",\"649800112\",\"649800112\",\"000937180\",\"000937180\",\"684620163\",\"684620163\",\"534890531\",\"534890531\",\"658620053\",\"658620053\",\"684620360\",\"684620360\",\"000930311\",\"000930311\",\"002282127\",\"002282127\",\"000932932\",\"000932932\",\"000544728\",\"000544728\",\"005550980\",\"005550980\",\"630100027\",\"630100027\",\"000931026\",\"000931026\",\"000930132\",\"000930132\",\"000933160\",\"000933160\",\"000935119\",\"000935119\",\"000937335\",\"000937335\",\"000934359\",\"000934359\",\"398220205\",\"398220205\",\"634590101\",\"634590101\",\"000937327\",\"000937327\",\"000074642\",\"000074642\",\"000040800\",\"000040800\",\"000937485\",\"000937485\",\"005550252\",\"005550252\",\"000934233\",\"000934233\",\"003783857\",\"003783857\",\"000932055\",\"000932055\",\"005550138\",\"005550138\",\"000937157\",\"000937157\",\"000933123\",\"000933123\",\"001730135\",\"001730135\",\"000932275\",\"000932275\",\"000937114\",\"000937114\",\"001723757\",\"001723757\",\"001723757\",\"001723757\",\"684620361\",\"684620361\",\"001730755\",\"001730755\",\"000937305\",\"000937305\",\"473350580\",\"473350580\",\"000930148\",\"000930148\",\"000930812\",\"000930812\",\"000780358\",\"000780358\",\"501110395\",\"501110395\",\"525440913\",\"525440913\",\"001722662\",\"001722662\",\"000757700\",\"000757700\",\"646790936\",\"646790936\",\"655800302\",\"655800302\",\"000937224\",\"000937224\",\"651620212\",\"651620212\",\"007815186\",\"007815186\",\"000935117\",\"000935117\",\"005550859\",\"005550859\",\"000935353\",\"000935353\",\"000937285\",\"000937285\",\"005915052\",\"005915052\",\"000931087\",\"000931087\",\"000932130\",\"000932130\",\"000372250\",\"000372250\",\"000743290\",\"000743290\",\"003780722\",\"003780722\",\"005550572\",\"005550572\",\"003781815\",\"003781815\",\"680120104\",\"680120104\",\"000935507\",\"000935507\",\"000934234\",\"000934234\",\"000937254\",\"000937254\",\"534890400\",\"534890400\",\"000937181\",\"000937181\",\"597625021\",\"597625021\",\"004563210\",\"004563210\",\"000937336\",\"000937336\",\"633040693\",\"633040693\",\"003786231\",\"003786231\",\"000935501\",\"000935501\",\"658620064\",\"658620064\",\"591480006\",\"591480006\",\"000560188\",\"000560188\",\"005551056\",\"005551056\",\"000930926\",\"000930926\",\"000930924\",\"000930924\",\"662130421\",\"662130421\",\"000935062\",\"000935062\",\"000935208\",\"000935208\",\"498840922\",\"498840922\",\"000931177\",\"000931177\",\"001724960\",\"001724960\",\"005915442\",\"005915442\",\"000747068\",\"000747068\",\"992070461\",\"992070461\",\"000931893\",\"000931893\",\"161100075\",\"161100075\",\"004561550\",\"004561550\",\"000934336\",\"000934336\",\"525440152\",\"525440152\",\"000930900\",\"000930900\",\"512480151\",\"512480151\",\"007815189\",\"007815189\",\"634590404\",\"634590404\",\"000933193\",\"000933193\",\"003783632\",\"003783632\",\"000060749\",\"000060749\",\"001850117\",\"001850117\",\"634590402\",\"634590402\",\"005550873\",\"005550873\",\"000935851\",\"000935851\",\"001727310\",\"001727310\",\"000930670\",\"000930670\",\"101440604\",\"101440604\",\"000931006\",\"000931006\",\"005550926\",\"005550926\",\"000930318\",\"000930318\",\"000930778\",\"000930778\",\"005910406\",\"005910406\",\"551110198\",\"551110198\",\"551110268\",\"551110268\",\"005912473\",\"005912473\",\"000937168\",\"000937168\",\"005550997\",\"005550997\",\"685460142\",\"685460142\",\"000937326\",\"000937326\",\"000933125\",\"000933125\",\"597624960\",\"597624960\",\"000930752\",\"000930752\",\"000937270\",\"000937270\",\"003781150\",\"003781150\",\"006035771\",\"006035771\",\"000935710\",\"000935710\",\"512850083\",\"512850083\",\"512850083\",\"512850083\",\"003782077\",\"003782077\",\"501110325\",\"501110325\",\"540920189\",\"540920189\",\"000937364\",\"000937364\",\"000932061\",\"000932061\",\"003782073\",\"003782073\",\"005551022\",\"005551022\",\"000931174\",\"000931174\",\"000937314\",\"000937314\",\"000937383\",\"000937383\",\"000743020\",\"000743020\",\"000710155\",\"000710155\",\"501110482\",\"501110482\",\"000934740\",\"000934740\",\"683820022\",\"683820022\",\"000937244\",\"000937244\",\"001730758\",\"001730758\",\"005550925\",\"005550925\",\"005550925\",\"005550925\",\"000937338\",\"000937338\",\"001730759\",\"001730759\",\"000935271\",\"000935271\",\"002282803\",\"002282803\",\"005550513\",\"005550513\",\"647640918\",\"647640918\",\"001725729\",\"001725729\",\"005550139\",\"005550139\",\"000935172\",\"000935172\",\"000930221\",\"000930221\",\"001724097\",\"001724097\",\"001724097\",\"001724097\",\"597625011\",\"597625011\",\"000074892\",\"000074892\",\"000930784\",\"000930784\",\"000932158\",\"000932158\",\"501110468\",\"501110468\",\"501110468\",\"501110468\",\"007815185\",\"007815185\",\"433860660\",\"433860660\",\"003781165\",\"003781165\",\"000937220\",\"000937220\",\"101440606\",\"101440606\",\"000937443\",\"000937443\",\"000930813\",\"000930813\",\"000930813\",\"000930813\",\"501110483\",\"501110483\",\"655800303\",\"655800303\",\"003786440\",\"003786440\",\"003782675\",\"003782675\",\"001730561\",\"001730561\",\"551110344\",\"551110344\",\"001850149\",\"001850149\",\"001726359\",\"001726359\",\"540920517\",\"540920517\",\"501110333\",\"501110333\",\"000711017\",\"000711017\",\"000937795\",\"000937795\",\"005550285\",\"005550285\",\"000692600\",\"000692600\",\"684620358\",\"684620358\",\"683820024\",\"683820024\",\"000937208\",\"000937208\",\"003782650\",\"003782650\",\"000934067\",\"000934067\",\"634590215\",\"634590215\",\"167290042\",\"167290042\",\"000930174\",\"000930174\",\"005550066\",\"005550066\",\"001724626\",\"001724626\",\"003783475\",\"003783475\",\"000937223\",\"000937223\",\"005551020\",\"005551020\",\"005550590\",\"005550590\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931041\",\"000931041\",\"634590412\",\"634590412\",\"001724366\",\"001724366\",\"000930314\",\"000930314\",\"000023250\",\"000023250\",\"005915307\",\"005915307\",\"677670133\",\"677670133\",\"138110583\",\"138110583\",\"001439919\",\"001439919\",\"000937207\",\"000937207\",\"000932929\",\"000932929\",\"003782074\",\"003782074\",\"003787003\",\"003787003\",\"000930090\",\"000930090\",\"597625022\",\"597625022\",\"005550634\",\"005550634\",\"003781803\",\"003781803\",\"003780218\",\"003780218\",\"498840401\",\"498840401\",\"001722908\",\"001722908\",\"005550484\",\"005550484\",\"005550832\",\"005550832\",\"005550832\",\"005550832\",\"658620537\",\"658620537\",\"681800481\",\"681800481\",\"005550589\",\"005550589\",\"001431227\",\"001431227\",\"000935142\",\"000935142\",\"003780505\",\"003780505\",\"000930782\",\"000930782\",\"000937381\",\"000937381\",\"684620160\",\"684620160\",\"000560170\",\"000560170\",\"003100282\",\"003100282\",\"512850697\",\"512850697\",\"512850406\",\"512850406\",\"000935711\",\"000935711\",\"000930320\",\"000930320\",\"000930928\",\"000930928\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"512850523\",\"512850523\",\"001730565\",\"001730565\",\"005970044\",\"005970044\",\"000937438\",\"000937438\",\"000934029\",\"000934029\",\"665820313\",\"665820313\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"000937241\",\"000937241\",\"003781821\",\"003781821\",\"000931016\",\"000931016\",\"000937242\",\"000937242\",\"501110471\",\"501110471\",\"001727312\",\"001727312\",\"001724365\",\"001724365\",\"002450041\",\"002450041\",\"681800501\",\"681800501\",\"003782537\",\"003782537\",\"516724033\",\"516724033\",\"000935050\",\"000935050\",\"000932210\",\"000932210\",\"000937214\",\"000937214\",\"000560176\",\"000560176\",\"005550835\",\"005550835\",\"005550835\",\"005550835\",\"005271413\",\"005271413\",\"655800304\",\"655800304\",\"633040829\",\"633040829\",\"000060112\",\"000060112\",\"000937304\",\"000937304\",\"003780183\",\"003780183\",\"512850082\",\"512850082\",\"001724286\",\"001724286\",\"000023239\",\"000023239\",\"000937238\",\"000937238\",\"000710156\",\"000710156\",\"512850079\",\"512850079\",\"000937366\",\"000937366\",\"501110433\",\"501110433\",\"001860520\",\"001860520\",\"005550071\",\"005550071\",\"000935247\",\"000935247\",\"501110459\",\"501110459\",\"501110852\",\"501110852\",\"501110852\",\"501110852\",\"000939107\",\"000939107\",\"005550140\",\"005550140\",\"003780184\",\"003780184\",\"001431257\",\"001431257\",\"006035438\",\"006035438\",\"007811966\",\"007811966\",\"597622003\",\"597622003\",\"005550715\",\"005550715\",\"003784884\",\"003784884\",\"000937350\",\"000937350\",\"516724042\",\"516724042\",\"000937294\",\"000937294\",\"003781053\",\"003781053\",\"000935140\",\"000935140\",\"001722907\",\"001722907\",\"681800480\",\"681800480\",\"007811452\",\"007811452\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"501110648\",\"501110648\",\"001431241\",\"001431241\",\"000710419\",\"000710419\",\"006033741\",\"006033741\",\"007811487\",\"007811487\",\"000937464\",\"000937464\",\"501110916\",\"501110916\",\"008320038\",\"008320038\",\"000937116\",\"000937116\",\"605052671\",\"605052671\",\"005550861\",\"005550861\",\"006033079\",\"006033079\",\"000934444\",\"000934444\",\"684530375\",\"684530375\",\"005915554\",\"005915554\",\"000933109\",\"000933109\",\"000930220\",\"000930220\",\"000540109\",\"000540109\",\"620370577\",\"620370577\",\"003780152\",\"003780152\",\"000935703\",\"000935703\",\"000937287\",\"000937287\",\"000931048\",\"000931048\",\"000930026\",\"000930026\",\"000935105\",\"000935105\",\"000930658\",\"000930658\",\"492300640\",\"492300640\",\"000935769\",\"000935769\",\"000937465\",\"000937465\",\"001725033\",\"001725033\",\"003782180\",\"003782180\",\"005550286\",\"005550286\",\"000930321\",\"000930321\",\"658620142\",\"658620142\",\"005550874\",\"005550874\",\"000937247\",\"000937247\",\"005271311\",\"005271311\",\"005550882\",\"005550882\",\"000934069\",\"000934069\",\"000931003\",\"000931003\",\"000937772\",\"000937772\",\"000937426\",\"000937426\",\"000932046\",\"000932046\",\"000934742\",\"000934742\",\"000933196\",\"000933196\",\"627560712\",\"627560712\",\"000931062\",\"000931062\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"504580925\",\"504580925\",\"512850690\",\"512850690\",\"001730949\",\"001730949\",\"007811506\",\"007811506\",\"000937154\",\"000937154\",\"005550607\",\"005550607\",\"501110787\",\"501110787\",\"000024464\",\"000024464\",\"001725032\",\"001725032\",\"001724357\",\"001724357\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000931044\",\"000931044\",\"005550633\",\"005550633\",\"534890648\",\"534890648\",\"000544182\",\"000544182\",\"658620012\",\"658620012\",\"003781819\",\"003781819\",\"512850595\",\"512850595\",\"005910302\",\"005910302\",\"000937202\",\"000937202\",\"001725410\",\"001725410\",\"597625018\",\"597625018\",\"000937296\",\"000937296\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"003783856\",\"003783856\",\"003781813\",\"003781813\",\"007815191\",\"007815191\",\"000937355\",\"000937355\",\"003783205\",\"003783205\",\"000933011\",\"000933011\",\"633040827\",\"633040827\",\"000935850\",\"000935850\",\"007811787\",\"007811787\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000934235\",\"000934235\",\"004561420\",\"004561420\",\"003781110\",\"003781110\",\"504580580\",\"504580580\",\"000937178\",\"000937178\",\"000937259\",\"000937259\",\"605050065\",\"605050065\",\"605050065\",\"605050065\",\"000935049\",\"000935049\",\"597625024\",\"597625024\",\"005913202\",\"005913202\",\"000090055\",\"000090055\",\"000935214\",\"000935214\",\"000932274\",\"000932274\",\"003781823\",\"003781823\",\"007815187\",\"007815187\",\"000932267\",\"000932267\",\"000931015\",\"000931015\",\"000931060\",\"000931060\",\"000931060\",\"000931060\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"000932272\",\"000932272\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001865040\",\"001865040\",\"000932263\",\"000932263\",\"000931172\",\"000931172\",\"001861090\",\"001861090\",\"000934741\",\"000934741\",\"001850505\",\"001850505\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"000937198\",\"000937198\",\"000935127\",\"000935127\",\"001730556\",\"001730556\",\"000935116\",\"000935116\",\"001431473\",\"001431473\",\"005559014\",\"005559014\",\"005559014\",\"005559014\",\"001723649\",\"001723649\",\"124961283\",\"124961283\",\"000930155\",\"000930155\",\"005915335\",\"005915335\",\"000780360\",\"000780360\",\"000937240\",\"000937240\",\"162520572\",\"162520572\",\"001727311\",\"001727311\",\"001725411\",\"001725411\",\"000934356\",\"000934356\",\"000932075\",\"000932075\",\"001730933\",\"001730933\",\"000024463\",\"000024463\",\"683820023\",\"683820023\",\"000937267\",\"000937267\",\"584680130\",\"584680130\",\"003783855\",\"003783855\",\"000932204\",\"000932204\",\"000932204\",\"000932204\",\"001312478\",\"001312478\",\"000937147\",\"000937147\",\"000930756\",\"000930756\",\"000930756\",\"000930756\",\"000023229\",\"000023229\",\"003780724\",\"003780724\",\"001730178\",\"001730178\",\"501110393\",\"501110393\",\"000937303\",\"000937303\",\"000930733\",\"000930733\",\"000930733\",\"000930733\",\"000933002\",\"000933002\",\"501110307\",\"501110307\",\"000746594\",\"000746594\",\"008321024\",\"008321024\",\"000937365\",\"000937365\",\"000930536\",\"000930536\",\"000937671\",\"000937671\",\"512850691\",\"512850691\",\"678770147\",\"678770147\",\"008321025\",\"008321025\",\"000930109\",\"000930109\",\"501110327\",\"501110327\",\"605050185\",\"605050185\",\"646790953\",\"646790953\",\"000935124\",\"000935124\",\"000934443\",\"000934443\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"665820312\",\"665820312\",\"000935150\",\"000935150\",\"597621301\",\"597621301\",\"000931061\",\"000931061\",\"000937466\",\"000937466\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000930753\",\"000930753\",\"000851322\",\"000851322\",\"000935118\",\"000935118\",\"003786233\",\"003786233\",\"000932203\",\"000932203\",\"516724016\",\"516724016\",\"000935215\",\"000935215\",\"290330020\",\"290330020\",\"420430190\",\"420430190\",\"005550834\",\"005550834\",\"005550834\",\"005550834\",\"000935502\",\"000935502\",\"007811507\",\"007811507\",\"000622085\",\"000622085\",\"000937368\",\"000937368\",\"007812201\",\"007812201\",\"516724032\",\"516724032\",\"501110334\",\"501110334\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"003780350\",\"003780350\",\"512850538\",\"512850538\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"633040830\",\"633040830\",\"000935213\",\"000935213\",\"000938940\",\"000938940\",\"655970701\",\"655970701\",\"002282551\",\"002282551\",\"597622004\",\"597622004\",\"000930319\",\"000930319\",\"000937425\",\"000937425\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"000934068\",\"000934068\",\"000935455\",\"000935455\",\"000931052\",\"000931052\",\"597622858\",\"597622858\",\"000930983\",\"000930983\",\"000935207\",\"000935207\",\"002282634\",\"002282634\",\"005559008\",\"005559008\",\"005559008\",\"005559008\",\"619581001\",\"619581001\",\"498840403\",\"498840403\",\"000935036\",\"000935036\",\"684620208\",\"684620208\",\"000931005\",\"000931005\",\"000930711\",\"000930711\",\"005550969\",\"005550969\",\"655970118\",\"655970118\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"002282128\",\"002282128\",\"001431240\",\"001431240\",\"000937219\",\"000937219\",\"416160759\",\"416160759\",\"000939133\",\"000939133\",\"101440602\",\"101440602\",\"005550808\",\"005550808\",\"000931078\",\"000931078\",\"000937477\",\"000937477\",\"000935290\",\"000935290\",\"000930039\",\"000930039\",\"005559025\",\"005559025\",\"167140663\",\"167140663\",\"001725312\",\"001725312\",\"005551021\",\"005551021\",\"000937239\",\"000937239\",\"000938036\",\"000938036\",\"000931025\",\"000931025\",\"000937670\",\"000937670\",\"000937167\",\"000937167\",\"005550831\",\"005550831\",\"005550831\",\"005550831\",\"000933195\",\"000933195\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"000932270\",\"000932270\",\"004560457\",\"004560457\",\"000930787\",\"000930787\",\"000937369\",\"000937369\",\"000930029\",\"000930029\",\"001724364\",\"001724364\",\"001724364\",\"001724364\",\"501110441\",\"501110441\",\"633040579\",\"633040579\",\"672530902\",\"672530902\",\"512850091\",\"512850091\",\"512850091\",\"512850091\",\"501110901\",\"501110901\",\"501110467\",\"501110467\",\"000934059\",\"000934059\",\"000540017\",\"000540017\",\"274370201\",\"274370201\",\"684530850\",\"684530850\",\"003781811\",\"003781811\",\"681800478\",\"681800478\",\"551110343\",\"551110343\",\"658620144\",\"658620144\",\"000930892\",\"000930892\",\"000934404\",\"000934404\",\"000933010\",\"000933010\",\"001850048\",\"001850048\",\"003100281\",\"003100281\",\"317220278\",\"317220278\",\"000935768\",\"000935768\",\"006032406\",\"006032406\",\"000780486\",\"000780486\",\"003783482\",\"003783482\",\"000930058\",\"000930058\",\"000930058\",\"000930058\",\"597624910\",\"597624910\",\"005551055\",\"005551055\",\"001725412\",\"001725412\",\"597625008\",\"597625008\",\"001723760\",\"001723760\",\"000930463\",\"000930463\",\"000074882\",\"000074882\",\"162520591\",\"162520591\",\"000060277\",\"000060277\",\"005550779\",\"005550779\",\"000930154\",\"000930154\",\"000937152\",\"000937152\",\"000937334\",\"000937334\",\"005559010\",\"005559010\",\"005559010\",\"005559010\",\"646790929\",\"646790929\",\"658620052\",\"658620052\",\"551110197\",\"551110197\",\"681800479\",\"681800479\",\"597625033\",\"597625033\",\"000934339\",\"000934339\",\"501110398\",\"501110398\",\"992070467\",\"992070467\",\"001431475\",\"001431475\",\"000930810\",\"000930810\",\"000930810\",\"000930810\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"000935505\",\"000935505\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"000935125\",\"000935125\",\"000930173\",\"000930173\",\"647200323\",\"647200323\",\"231550062\",\"231550062\",\"001431268\",\"001431268\",\"001850124\",\"001850124\",\"605051308\",\"605051308\",\"001730562\",\"001730562\",\"005550833\",\"005550833\",\"005551883\",\"005551883\",\"000935852\",\"000935852\",\"501110518\",\"501110518\",\"000930011\",\"000930011\",\"000034222\",\"000034222\",\"005550211\",\"005550211\",\"001690081\",\"001690081\",\"007815405\",\"007815405\",\"001861088\",\"001861088\",\"001725313\",\"001725313\",\"000937182\",\"000937182\",\"001725034\",\"001725034\",\"000239350\",\"000239350\",\"000933167\",\"000933167\",\"000933167\",\"000933167\",\"001722089\",\"001722089\",\"473350894\",\"473350894\",\"000937367\",\"000937367\",\"000934405\",\"000934405\",\"007815181\",\"007815181\",\"000930757\",\"000930757\",\"003781125\",\"003781125\",\"001850053\",\"001850053\",\"000931118\",\"000931118\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"005970046\",\"005970046\",\"551110180\",\"551110180\",\"000542526\",\"000542526\",\"501110324\",\"501110324\",\"000931122\",\"000931122\",\"005913210\",\"005913210\",\"005550059\",\"005550059\",\"005550860\",\"005550860\",\"000937385\",\"000937385\",\"000937442\",\"000937442\",\"003781105\",\"003781105\",\"000746122\",\"000746122\",\"000937536\",\"000937536\",\"001453817\",\"001453817\",\"003780501\",\"003780501\",\"000933129\",\"000933129\",\"000540166\",\"000540166\",\"006032545\",\"006032545\",\"001450091\",\"001450091\",\"001723650\",\"001723650\",\"000935420\",\"000935420\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"000930755\",\"000930755\",\"000937295\",\"000937295\",\"620370999\",\"620370999\",\"658620050\",\"658620050\",\"000930147\",\"000930147\",\"005550887\",\"005550887\",\"000932080\",\"000932080\",\"001724096\",\"001724096\",\"001724096\",\"001724096\",\"005910388\",\"005910388\",\"000930028\",\"000930028\",\"504580551\",\"504580551\",\"000935504\",\"000935504\",\"605050146\",\"605050146\",\"501110470\",\"501110470\",\"525440291\",\"525440291\",\"525440291\",\"525440291\",\"000937156\",\"000937156\",\"003780186\",\"003780186\",\"000930688\",\"000930688\",\"000937325\",\"000937325\",\"551110196\",\"551110196\",\"512850539\",\"512850539\",\"501110647\",\"501110647\",\"001722083\",\"001722083\",\"001722083\",\"001722083\",\"000937286\",\"000937286\",\"000074471\",\"000074471\",\"008321080\",\"008321080\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"005550733\",\"005550733\",\"007811486\",\"007811486\",\"501110469\",\"501110469\",\"512850692\",\"512850692\",\"003780503\",\"003780503\",\"003780208\",\"003780208\",\"000937234\",\"000937234\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"000930041\",\"000930041\",\"007812054\",\"007812054\",\"001723758\",\"001723758\",\"000023251\",\"000023251\",\"658620054\",\"658620054\",\"512850063\",\"512850063\",\"512850063\",\"512850063\",\"001725311\",\"001725311\",\"003782625\",\"003782625\",\"001724285\",\"001724285\",\"000935061\",\"000935061\",\"000937121\",\"000937121\",\"003784151\",\"003784151\",\"250100205\",\"250100205\",\"000935206\",\"000935206\",\"000023238\",\"000023238\",\"005559027\",\"005559027\",\"516724030\",\"516724030\",\"000930012\",\"000930012\",\"000023228\",\"000023228\",\"162520516\",\"162520516\",\"003780143\",\"003780143\",\"633040553\",\"633040553\",\"005559009\",\"005559009\",\"003781411\",\"003781411\",\"005550899\",\"005550899\",\"003781809\",\"003781809\",\"000937292\",\"000937292\",\"000931024\",\"000931024\",\"000780521\",\"000780521\",\"000935506\",\"000935506\",\"000935112\",\"000935112\",\"000934030\",\"000934030\",\"000934030\",\"000934030\",\"000935171\",\"000935171\",\"005550869\",\"005550869\",\"000930771\",\"000930771\",\"501110323\",\"501110323\",\"000937158\",\"000937158\",\"002282550\",\"002282550\",\"000743040\",\"000743040\",\"000937796\",\"000937796\",\"000747070\",\"000747070\",\"000935385\",\"000935385\",\"605052580\",\"605052580\",\"000935035\",\"000935035\",\"000937436\",\"000937436\",\"685460229\",\"685460229\",\"000935246\",\"000935246\",\"000935126\",\"000935126\",\"007812020\",\"007812020\",\"658620155\",\"658620155\",\"000930054\",\"000930054\",\"005559028\",\"005559028\",\"005559028\",\"005559028\",\"000930811\",\"000930811\",\"000930811\",\"000930811\",\"000745182\",\"000745182\",\"534890146\",\"534890146\",\"000933012\",\"000933012\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"534890143\",\"534890143\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"000931049\",\"000931049\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"005551057\",\"005551057\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"000930308\",\"000930308\",\"000937386\",\"000937386\",\"006035165\",\"006035165\",\"000930083\",\"000930083\",\"000937201\",\"000937201\",\"005550886\",\"005550886\",\"005550302\",\"005550302\",\"001439920\",\"001439920\",\"000935770\",\"000935770\",\"003780232\",\"003780232\",\"001730760\",\"001730760\",\"512850524\",\"512850524\",\"005550323\",\"005550323\",\"000937293\",\"000937293\",\"002282633\",\"002282633\",\"658620156\",\"658620156\",\"658620044\",\"658620044\",\"000932047\",\"000932047\",\"000930225\",\"000930225\",\"003781140\",\"003781140\",\"007815188\",\"007815188\",\"007812053\",\"007812053\",\"005550483\",\"005550483\",\"501110851\",\"501110851\",\"006033856\",\"006033856\",\"000937258\",\"000937258\",\"000937153\",\"000937153\",\"005550967\",\"005550967\",\"000930948\",\"000930948\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"001780615\",\"001780615\",\"000932931\",\"000932931\",\"000935141\",\"000935141\",\"000097663\",\"000097663\",\"689683500\",\"689683500\",\"000932060\",\"000932060\",\"000930233\",\"000930233\",\"504580579\",\"504580579\",\"597622002\",\"597622002\",\"000930657\",\"000930657\",\"006032957\",\"006032957\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"000930027\",\"000930027\",\"501110309\",\"501110309\",\"000937236\",\"000937236\",\"001861092\",\"001861092\",\"504190488\",\"504190488\",\"000937227\",\"000937227\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"001730595\",\"001730595\",\"000935256\",\"000935256\",\"000935256\",\"000935256\",\"000937169\",\"000937169\",\"512850424\",\"512850424\",\"000933127\",\"000933127\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"597625019\",\"597625019\",\"000930051\",\"000930051\",\"000930051\",\"000930051\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"000937493\",\"000937493\",\"007773105\",\"007773105\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"003785124\",\"003785124\",\"537460190\",\"537460190\",\"000930017\",\"000930017\",\"000937243\",\"000937243\",\"003780216\",\"003780216\",\"000932264\",\"000932264\",\"000934338\",\"000934338\",\"000937382\",\"000937382\",\"005550635\",\"005550635\",\"000937113\",\"000937113\",\"003786232\",\"003786232\",\"000937256\",\"000937256\",\"000937256\",\"000937256\",\"003785123\",\"003785123\",\"634590416\",\"634590416\",\"000937314\",\"000937314\",\"000937314\",\"000937314\",\"000937314\",\"005551883\",\"005551883\",\"005551883\",\"003783857\",\"003783857\",\"003783857\",\"003783857\",\"003783857\",\"000937202\",\"000937202\",\"000937202\",\"000937202\",\"000937202\",\"655800302\",\"655800302\",\"655800302\",\"655800302\",\"003780143\",\"005550808\",\"605052671\",\"001450091\",\"000933010\",\"000933010\",\"501110467\",\"000023228\",\"001730135\",\"000937241\",\"000938036\",\"000938036\",\"000780358\",\"634590101\",\"634590101\",\"000932130\",\"001722089\",\"001722089\",\"003781150\",\"000930688\",\"000930688\",\"000930688\",\"000930688\",\"005550926\",\"000932061\",\"000932061\",\"000747068\",\"000747068\",\"665820312\",\"000934339\",\"002282633\",\"000935035\",\"000935035\",\"000935035\",\"000935035\",\"000937219\",\"000937219\",\"000937219\",\"000937219\",\"000937219\",\"000931177\",\"000931177\",\"000931177\",\"000931177\",\"000931177\",\"000930319\",\"000930319\",\"000930319\",\"000930319\",\"000023229\",\"000023229\",\"000934742\",\"000934742\",\"000934742\",\"000934742\",\"000934742\",\"003786232\",\"003786232\",\"003786232\",\"003787002\",\"003787002\",\"003787002\",\"003787002\",\"003787002\",\"655970701\",\"655970701\",\"655970701\",\"655970701\",\"003781110\",\"003781110\",\"000930755\",\"000930755\",\"000930755\",\"005915052\",\"005915052\",\"525440152\",\"525440152\",\"000934338\",\"000934338\",\"000930810\",\"000930810\",\"001431475\",\"005550899\",\"005550899\",\"005550899\",\"001850117\",\"001850117\",\"000692600\",\"000692600\",\"000692600\",\"000930813\",\"000930813\",\"000930813\",\"000074471\",\"685460142\",\"005550980\",\"000930812\",\"000930812\",\"000931078\",\"000931078\",\"000931078\",\"000931078\",\"681800481\",\"681800481\",\"681800481\",\"161100075\",\"161100075\",\"000939107\",\"000939107\",\"000939107\",\"001453817\",\"000931044\",\"000931044\",\"000931044\",\"000931044\",\"000931044\",\"003780208\",\"003780208\",\"003780208\",\"003780208\",\"684620361\",\"684620361\",\"649800112\",\"649800112\",\"006032545\",\"006032545\",\"006032545\",\"005550286\",\"005550286\",\"005550286\",\"007811487\",\"007811487\",\"000935105\",\"000935105\",\"000935105\",\"005550302\",\"005550302\",\"000937198\",\"000937198\",\"000937198\",\"000930318\",\"001730562\",\"000933127\",\"000933127\",\"000933127\",\"000933127\",\"000933127\",\"005559025\",\"005559025\",\"005559025\",\"000930756\",\"000930756\",\"007811507\",\"007811507\",\"007811507\",\"007811507\",\"007811507\",\"001860520\",\"001860520\",\"001860520\",\"001860520\",\"001860520\",\"162520572\",\"162520572\",\"001725034\",\"001725034\",\"605051308\",\"605051308\",\"605051308\",\"605051308\",\"000937167\",\"000937167\",\"000937167\",\"000937167\",\"000937167\",\"000937178\",\"000937178\",\"000937178\",\"000937178\",\"000931041\",\"000931041\",\"000931041\",\"591480006\",\"591480006\",\"501110647\",\"501110647\",\"516724030\",\"516724030\",\"000937367\",\"000937367\",\"000937367\",\"000937367\",\"000937367\",\"000930321\",\"000930321\",\"000930321\",\"000930321\",\"000930321\",\"005550588\",\"005550588\",\"005550588\",\"005550588\",\"001727310\",\"001727310\",\"001727310\",\"001727310\",\"001727310\",\"000937116\",\"000937116\",\"000937116\",\"000937116\",\"992070461\",\"992070461\",\"504580925\",\"504580925\",\"504580925\",\"504580925\",\"597622858\",\"597622858\",\"597622858\",\"597622858\",\"597622858\",\"005550323\",\"005550323\",\"005550323\",\"005550323\",\"005550323\",\"005559014\",\"005559014\",\"005559014\",\"005559014\",\"005559014\",\"005559014\",\"000930983\",\"000930983\",\"000930983\",\"000930983\",\"000937383\",\"000937383\",\"000937383\",\"000937383\",\"005550589\",\"005550589\",\"005550589\",\"005550589\",\"001730595\",\"001730595\",\"001730595\",\"681800478\",\"681800478\",\"000023250\",\"000023250\",\"001850053\",\"001850053\",\"001730759\",\"001730759\",\"001730759\",\"001730759\",\"001730759\",\"000933012\",\"000933012\",\"005551056\",\"005551056\",\"005551056\",\"005551056\",\"005551056\",\"000935505\",\"000935505\",\"000935505\",\"000935505\",\"000935505\",\"000780486\",\"000780486\",\"000780486\",\"689683500\",\"689683500\",\"689683500\",\"689683500\",\"689683500\",\"005915442\",\"647640805\",\"647640805\",\"647640805\",\"647640805\",\"647640805\",\"005550833\",\"005550833\",\"005550833\",\"005550733\",\"005550733\",\"005550733\",\"005550733\",\"005550733\",\"005913202\",\"005913202\",\"005913202\",\"003780724\",\"003780724\",\"003780724\",\"003783475\",\"003783475\",\"003783475\",\"003783475\",\"003783475\",\"597624960\",\"597624960\",\"005550779\",\"005550779\",\"005550779\",\"005550779\",\"005550779\",\"000932158\",\"000932158\",\"000932158\",\"647640918\",\"647640918\",\"647640918\",\"647640918\",\"000560170\",\"000560170\",\"004561420\",\"004561420\",\"000711017\",\"000935271\",\"000935271\",\"000935271\",\"000935271\",\"005550066\",\"005550066\",\"005550066\",\"005550066\",\"005550874\",\"005550874\",\"005550874\",\"005550874\",\"005550874\",\"658620142\",\"658620142\",\"000023238\",\"000023238\",\"000560176\",\"000931174\",\"000931174\",\"000931174\",\"000933193\",\"000933193\",\"000933193\",\"000933193\",\"003784151\",\"003784151\",\"000937113\",\"000937113\",\"000937113\",\"000937113\",\"000937113\",\"000937796\",\"000743040\",\"000743040\",\"005551020\",\"005551020\",\"005551020\",\"005551020\",\"005551020\",\"501110323\",\"501110323\",\"501110323\",\"501110323\",\"000710419\",\"000710419\",\"000710419\",\"000710419\",\"000710419\",\"007811486\",\"007811486\",\"684620358\",\"684620358\",\"684620358\",\"000937293\",\"000937293\",\"000937293\",\"000937293\",\"000937293\",\"003780184\",\"003780184\",\"003780184\",\"000937671\",\"000937671\",\"000937671\",\"000937671\",\"000937671\",\"000937220\",\"000937220\",\"000937220\",\"000937220\",\"000937220\",\"003787003\",\"003787003\",\"003787003\",\"003787003\",\"000933129\",\"000933129\",\"000933129\",\"000933129\",\"000935213\",\"000935213\",\"000935213\",\"000935213\",\"000935150\",\"000935150\",\"000935150\",\"000935150\",\"634590412\",\"634590412\",\"634590412\",\"634590412\",\"000931122\",\"000931122\",\"000931122\",\"000931122\",\"504580580\",\"504580580\",\"504580580\",\"504580580\",\"005550635\",\"005550635\",\"005550635\",\"005550635\",\"005550635\",\"003781411\",\"003781411\",\"003781411\",\"003781411\",\"003781411\",\"534890648\",\"534890648\",\"534890648\",\"534890648\",\"597622002\",\"597622002\",\"597622002\",\"597622002\",\"597625021\",\"597625021\",\"597625021\",\"597625021\",\"597625021\",\"005550572\",\"005550572\",\"006033856\",\"006033856\",\"006033856\",\"605050065\",\"605050065\",\"605050065\",\"605050065\",\"605050065\",\"473350580\",\"473350580\",\"473350580\",\"473350580\",\"473350580\",\"597625018\",\"597625018\",\"501110471\",\"501110471\",\"501110471\",\"501110471\",\"000937247\",\"000937247\",\"000937247\",\"000937247\",\"000937247\",\"003782650\",\"003782650\",\"003782650\",\"683820024\",\"683820024\",\"683820024\",\"683820024\",\"630100027\",\"630100027\",\"630100027\",\"630100027\",\"630100027\",\"001850124\",\"003784884\",\"003781140\",\"003781140\",\"003781140\",\"512850523\",\"512850523\",\"512850523\",\"512850523\",\"000933011\",\"000937267\",\"000937267\",\"000937267\",\"000937267\",\"000937267\",\"633040827\",\"647200323\",\"647200323\",\"647200323\",\"003782077\",\"003782077\",\"003782077\",\"003782077\",\"003782077\",\"501110395\",\"501110395\",\"501110395\",\"501110395\",\"501110395\",\"001722907\",\"001722907\",\"001722907\",\"001722907\",\"684620208\",\"684620208\",\"684620208\",\"684620208\",\"684620208\",\"000931077\",\"000931077\",\"597625019\",\"597625019\",\"597625019\",\"597625019\",\"597625019\",\"551110343\",\"551110343\",\"551110343\",\"501110469\",\"501110469\",\"501110469\",\"501110469\",\"501110469\",\"003783205\",\"003783205\",\"003783205\",\"003783205\",\"003783205\",\"005551057\",\"005551057\",\"005551057\",\"005551057\",\"006033079\",\"006033079\",\"501110333\",\"501110333\",\"501110333\",\"000935852\",\"000935852\",\"000935852\",\"000935852\",\"000935852\",\"525440913\",\"525440913\",\"525440913\",\"512850424\",\"512850424\",\"512850424\",\"512850424\",\"001723757\",\"001723757\",\"001723757\",\"001723757\",\"001723757\",\"658620053\",\"658620053\",\"658620053\",\"000090055\",\"000090055\",\"000090055\",\"655970118\",\"655970118\",\"000934059\",\"000934059\",\"000934059\",\"000934059\",\"000934059\",\"005550859\",\"005550859\",\"005550859\",\"005550859\",\"000930154\",\"000930154\",\"000930154\",\"000930154\",\"005550831\",\"005550831\",\"005550831\",\"005550831\",\"005550831\",\"501110483\",\"501110483\",\"501110483\",\"000933196\",\"000933196\",\"000933196\",\"000933196\",\"000933196\",\"001724286\",\"001724286\",\"001724286\",\"001724286\",\"001724286\",\"433860660\",\"001724357\",\"001724357\",\"001724357\",\"001724357\",\"005550140\",\"005550140\",\"005550140\",\"005550140\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"001725728\",\"000780359\",\"000780359\",\"000780359\",\"000780359\",\"167290042\",\"167290042\",\"167290042\",\"167290042\",\"167290042\",\"000937325\",\"000937325\",\"000937325\",\"000937325\",\"000937325\",\"000937670\",\"000937670\",\"000937670\",\"000937670\",\"000937670\",\"000937152\",\"000937152\",\"000937152\",\"534890143\",\"534890143\",\"534890143\",\"534890143\",\"597622003\",\"597622003\",\"597622003\",\"597622003\",\"000937443\",\"000937443\",\"000937443\",\"000937443\",\"000937443\",\"492300640\",\"492300640\",\"492300640\",\"006035438\",\"006035438\",\"006035438\",\"006035438\",\"006035438\",\"000937157\",\"000937157\",\"000937157\",\"000937157\",\"000937157\",\"000933123\",\"000933123\",\"000933123\",\"000933123\",\"000933123\",\"005550607\",\"005550607\",\"005550607\",\"005550607\",\"684620160\",\"684620160\",\"684620160\",\"684620160\",\"000937772\",\"000937772\",\"000937772\",\"000937772\",\"000937772\",\"003781821\",\"003781821\",\"003781821\",\"003781821\",\"501110393\",\"501110393\",\"501110393\",\"501110393\",\"501110393\",\"003783632\",\"003783632\",\"003783632\",\"000542526\",\"000542526\",\"000542526\",\"000542526\",\"000542526\",\"597624910\",\"597624910\",\"005910406\",\"005910406\",\"005910406\",\"005910406\",\"003781813\",\"003781813\",\"003781813\",\"003781813\",\"001431227\",\"001431227\",\"001431227\",\"001431227\",\"000932264\",\"000932264\",\"000932264\",\"003780501\",\"003780501\",\"003780501\",\"003780501\",\"003780501\",\"000930155\",\"000930155\",\"000930155\",\"000930155\",\"003781053\",\"003781053\",\"003781053\",\"003781053\",\"633040553\",\"633040553\",\"633040553\",\"633040553\",\"633040553\",\"658620144\",\"658620144\",\"658620144\",\"274370201\",\"274370201\",\"274370201\",\"274370201\",\"658620155\",\"658620155\",\"658620155\",\"658620155\",\"684620360\",\"684620360\",\"684620360\",\"684620360\",\"001724365\",\"001724365\",\"001724365\",\"001724365\",\"001724365\",\"000931016\",\"000931016\",\"000931016\",\"000931016\",\"000931016\",\"005550715\",\"005550715\",\"005550715\",\"005550715\",\"005550715\",\"651620212\",\"651620212\",\"651620212\",\"651620212\",\"005550832\",\"005550832\",\"005550832\",\"005550832\",\"005550832\",\"512850690\",\"512850690\",\"512850690\",\"512850690\",\"512850690\",\"003780722\",\"003780722\",\"003780722\",\"003780722\",\"003780722\",\"000937158\",\"000937158\",\"000937158\",\"000937158\",\"000937158\",\"003783855\",\"003783855\",\"001725729\",\"001725729\",\"001725729\",\"001725729\",\"004563210\",\"004563210\",\"646790953\",\"646790953\",\"646790953\",\"646790953\",\"646790953\",\"007815405\",\"007815405\",\"007815405\",\"162520591\",\"162520591\",\"162520591\",\"162520591\",\"000937154\",\"000937154\",\"000937154\",\"003783856\",\"003783856\",\"000934234\",\"000934234\",\"000934234\",\"000934234\",\"000934234\",\"498840401\",\"498840401\",\"498840401\",\"498840401\",\"003780152\",\"003780152\",\"003780152\",\"003780152\",\"000937536\",\"000937536\",\"002282551\",\"002282551\",\"002282551\",\"000933125\",\"000933125\",\"000933125\",\"000933125\",\"000933125\",\"000930109\",\"000930109\",\"005550634\",\"005550634\",\"005550634\",\"005550634\",\"005550634\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"512850087\",\"000935112\",\"000935112\",\"000935112\",\"000935112\",\"001723649\",\"001723649\",\"001723649\",\"001723649\",\"000935455\",\"000935455\",\"000935455\",\"000935455\",\"000935455\",\"000023239\",\"000023239\",\"000935501\",\"000935501\",\"000935501\",\"000935501\",\"000935501\",\"597625024\",\"597625024\",\"597625024\",\"597625024\",\"597625024\",\"001730760\",\"001730760\",\"001730760\",\"001730760\",\"001730760\",\"000930778\",\"000930778\",\"000930778\",\"000930778\",\"501110307\",\"501110307\",\"501110307\",\"000932929\",\"000932929\",\"000932929\",\"000932929\",\"008321080\",\"008321080\",\"001730949\",\"001730949\",\"001730949\",\"001730949\",\"001730949\",\"000540017\",\"000540017\",\"000540017\",\"005550887\",\"005550887\",\"005550887\",\"005550887\",\"000935124\",\"000935124\",\"000935124\",\"000935124\",\"000935124\",\"005550886\",\"005550886\",\"005550886\",\"504580579\",\"504580579\",\"504580579\",\"504580579\",\"658620064\",\"658620064\",\"658620064\",\"658620064\",\"658620064\",\"000934740\",\"000934740\",\"000934740\",\"000934740\",\"000934740\",\"290330020\",\"290330020\",\"290330020\",\"290330020\",\"290330020\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"001724280\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"005559032\",\"003781105\",\"003781105\",\"001439919\",\"001439919\",\"001439919\",\"001439919\",\"001439919\",\"001724364\",\"001724364\",\"001724364\",\"001724364\",\"001724364\",\"001724364\",\"001724364\",\"005550590\",\"000934404\",\"000934404\",\"000934404\",\"000934404\",\"000934404\",\"000937147\",\"000937147\",\"000937147\",\"000937147\",\"000937147\",\"000930670\",\"000930670\",\"000930670\",\"000930670\",\"000930670\",\"000023251\",\"000023251\",\"000935289\",\"000935289\",\"000935289\",\"000935289\",\"000935289\",\"000933195\",\"000933195\",\"000933195\",\"000933195\",\"003780186\",\"003100281\",\"003100281\",\"003100281\",\"007811787\",\"007811787\",\"007811787\",\"007811787\",\"512850406\",\"512850406\",\"512850406\",\"003781809\",\"003781809\",\"003781809\",\"003781809\",\"000743290\",\"000743290\",\"000743290\",\"000743290\",\"003780218\",\"003780218\",\"003780218\",\"000937121\",\"000937121\",\"000937121\",\"000937121\",\"000937121\",\"007815187\",\"007815187\",\"007815187\",\"007815187\",\"007815187\",\"000746594\",\"000746594\",\"000746594\",\"000746594\",\"005970046\",\"005970046\",\"005970046\",\"005970046\",\"005970046\",\"001850149\",\"001850149\",\"512850691\",\"512850691\",\"512850691\",\"512850691\",\"512850691\",\"003782076\",\"003782076\",\"003782076\",\"003782076\",\"000930220\",\"000930220\",\"000930220\",\"000930220\",\"000930220\",\"540920189\",\"540920189\",\"540920189\",\"540920189\",\"540920189\",\"000937477\",\"000937477\",\"000937477\",\"000937477\",\"000937477\",\"000930041\",\"000930041\",\"000930041\",\"000930041\",\"620370999\",\"620370999\",\"620370999\",\"501110398\",\"501110398\",\"501110398\",\"501110398\",\"551110344\",\"551110344\",\"551110344\",\"551110344\",\"551110344\",\"003781823\",\"003781823\",\"003781823\",\"003781823\",\"003781823\",\"000930132\",\"000930132\",\"000930132\",\"000930132\",\"003782625\",\"003782625\",\"003782625\",\"003782625\",\"003782625\",\"250100205\",\"005915554\",\"005915554\",\"534890510\",\"534890510\",\"534890510\",\"534890510\",\"534890510\",\"683820022\",\"683820022\",\"683820022\",\"683820022\",\"683820022\",\"501110482\",\"501110482\",\"501110482\",\"000937295\",\"000937295\",\"000937295\",\"000937295\",\"000930658\",\"000930658\",\"000930658\",\"000934336\",\"000934336\",\"000934336\",\"000934336\",\"000930928\",\"000930928\",\"000930928\",\"000930928\",\"000930928\",\"001722908\",\"001722908\",\"001722908\",\"001730755\",\"001730755\",\"001730755\",\"001730755\",\"001730755\",\"000930536\",\"000930536\",\"000930536\",\"000930536\",\"000930536\",\"000937238\",\"000937238\",\"000937238\",\"000937238\",\"000937244\",\"000937244\",\"000937244\",\"000937244\",\"000937244\",\"000937248\",\"000937248\",\"000937248\",\"000937248\",\"000937248\",\"000937287\",\"000937287\",\"000937287\",\"000937287\",\"000937287\",\"000930311\",\"000930311\",\"605052580\",\"605052580\",\"605052580\",\"001722662\",\"001722662\",\"001722662\",\"001722662\",\"001722662\",\"000560188\",\"000560188\",\"000560188\",\"000937464\",\"000937464\",\"000937464\",\"000937464\",\"000937464\",\"001730933\",\"001730933\",\"001730933\",\"001730933\",\"992070467\",\"992070467\",\"992070467\",\"681800479\",\"681800479\",\"681800479\",\"001850505\",\"001850505\",\"007812053\",\"007812053\",\"007812053\",\"007812053\",\"007812053\",\"000935206\",\"000935206\",\"000932055\",\"000932055\",\"000930948\",\"000930948\",\"000930948\",\"000935062\",\"000935062\",\"000935062\",\"005271311\",\"005271311\",\"005271311\",\"005271311\",\"000930148\",\"000930148\",\"000930148\",\"000930148\",\"000930148\",\"504580551\",\"504580551\",\"504580551\",\"504580551\",\"007815184\",\"007815184\",\"007815184\",\"007815184\",\"000937304\",\"000937304\",\"000937304\",\"000937304\",\"000937304\",\"633040693\",\"633040693\",\"633040693\",\"633040693\",\"633040693\",\"678770147\",\"678770147\",\"525440291\",\"525440291\",\"525440291\",\"658620012\",\"658620012\",\"658620012\",\"658620012\",\"658620012\",\"000930537\",\"000930537\",\"000930537\",\"000930537\",\"000930537\",\"501110518\",\"501110518\",\"501110518\",\"000935215\",\"000935215\",\"000935215\",\"000935215\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"001725413\",\"000931005\",\"000931005\",\"000931005\",\"000931005\",\"000931005\",\"000745182\",\"000745182\",\"000745182\",\"000745182\",\"000930926\",\"000930926\",\"000930926\",\"000930926\",\"000930926\",\"504190488\",\"504190488\",\"504190488\",\"504190488\",\"504190488\",\"000930051\",\"000930051\",\"000930051\",\"000930051\",\"000930051\",\"231550062\",\"231550062\",\"231550062\",\"231550062\",\"231550062\",\"665820313\",\"665820313\",\"005550882\",\"005550882\",\"005550882\",\"005550882\",\"005550882\",\"672530902\",\"672530902\",\"672530902\",\"005550925\",\"005550925\",\"005550925\",\"005550925\",\"005550925\",\"000540109\",\"000540109\",\"000540109\",\"008321024\",\"000935207\",\"000935207\",\"000937493\",\"000937493\",\"000937493\",\"000937493\",\"000937493\",\"000935768\",\"000935768\",\"000935768\",\"000935768\",\"516724016\",\"001730556\",\"001730556\",\"001730556\",\"001730556\",\"001730556\",\"000935061\",\"000935061\",\"000935061\",\"000935061\",\"000935061\",\"000930771\",\"000930771\",\"000930771\",\"000930771\",\"000930771\",\"000935247\",\"000935247\",\"000935247\",\"001725032\",\"001725032\",\"001725032\",\"000933002\",\"000933002\",\"000933002\",\"000933002\",\"000933002\",\"005550485\",\"005550485\",\"005550485\",\"005550485\",\"005550485\",\"534890531\",\"534890531\",\"534890531\",\"534890531\",\"000930090\",\"000930090\",\"000930090\",\"000930090\",\"684530375\",\"684530375\",\"684530375\",\"684530375\",\"684530375\",\"000372250\",\"000372250\",\"000372250\",\"000372250\",\"000372250\",\"005550860\",\"005550860\",\"005550860\",\"005550860\",\"005550860\",\"003782180\",\"003782180\",\"003782180\",\"003782180\",\"003782180\",\"000931172\",\"000931172\",\"000931172\",\"000931172\",\"000931172\",\"620370577\",\"620370577\",\"620370577\",\"620370577\",\"620370577\",\"000935507\",\"000935507\",\"000935507\",\"634590402\",\"634590402\",\"634590402\",\"634590402\",\"634590402\",\"000934356\",\"000934356\",\"000934356\",\"001312478\",\"001312478\",\"001312478\",\"001312478\",\"000930753\",\"000930753\",\"000930753\",\"000930753\",\"000930753\",\"001725410\",\"001725410\",\"003781800\",\"003781800\",\"003781800\",\"003781800\",\"000937336\",\"000937336\",\"000937336\",\"000937336\",\"000937336\",\"007815188\",\"007815188\",\"000931087\",\"000931087\",\"000931087\",\"000931087\",\"000931087\",\"000937540\",\"000937540\",\"000937540\",\"000937540\",\"000935851\",\"000935851\",\"000935851\",\"000935851\",\"681800480\",\"681800480\",\"681800480\",\"681800480\",\"681800480\",\"000935119\",\"000935119\",\"000935119\",\"000935119\",\"000932075\",\"000932075\",\"000932075\",\"000932075\",\"000932075\",\"000935769\",\"000935769\",\"000935769\",\"000935769\",\"000935769\",\"000935171\",\"000935171\",\"000935171\",\"000935171\",\"000935171\",\"000932270\",\"000932270\",\"000932270\",\"000932270\",\"000932270\",\"512850697\",\"512850697\",\"512850697\",\"000931118\",\"000931118\",\"000931118\",\"000931118\",\"000937285\",\"000937285\",\"000937285\",\"000937285\",\"000937285\",\"000937327\",\"000937327\",\"000937327\",\"000937327\",\"000937327\",\"003781803\",\"003781803\",\"003781803\",\"003781803\",\"003781803\",\"000937355\",\"000937355\",\"000937355\",\"000937355\",\"684530850\",\"684530850\",\"684530850\",\"684530850\",\"684530850\",\"003782073\",\"003782073\",\"003782073\",\"003782073\",\"003780505\",\"003780505\",\"003780505\",\"003780505\",\"003780505\",\"000937366\",\"000937366\",\"000937366\",\"000937366\",\"000937366\",\"000743020\",\"000743020\",\"000743020\",\"000743020\",\"000743020\",\"000930026\",\"000930026\",\"000930026\",\"000930026\",\"001724366\",\"001724366\",\"001724366\",\"001724366\",\"001724366\",\"000933160\",\"000933160\",\"000933160\",\"000933160\",\"000933160\",\"000932931\",\"551110180\",\"551110180\",\"007815191\",\"007815191\",\"007815191\",\"007815191\",\"007815191\",\"000935116\",\"000935116\",\"000930012\",\"000930012\",\"000930012\",\"000930012\",\"000930012\",\"001722083\",\"001722083\",\"001722083\",\"001722083\",\"001722083\",\"006035165\",\"006035165\",\"006035165\",\"000937436\",\"000937436\",\"000937436\",\"000937436\",\"000034222\",\"000034222\",\"000034222\",\"000034222\",\"000034222\",\"000930900\",\"000930900\",\"000930900\",\"000930900\",\"000930900\",\"634590416\",\"634590416\",\"634590416\",\"634590416\",\"000932210\",\"000932210\",\"000932210\",\"000932210\",\"000932210\",\"001725311\",\"001725311\",\"001725311\",\"001725311\",\"001725311\",\"501110441\",\"501110441\",\"501110441\",\"501110441\",\"501110441\",\"000934405\",\"000934405\",\"000934405\",\"000934405\",\"000934405\",\"597625008\",\"597625008\",\"597625008\",\"597625008\",\"597625008\",\"001725033\",\"001725033\",\"001725033\",\"516724033\",\"516724033\",\"516724033\",\"516724033\",\"101440604\",\"101440604\",\"101440604\",\"101440604\",\"000937153\",\"000937153\",\"000937153\",\"000937153\",\"000937153\",\"001431268\",\"001431268\",\"001431268\",\"001431268\",\"000074642\",\"000074642\",\"646790936\",\"646790936\",\"646790936\",\"646790936\",\"646790936\",\"001431473\",\"001431473\",\"000930924\",\"000930924\",\"000930924\",\"000930924\",\"000930924\",\"002282803\",\"002282803\",\"002282803\",\"000930752\",\"000930752\",\"000930752\",\"000930752\",\"000930752\",\"551110196\",\"551110196\",\"551110196\",\"551110196\",\"000040800\",\"000040800\",\"000040800\",\"000040800\",\"000040800\",\"000780360\",\"000780360\",\"000780360\",\"000780360\",\"005559028\",\"005559028\",\"005559028\",\"005559028\",\"005559028\",\"005559028\",\"005559028\",\"000937425\",\"000937425\",\"000937425\",\"000937425\",\"000937425\",\"000937296\",\"000937296\",\"000937296\",\"000937296\",\"000937296\",\"000937258\",\"000937258\",\"000937258\",\"000937258\",\"000937258\",\"000937386\",\"000937386\",\"000937386\",\"000937386\",\"000937386\",\"000937243\",\"000937243\",\"000937243\",\"000937243\",\"000937243\",\"551110268\",\"551110268\",\"551110268\",\"551110268\",\"551110268\",\"000935850\",\"000935850\",\"000935850\",\"000935850\",\"000935850\",\"005550483\",\"005550483\",\"005550483\",\"005550483\",\"005550483\",\"007812054\",\"007812054\",\"007812054\",\"007812054\",\"007812054\",\"002282128\",\"002282128\",\"002282128\",\"002282128\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000933171\",\"000935710\",\"000935710\",\"000935710\",\"000935710\",\"000935710\",\"619581001\",\"619581001\",\"619581001\",\"619581001\",\"000937465\",\"000937465\",\"000937465\",\"000937465\",\"000937465\",\"004561550\",\"004561550\",\"004561550\",\"004561550\",\"000930757\",\"000930757\",\"000930757\",\"000930757\",\"001724960\",\"001724960\",\"001724960\",\"001724960\",\"000931026\",\"000931026\",\"000931026\",\"000931026\",\"000931026\",\"000934741\",\"000934741\",\"000934741\",\"000934741\",\"000710156\",\"000710156\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"005559016\",\"000930174\",\"000930174\",\"000930174\",\"000930174\",\"000930174\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"005559012\",\"001724097\",\"001724097\",\"001724097\",\"001724097\",\"001724097\",\"001724097\",\"167140663\",\"167140663\",\"167140663\",\"167140663\",\"167140663\",\"001727311\",\"001727311\",\"001727311\",\"001727311\",\"001727311\",\"006032406\",\"006032406\",\"006032406\",\"006032406\",\"000024463\",\"000024463\",\"000024463\",\"000024463\",\"000935126\",\"000935126\",\"000935126\",\"000935126\",\"501110433\",\"501110433\",\"501110433\",\"501110433\",\"501110433\",\"551110198\",\"551110198\",\"551110198\",\"551110198\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"005559026\",\"101440602\",\"101440602\",\"101440602\",\"101440602\",\"101440602\",\"000937305\",\"000937305\",\"000937305\",\"000937305\",\"000937305\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"000930149\",\"007815189\",\"007815189\",\"007815189\",\"007815189\",\"007815189\",\"000935506\",\"000935506\",\"000935506\",\"000935506\",\"000935506\",\"002282634\",\"002282634\",\"002282634\",\"002282634\",\"000937256\",\"000937256\",\"000937256\",\"000937256\",\"000937256\",\"000937256\",\"000937256\",\"000937256\",\"000937114\",\"000937114\",\"000937114\",\"000937114\",\"000937114\",\"138110583\",\"138110583\",\"138110583\",\"000935125\",\"000935125\",\"000935125\",\"000935125\",\"000935125\",\"005559027\",\"005559027\",\"005559027\",\"005559027\",\"005559027\",\"000935353\",\"000935353\",\"000935353\",\"001861090\",\"001861090\",\"001861090\",\"001861090\",\"001726359\",\"001726359\",\"001726359\",\"001726359\",\"000937369\",\"000937369\",\"000937369\",\"000937369\",\"000937369\",\"000931052\",\"000931052\",\"000931052\",\"000931052\",\"000931052\",\"512850692\",\"512850692\",\"512850692\",\"512850692\",\"000710155\",\"000710155\",\"000934069\",\"000934069\",\"000934069\",\"000934069\",\"000934069\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"001723762\",\"000711015\",\"000711015\",\"000937334\",\"000937334\",\"000937334\",\"000937334\",\"000937334\",\"006035771\",\"006035771\",\"006035771\",\"006035771\",\"006035771\",\"000937292\",\"000937292\",\"000937292\",\"000937292\",\"000937292\",\"008320038\",\"008320038\",\"008320038\",\"008320038\",\"008320038\",\"000933167\",\"000933167\",\"000933167\",\"000933167\",\"000933167\",\"000060277\",\"000060277\",\"000060277\",\"000060277\",\"000060277\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"000934236\",\"003783495\",\"003783495\",\"003783495\",\"003783495\",\"000780521\",\"000780521\",\"000780521\",\"597622004\",\"597622004\",\"597622004\",\"597622004\",\"597622004\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000934237\",\"000935703\",\"000935703\",\"000935703\",\"000935703\",\"000935703\",\"000938940\",\"000938940\",\"000938940\",\"000938940\",\"000938940\",\"001724358\",\"001724358\",\"001724358\",\"001724358\",\"001724358\",\"005912473\",\"005912473\",\"005912473\",\"005912473\",\"005912473\",\"000937201\",\"000937201\",\"000937201\",\"000937201\",\"534890400\",\"534890400\",\"534890400\",\"512850538\",\"512850538\",\"512850538\",\"512850538\",\"512850538\",\"001723760\",\"001723760\",\"001723760\",\"516724042\",\"516724042\",\"516724042\",\"000932268\",\"000932268\",\"000932268\",\"000932268\",\"005550835\",\"005550835\",\"005550835\",\"005550835\",\"005550835\",\"005550835\",\"005550835\",\"005550835\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"000930734\",\"000934029\",\"000934029\",\"000934029\",\"000934029\",\"000934029\",\"000935502\",\"000935502\",\"000935502\",\"000935502\",\"000935502\",\"003786233\",\"003786233\",\"003786233\",\"003786233\",\"000931049\",\"000931049\",\"000931049\",\"000931049\",\"000931049\",\"005550285\",\"005550285\",\"005550285\",\"005550285\",\"005550285\",\"005550873\",\"005550873\",\"002450041\",\"002450041\",\"002450041\",\"002450041\",\"516724032\",\"516724032\",\"516724032\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000933147\",\"000930811\",\"000930811\",\"000930811\",\"000930811\",\"000930811\",\"002282550\",\"002282550\",\"002282550\",\"002282550\",\"002282550\",\"000937382\",\"000937382\",\"000937382\",\"655800303\",\"655800303\",\"655800303\",\"655800303\",\"655800303\",\"000932060\",\"000932060\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"000930135\",\"001723758\",\"001723758\",\"001723758\",\"001723758\",\"003783482\",\"003783482\",\"003783482\",\"003783482\",\"003783482\",\"512850082\",\"512850082\",\"512850082\",\"512850082\",\"512850082\",\"000932046\",\"000932046\",\"000932046\",\"000934233\",\"005559009\",\"005559009\",\"005559009\",\"005559009\",\"005559009\",\"005550834\",\"005550834\",\"005550834\",\"005550834\",\"005550834\",\"005550834\",\"005550834\",\"005910302\",\"005910302\",\"007811506\",\"007811506\",\"007811506\",\"007811506\",\"007811506\",\"658620052\",\"658620052\",\"658620052\",\"658620052\",\"658620052\",\"501110470\",\"501110470\",\"501110470\",\"501110470\",\"501110470\",\"000935142\",\"000935142\",\"000935142\",\"000935142\",\"000935142\",\"684620163\",\"684620163\",\"684620163\",\"684620163\",\"000930054\",\"000930054\",\"000930054\",\"000930054\",\"000930054\",\"001431241\",\"001431241\",\"001431241\",\"677670133\",\"677670133\",\"677670133\",\"677670133\",\"677670133\",\"633040829\",\"633040829\",\"633040829\",\"633040829\",\"633040829\",\"001431425\",\"001431425\",\"001431425\",\"001431425\",\"001431425\",\"685460229\",\"685460229\",\"007815181\",\"007815181\",\"007815181\",\"007815181\",\"000937206\",\"000937206\",\"000937206\",\"000937206\",\"000930039\",\"000930039\",\"000930039\",\"000930039\",\"000930039\",\"001861092\",\"001861092\",\"001861092\",\"001861092\",\"001861092\",\"555130073\",\"555130073\",\"555130073\",\"555130073\",\"555130073\",\"000930787\",\"000930787\",\"000930787\",\"000930787\",\"000930787\",\"658620537\",\"658620537\",\"658620537\",\"658620537\",\"658620537\",\"000937350\",\"000937350\",\"000937350\",\"000937350\",\"501110309\",\"501110309\",\"501110309\",\"501110309\",\"000935214\",\"000935214\",\"000935214\",\"000935214\",\"000935214\",\"007815186\",\"007815186\",\"007815186\",\"007815186\",\"007815186\",\"003782074\",\"003782074\",\"003782074\",\"003782074\",\"501110916\",\"501110916\",\"501110916\",\"501110916\",\"000746122\",\"003786440\",\"597625022\",\"597625022\",\"597625022\",\"512850091\",\"512850091\",\"512850091\",\"512850091\",\"512850091\",\"512850091\",\"512850091\",\"512850091\",\"000937234\",\"000937234\",\"000937234\",\"000937234\",\"000937234\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"512850114\",\"004560457\",\"004560457\",\"004560457\",\"501110851\",\"501110851\",\"501110851\",\"501110851\",\"501110851\",\"000937426\",\"000937426\",\"000937426\",\"000937426\",\"000937426\",\"003780183\",\"003780183\",\"003780183\",\"003780183\",\"501110648\",\"000935420\",\"000935420\",\"000932263\",\"000932263\",\"001730758\",\"001730758\",\"001730758\",\"001730758\",\"000930784\",\"000930784\",\"000930784\",\"000930784\",\"634590215\",\"634590215\",\"634590215\",\"634590215\",\"634590215\",\"003785124\",\"003785124\",\"658620054\",\"658620054\",\"658620054\",\"658620054\",\"001439920\",\"001439920\",\"001439920\",\"001439920\",\"001439920\",\"000935141\",\"000935141\",\"000935141\",\"000935141\",\"000935141\",\"000932275\",\"000932275\",\"000932275\",\"000932275\",\"000932275\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000930199\",\"000937156\",\"000937156\",\"000937156\",\"000937156\",\"000937156\",\"000622085\",\"000622085\",\"000622085\",\"000622085\",\"000932932\",\"000932932\",\"000932932\",\"000932932\",\"000932932\",\"000937335\",\"000937335\",\"000937335\",\"000937335\",\"000932080\",\"000932080\",\"000932080\",\"000932080\",\"000932080\",\"000937365\",\"000937365\",\"000937365\",\"000937365\",\"501110325\",\"501110325\",\"501110325\",\"000931048\",\"000931048\",\"000931048\",\"000931048\",\"000931048\",\"000937180\",\"000937180\",\"000937180\",\"000937180\",\"000937180\",\"001724096\",\"001724096\",\"001724096\",\"001724096\",\"001724096\",\"001724096\",\"001724096\",\"001724096\",\"646790929\",\"646790929\",\"646790929\",\"646790929\",\"646790929\",\"683820023\",\"683820023\",\"683820023\",\"683820023\",\"683820023\",\"001724626\",\"001724626\",\"001724626\",\"001724626\",\"001724626\",\"000937442\",\"000937442\",\"000937442\",\"000937442\",\"000937442\",\"000931006\",\"000931006\",\"000931006\",\"000931006\",\"000931006\",\"005913210\",\"005913210\",\"005913210\",\"005550513\",\"005550513\",\"005550513\",\"005550513\",\"000935290\",\"000935290\",\"000935290\",\"000935290\",\"000935290\",\"000935049\",\"000935049\",\"000935049\",\"006032957\",\"006032957\",\"398220205\",\"398220205\",\"398220205\",\"398220205\",\"000937326\",\"000937326\",\"000937326\",\"000937326\",\"000937326\",\"680120104\",\"680120104\",\"680120104\",\"680120104\",\"680120104\",\"537460190\",\"537460190\",\"537460190\",\"003781811\",\"003781811\",\"003781811\",\"003781811\",\"003781811\",\"001431240\",\"001431240\",\"001431240\",\"001431240\",\"001431240\",\"000937286\",\"000937286\",\"000937286\",\"000937286\",\"000937286\",\"005550633\",\"005550633\",\"005550633\",\"005550633\",\"005550633\",\"000934443\",\"000934443\",\"000934443\",\"000934443\",\"000934443\",\"000937181\",\"000937181\",\"000937181\",\"000937181\",\"000937181\",\"000930221\",\"000930221\",\"000930221\",\"000930221\",\"000930221\",\"512850524\",\"512850524\",\"512850524\",\"512850524\",\"512850524\",\"498840403\",\"498840403\",\"498840403\",\"498840403\",\"498840403\",\"501110901\",\"501110901\",\"501110901\",\"501110901\",\"501110901\",\"000937303\",\"000937303\",\"000937303\",\"000937303\",\"000937303\",\"000935385\",\"000935385\",\"000935385\",\"000935385\",\"000937466\",\"000937466\",\"000937466\",\"000937466\",\"000937466\",\"005550861\",\"005550861\",\"005550861\",\"005550861\",\"005550861\",\"512850083\",\"512850083\",\"512850083\",\"512850083\",\"512850083\",\"512850083\",\"512850083\",\"512850083\",\"000937270\",\"000937270\",\"000937270\",\"000937270\",\"000937270\",\"001725313\",\"001725313\",\"001725313\",\"001725313\",\"001725313\",\"003780503\",\"003780503\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000933165\",\"000937208\",\"000937208\",\"000937208\",\"000937208\",\"000937208\",\"000937385\",\"000937385\",\"000937385\",\"000937385\",\"000937385\",\"007812201\",\"007812201\",\"007812201\",\"007812201\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"001723761\",\"000930308\",\"000930308\",\"000930308\",\"000930308\",\"000930308\",\"473350894\",\"473350894\",\"473350894\",\"473350894\",\"473350894\",\"000747070\",\"000939133\",\"000939133\",\"000939133\",\"000939133\",\"000939133\",\"000934359\",\"000934359\",\"000934359\",\"000934359\",\"000934359\",\"005550997\",\"005550997\",\"005550997\",\"005550997\",\"003786231\",\"003786231\",\"003786231\",\"003786231\",\"501110459\",\"501110459\",\"501110459\",\"501110459\",\"501110459\",\"512480151\",\"512480151\",\"512480151\",\"000937242\",\"000937242\",\"000937242\",\"000937242\",\"000937242\",\"162520590\",\"162520590\",\"162520590\",\"420430190\",\"420430190\",\"420430190\",\"420430190\",\"420430190\",\"501110334\",\"501110334\",\"501110334\",\"501110334\",\"501110334\",\"000930782\",\"000930782\",\"000930782\",\"000930782\",\"000930782\",\"005550138\",\"005550138\",\"005550138\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"501110328\",\"003781165\",\"003781165\",\"003781165\",\"003781165\",\"003781165\",\"501110327\",\"501110327\",\"003781815\",\"003781815\",\"003781815\",\"003781815\",\"003781815\",\"000935140\",\"000935140\",\"000935140\",\"000935140\",\"000935140\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"005559034\",\"000935712\",\"000935712\",\"001431257\",\"001431257\",\"001431257\",\"000937259\",\"000937259\",\"000937259\",\"000937259\",\"000937259\",\"003781819\",\"003781819\",\"003781819\",\"003781819\",\"003781819\",\"005915335\",\"005915335\",\"005915335\",\"001690081\",\"001690081\",\"001690081\",\"001690081\",\"001690081\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"000930576\",\"658620156\",\"658620156\",\"658620156\",\"658620156\",\"658620156\",\"000024464\",\"000024464\",\"000024464\",\"000934444\",\"000934444\",\"000934444\",\"000934444\",\"000934444\",\"000934030\",\"000934030\",\"000934030\",\"000934030\",\"000934030\",\"000934030\",\"000934030\",\"000930733\",\"000930733\",\"000930733\",\"000930733\",\"000930733\",\"000930733\",\"000930733\",\"000930733\",\"006033741\",\"006033741\",\"005550969\",\"005550969\",\"005550969\",\"005550969\",\"005550969\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"000937115\",\"005550139\",\"005550139\",\"005550139\",\"000937438\",\"000937438\",\"584680130\",\"584680130\",\"584680130\",\"584680130\",\"584680130\",\"000937338\",\"000937338\",\"000937338\",\"000930011\",\"000930011\",\"000930011\",\"000930011\",\"003781807\",\"003781807\",\"003781807\",\"003781807\",\"003781807\",\"000935246\",\"000935246\",\"000935246\",\"000935246\",\"005551022\",\"005551022\",\"005551022\",\"005551022\",\"005559008\",\"005559008\",\"005559008\",\"005559008\",\"005559008\",\"005559008\",\"005559008\",\"005559008\",\"005910388\",\"005910388\",\"005910388\",\"005910388\",\"005910388\",\"000937224\",\"000937224\",\"000937224\",\"000937224\",\"000937224\",\"000757700\",\"000757700\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"501110563\",\"605050185\",\"605050185\",\"605050185\",\"605050185\",\"605050185\",\"005550252\",\"005550252\",\"005550252\",\"005550252\",\"000930058\",\"000930058\",\"000930058\",\"000930058\",\"000930058\",\"000930058\",\"000930058\",\"000544182\",\"000544182\",\"000544182\",\"001724285\",\"001724285\",\"001724285\",\"001724285\",\"001724285\",\"000930017\",\"000930017\",\"000930017\",\"000930017\",\"000930017\",\"000937240\",\"000937240\",\"000937240\",\"000937240\",\"000937240\",\"501110468\",\"501110468\",\"501110468\",\"501110468\",\"501110468\",\"000932274\",\"000932274\",\"000932274\",\"000932274\",\"000932274\",\"000937254\",\"000937254\",\"000937254\",\"000937254\",\"000937254\",\"000930463\",\"000930463\",\"000930463\",\"000930463\",\"000930463\",\"000932203\",\"000932203\",\"000932203\",\"000932203\",\"000932203\",\"317220278\",\"317220278\",\"317220278\",\"317220278\",\"000931060\",\"000931060\",\"000931060\",\"000931060\",\"000931060\",\"000931060\",\"000931060\",\"000931060\",\"512850079\",\"512850079\",\"512850079\",\"512850079\",\"512850079\",\"627560712\",\"627560712\",\"627560712\",\"627560712\",\"627560712\",\"681800501\",\"681800501\",\"681800501\",\"681800501\",\"681800501\",\"005551055\",\"005551055\",\"005551055\",\"005551055\",\"005551055\",\"001730565\",\"001730565\",\"001730565\",\"001730565\",\"000937437\",\"000937437\",\"000937437\",\"000937437\",\"498840922\",\"498840922\",\"001730561\",\"001730561\",\"001730561\",\"000932204\",\"000932204\",\"000932204\",\"000932204\",\"000932204\",\"000932204\",\"000932204\",\"000931024\",\"000931024\",\"000931024\",\"000931024\",\"001865040\",\"001865040\",\"005550059\",\"005550059\",\"005550059\",\"005550059\",\"534890146\",\"534890146\",\"534890146\",\"534890146\",\"534890146\",\"000930027\",\"000930027\",\"000930027\",\"000930892\",\"000930892\",\"000930892\",\"000930892\",\"005559010\",\"005559010\",\"005559010\",\"005559010\",\"005559010\",\"000937485\",\"000937485\",\"000937485\",\"000937485\",\"003100282\",\"003100282\",\"007812020\",\"007812020\",\"007812020\",\"501110852\",\"501110852\",\"501110852\",\"501110852\",\"501110852\",\"501110852\",\"501110852\",\"501110852\",\"000930657\",\"000930657\",\"000930657\",\"000930657\",\"000937168\",\"000937168\",\"000937168\",\"000937168\",\"000937168\",\"000934067\",\"000934067\",\"000934067\",\"000934067\",\"007773105\",\"007773105\",\"007773105\",\"007773105\",\"007773105\",\"000937795\",\"000937795\",\"000937795\",\"000937795\",\"000937795\",\"003780350\",\"003780350\",\"003780350\",\"003780350\",\"633040579\",\"633040579\",\"633040579\",\"633040579\",\"633040579\",\"501110787\",\"501110787\",\"501110787\",\"501110787\",\"501110787\",\"000937227\",\"000937227\",\"000937227\",\"000937227\",\"000934232\",\"000934232\",\"000934232\",\"597621301\",\"597621301\",\"597621301\",\"597621301\",\"000937239\",\"000937239\",\"000937239\",\"000937239\",\"000937239\",\"000935711\",\"000935711\",\"000935711\",\"000935711\",\"001723650\",\"001723650\",\"000937223\",\"000937223\",\"000937223\",\"000937223\",\"000930233\",\"000930233\",\"000930233\",\"000930233\",\"000930233\",\"000544728\",\"000544728\",\"000544728\",\"000544728\",\"000544728\",\"000060749\",\"000060749\",\"000060749\",\"000937381\",\"000937381\",\"000937381\",\"000937381\",\"000937381\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937212\",\"000937294\",\"000935117\",\"000935117\",\"000935117\",\"000935117\",\"000935117\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"501110434\",\"655800304\",\"655800304\",\"655800304\",\"655800304\",\"655800304\",\"000934235\",\"000934235\",\"000934235\",\"000934235\",\"003780232\",\"003780232\",\"003780232\",\"003780232\",\"416160636\",\"416160636\",\"416160636\",\"416160636\",\"416160636\",\"008321025\",\"008321025\",\"008321025\",\"008321025\",\"000931062\",\"000931062\",\"000931062\",\"000931062\",\"000930711\",\"000930711\",\"000930711\",\"000930711\",\"000930711\",\"000935208\",\"000935208\",\"000935208\",\"005271413\",\"005271413\",\"005271413\",\"005271413\",\"005271413\",\"000060112\",\"000060112\",\"000060112\",\"000060112\",\"001727312\",\"001727312\",\"001727312\",\"001727312\",\"001727312\",\"003782537\",\"003782537\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"000937155\",\"597625011\",\"597625011\",\"597625011\",\"597625011\",\"597625011\",\"000935118\",\"000935118\",\"000935118\",\"000935118\",\"000935118\",\"658620050\",\"658620050\",\"658620050\",\"658620050\",\"658620050\",\"001725411\",\"001725411\",\"001725411\",\"000931025\",\"000931025\",\"000931025\",\"000931025\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937188\",\"000937214\",\"000937214\",\"000937214\",\"000937214\",\"000937214\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"000937370\",\"003780216\",\"003780216\",\"005550071\",\"005550071\",\"005550071\",\"005550071\",\"005550071\",\"605050146\",\"605050146\",\"605050146\",\"605050146\",\"001780615\",\"001780615\",\"001780615\",\"001780615\",\"000931061\",\"000931061\",\"000931061\",\"000931061\",\"000935050\",\"000935050\",\"000935504\",\"000935504\",\"000935504\",\"000935504\",\"512850539\",\"512850539\",\"512850539\",\"512850539\",\"512850539\",\"001861088\",\"001861088\",\"001861088\",\"001861088\",\"658620044\",\"658620044\",\"540920517\",\"540920517\",\"540920517\",\"540920517\",\"540920517\",\"000074892\",\"000074892\",\"633040830\",\"633040830\",\"633040830\",\"633040830\",\"000930029\",\"000930029\",\"000930029\",\"000931003\",\"000931003\",\"000931003\",\"000931003\",\"000931003\",\"162520516\",\"162520516\",\"162520516\",\"000930314\",\"000930314\",\"000930314\",\"000930314\",\"000930314\",\"000937182\",\"000937182\",\"000937182\",\"000937182\",\"000937182\",\"001850048\",\"001850048\",\"001850048\",\"000930320\",\"000930320\",\"000930320\",\"000938035\",\"000938035\",\"000938035\",\"000938035\",\"000938035\",\"634590404\",\"634590404\",\"634590404\",\"634590404\",\"634590404\",\"005550484\",\"005550484\",\"005550484\",\"005550484\",\"000930147\",\"000930147\",\"000930147\",\"000930147\",\"000930147\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"501110397\",\"416160759\",\"416160759\",\"416160759\",\"416160759\",\"416160759\",\"005550967\",\"005550967\",\"005550967\",\"005550967\",\"003781125\",\"003781125\",\"003781125\",\"003781125\",\"003781125\",\"000932267\",\"000932267\",\"000932267\",\"000932267\",\"000932267\",\"000932047\",\"000932047\",\"000932047\",\"000930225\",\"000930225\",\"000930225\",\"000930225\",\"000930225\",\"000930028\",\"000930028\",\"000930028\",\"512850595\",\"512850595\",\"512850595\",\"512850595\",\"101440606\",\"101440606\",\"101440606\",\"101440606\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000937146\",\"000935256\",\"000935256\",\"000935256\",\"000935256\",\"000935256\",\"000935256\",\"501110324\",\"501110324\",\"501110324\",\"501110324\",\"501110324\",\"000930173\",\"000930173\",\"000930173\",\"000930173\",\"000937207\",\"000937207\",\"003785123\",\"003785123\",\"000935127\",\"000935127\",\"000935127\",\"000935127\",\"001725312\",\"001725312\",\"001725312\",\"001725312\",\"001725312\",\"000937236\",\"000937236\",\"000937236\",\"000937236\",\"000937236\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"000937380\",\"005551021\",\"005551021\",\"005551021\",\"005551021\",\"005551021\",\"000931893\",\"000931893\",\"000931893\",\"000931893\",\"000931893\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"512850080\",\"000540166\",\"000540166\",\"000937169\",\"000937169\",\"000937169\",\"000937169\",\"000937169\",\"005970044\",\"005970044\",\"005970044\",\"005970044\",\"005970044\",\"000239350\",\"000239350\",\"000239350\",\"000239350\",\"000239350\",\"662130421\",\"662130421\",\"662130421\",\"662130421\",\"662130421\",\"000932272\",\"000932272\",\"000932272\",\"000932272\",\"000932272\",\"000934068\",\"000934068\",\"000934068\",\"000934068\",\"000934068\",\"005915307\",\"005915307\",\"597625033\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000931063\",\"000933109\",\"000933109\",\"000933109\",\"000933109\",\"000933109\",\"005550211\",\"005550211\",\"005550211\",\"005550211\",\"005550211\",\"000931015\",\"000931015\",\"000931015\",\"000931015\",\"000931015\",\"007811966\",\"007811966\",\"007811966\",\"007811966\",\"007811966\",\"000930083\",\"000930083\",\"000930083\",\"000930083\",\"000937368\",\"000937368\",\"000937368\",\"000937368\",\"000937368\",\"512850063\",\"512850063\",\"512850063\",\"512850063\",\"512850063\",\"512850063\",\"512850063\",\"512850063\",\"003782675\",\"003782675\",\"003782675\",\"003782675\",\"000935036\",\"000935036\",\"000935036\",\"000935036\",\"000935036\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"512850092\",\"000935172\",\"000935172\",\"000935172\",\"000935172\",\"551110197\",\"551110197\",\"551110197\",\"002282127\",\"002282127\",\"002282127\",\"002282717\",\"002282717\",\"002282717\",\"002282717\",\"007811452\",\"007811452\",\"007811452\",\"007811452\",\"007811452\",\"124961283\",\"124961283\",\"000937364\",\"000937364\",\"000937364\",\"000937364\",\"000937364\",\"001730178\",\"001730178\",\"005550869\",\"005550869\",\"005550869\",\"005550869\",\"001725412\",\"001725412\",\"001725412\",\"001725412\",\"001725412\",\"000851322\",\"000851322\",\"000851322\",\"000851322\",\"000935770\",\"000935770\",\"000935770\",\"000935770\",\"000935770\",\"000097663\",\"000097663\",\"000097663\",\"000097663\",\"000097663\",\"000074882\",\"000074882\",\"000074882\",\"000074882\",\"000074882\",\"007815185\",\"007815185\",\"007815185\",\"007815185\",\"007815185\"],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"ndc9\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Class Distribution\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4968b251-7caa-4fb2-b231-81294f56490a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot to visualise data distribution across full dataset\n",
        "fig = px.histogram(directory_df,\n",
        "                   x=\"ndc9\",\n",
        "                   title=\"Class Distribution\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyFK6vbe_2nv",
        "outputId": "6e0a08b9-3834-4941-fa05-7e256ec190bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes:  851\n",
            "Total number of samples in dataset:  5449\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of classes: \", len(list(class_distrib.keys())))\n",
        "print(\"Total number of samples in dataset: \", len((directory_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Issue: Sparse dataset and class imbalance\n",
        "\n",
        "To refine our problem space and minimise predictive errors during the building of a model, we seek to first reduce the dataset by retaining 40 classes that contain the highest number of sample images. Thereafter, we can consider assigning class weights and artificially increasing the number of samples."
      ],
      "metadata": {
        "id": "Old94eKpoKki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "SeEsyMjAFhQR",
        "outputId": "066afe87-47bc-4427-e62e-a4cc9975d529"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e06e4560-81ff-407a-80c2-53ee9d09caf2\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e06e4560-81ff-407a-80c2-53ee9d09caf2\")) {                    Plotly.newPlot(                        \"e06e4560-81ff-407a-80c2-53ee9d09caf2\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hole\":0.3,\"hovertemplate\":\"label=%{label}\\u003cbr\\u003evalue=%{value}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"labels\":[\"5559012\",\"5559032\",\"512850114\",\"930576\",\"937212\",\"512850087\",\"933147\",\"501110563\",\"937146\",\"5559016\",\"1723762\",\"512850092\",\"937188\",\"1724280\",\"930199\",\"931063\",\"937115\",\"934236\",\"933165\",\"1723761\",\"930149\",\"501110397\",\"930734\",\"512850080\",\"930135\",\"934237\",\"937380\",\"937155\",\"937370\",\"1725728\",\"5559034\",\"501110434\",\"933171\",\"501110328\",\"1725413\",\"5559026\",\"5559008\",\"512850091\",\"931060\",\"512850083\"],\"legendgroup\":\"\",\"name\":\"\",\"showlegend\":true,\"values\":[25,22,16,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,12,12,12,12],\"type\":\"pie\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Class Distribution\",\"x\":0.45},\"width\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e06e4560-81ff-407a-80c2-53ee9d09caf2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot and visualise distribution of samples in top 40 classes\n",
        "fig = px.pie(\n",
        "    names=list(class_names)[:40],\n",
        "    values=list(class_distrib.values())[:40],\n",
        "    width=600,\n",
        "    hole=.3,\n",
        "    title=\"Class Distribution\"\n",
        ")\n",
        "fig.update_layout({'title':{'x': .45}})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD7i_l2bcDxh",
        "outputId": "f2f50872-d690-470c-d4a0-a33a04e5cf2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 556 entries, 0 to 555\n",
            "Data columns (total 11 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   ndc9           556 non-null    object\n",
            " 1   filename       556 non-null    object\n",
            " 2   is_ref         556 non-null    bool  \n",
            " 3   is_front       556 non-null    bool  \n",
            " 4   ndc            556 non-null    object\n",
            " 5   splshape       556 non-null    object\n",
            " 6   splshape_text  556 non-null    object\n",
            " 7   splimprint     556 non-null    object\n",
            " 8   splcolor_text  556 non-null    object\n",
            " 9   rxstring       556 non-null    object\n",
            " 10  filepath       556 non-null    object\n",
            "dtypes: bool(2), object(9)\n",
            "memory usage: 40.3+ KB\n"
          ]
        }
      ],
      "source": [
        "# Retain 40 classes containing highest number of samples\n",
        "data_df = directory_df[directory_df['ndc9'].isin(list(class_names)[:40])].reset_index(drop=True)\n",
        "data_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDfW2Jpj3gj4"
      },
      "source": [
        "Replace *filepath* to your desired path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tWrHC683mDT"
      },
      "outputs": [],
      "source": [
        "# Replace path to images\n",
        "data_df['filepath'] = [os.path.join(IMAGE_DIR,\n",
        "                                    filepath.split('/')[-2],\n",
        "                                    filepath.split('/')[-1]) for filepath in data_df['filepath'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qAFqy-e0Jdm",
        "outputId": "347348db-0ec1-4838-ce94-84aff69970a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes in new dataset:  40\n"
          ]
        }
      ],
      "source": [
        "# Define classes as unique NDC9 values\n",
        "CLASSES = np.unique(data_df[\"ndc9\"].values)\n",
        "# Define number of classes\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "print(\"Number of classes in new dataset: \", NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVAl0uxo0U_h"
      },
      "source": [
        "# Splitting the Dataset\n",
        "\n",
        "Due to the imbalanced number of samples in each class, the stratified split method was utilised to preserve the original proportion of samples in each class.\n",
        "\n",
        "To minimise biasness in the dataset, different class weights were assigned to both majority and minority classes. In laymen terms, this means that higher class weights were set for the minority class, while lowered weights were given to the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAZXzPBi4Y9e"
      },
      "outputs": [],
      "source": [
        "# Define ratio of splits\n",
        "VAL_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye7wJilj0fGL"
      },
      "outputs": [],
      "source": [
        "# # Split dataset into train, test, and validaton sets with stratification\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT,\n",
        "#                                                   stratify=y, random_state=SEED)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VAL_SPLIT,\n",
        "#                                                   stratify=y_train, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWtwTxOX1LFw",
        "outputId": "78211942-1f1a-465a-9236-211e70b7ef88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(556,)\n",
            "(556, 40)\n"
          ]
        }
      ],
      "source": [
        "# # Define X and y variables\n",
        "# # Assign image file paths to X\n",
        "# X = data_df['filepath'].values\n",
        "# # Convert NDC9 strings to unique numerical categories\n",
        "# y_cat = data_df['ndc9'].astype('category').cat.codes.values\n",
        "\n",
        "# # Observe shapes of X and y\n",
        "# print(X.shape)\n",
        "# print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LhByr55osVm"
      },
      "source": [
        "# Image Preprocessing: Final Functions implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mnbp4iCogzp"
      },
      "source": [
        "Applying image pre-processing techniques to enhance or extract key features to learn.\n",
        "\n",
        "1. Colour (Images)\n",
        "2. Shape and Texture (HOG-LBP Feature Vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colour: Image Preprocessing"
      ],
      "metadata": {
        "id": "HvFsc3HPsKGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noxH2hUTnXcG"
      },
      "outputs": [],
      "source": [
        "class ColourPreprocessor(object):\n",
        "  \"\"\"\n",
        "  Preproccesses dataset of images for colour enhancement\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, target_size=224, clip_limit=3.0, tile_grid_size=(5, 5)):\n",
        "\n",
        "    # Initialise and assign variables\n",
        "    self.target_size = target_size\n",
        "    self.clip_limit = clip_limit\n",
        "    self.tile_grid_size = tile_grid_size\n",
        "\n",
        "  def image_generator(self, img):\n",
        "    \"\"\"\n",
        "    Generates preprocessed image\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # Preprocess image\n",
        "      pp_img = self.preprocess_image(img)\n",
        "\n",
        "      return pp_img\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"[Image generator] Error occured:\\n\", e)\n",
        "\n",
        "  def preprocess_image(self, img):\n",
        "    \"\"\"\n",
        "    Function that calls and applies all image preprocessing techniques\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # Apply blurring to reduce color inconsistencies and imprints\n",
        "      img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "      # Apply histogram equalisation\n",
        "      img = self.hist_equalisation(img)\n",
        "\n",
        "      return img\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"[Image preprocessing] Error occured:\\n\", e)\n",
        "\n",
        "  def hist_equalisation(self, img):\n",
        "    \"\"\"\n",
        "    Function to apply histogram equalisation technique to L channel of LAB image\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # Convert image to LAB\n",
        "      lab_img  = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "      # split the image into L, A, B channels\n",
        "      l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
        "\n",
        "      # Apply filter to smoothen image\n",
        "      lab_img = cv2.bilateralFilter(lab_img, 7 , 11, 11)\n",
        "\n",
        "      # Apply histogram equalisation technique on lightness (L) channel\n",
        "      clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "      clahed_l = clahe.apply(l_channel)\n",
        "\n",
        "      # Merge clahed_l with the remaining untouched A, B channels\n",
        "      merged_channels = cv2.merge((clahed_l, a_channel, b_channel))\n",
        "\n",
        "      # Convert back to BGR image\n",
        "      final_output = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "      # Return final processed image\n",
        "      return final_output\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"[Histogram equalisation] Error occured:\\n\", e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def colour_preprocessing_pipeline(image_paths,\n",
        "                                 is_float=True,\n",
        "                                 image_size=IMAGE_SIZE,\n",
        "                                 clip_limit=3.0, tile_grid_size=(5, 5)):\n",
        "  \"\"\"\n",
        "  Function to process pipeline for colour preprocessing\n",
        "  \"\"\"\n",
        "  #Instantiate class instance\n",
        "  colour_preprocessor = ColourPreprocessor(target_size=image_size,\n",
        "                                           clip_limit=clip_limit,\n",
        "                                           tile_grid_size=tile_grid_size)\n",
        "\n",
        "  # Store preprocessed data\n",
        "  X = []\n",
        "\n",
        "  # Apply functions on each image\n",
        "  for image_path in tqdm(image_paths):\n",
        "    # Read image\n",
        "    input_img = cv2.imread(image_path)\n",
        "    # Preprocess image\n",
        "    img = colour_preprocessor.image_generator(input_img)\n",
        "\n",
        "    # Append data to respective lists\n",
        "    X.append(img)\n",
        "\n",
        "  # Return preprocessed data\n",
        "  if is_float:\n",
        "    return np.array(X, np.float32)\n",
        "  return np.array(X)"
      ],
      "metadata": {
        "id": "_8peq4zU277n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess datasets\n",
        "X_train_colour_image = colour_preprocessing_pipeline(image_paths=train_df[\"original_filepath\"].values)\n",
        "X_val_colour_image = colour_preprocessing_pipeline(image_paths=val_df[\"original_filepath\"].values)\n",
        "X_test_colour_image = colour_preprocessing_pipeline(image_paths=test_df[\"original_filepath\"].values)"
      ],
      "metadata": {
        "id": "uXjgX91A3GnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxLqLGlsT1CB"
      },
      "source": [
        "## Shape + Texture Descriptor (Handcrafted features)\n",
        "\n",
        "Local Binary Pattern (LBP) is widely used in texture analysis to generate more intricate texture-based features. The LBP algorithm computes a new pixel value by  comparing the neighbours of each pixel with its center pixel as a threshold, where the sequence of binary values form the new final pixel in decimal form.\n",
        "\n",
        "Features of LBP were transformed into a feature histogram to efficiently represent textural properties of both the shape and imprint text regions in each image.\n",
        "\n",
        "To further improve feature engineering, the integration of Histogram of Oriented Gradients (HOG) was introduced as a descriptor for object localisation. It is a technique that counts events of gradient orientation in specific regions of an image.\n",
        "\n",
        "These two methods have been long-standing techniques used in facial recogntion models for its effectiveness in detecting key facial structures, in consideration of variations in illumination and occlusions.\n",
        "\n",
        "Combined, the LBP histogram and HOG feature vectors becomes a single array for each image to be fed as numerical input into a fusion model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSMlBYfBkoYC"
      },
      "outputs": [],
      "source": [
        "class TextureDescriptor(object):\n",
        "  \"\"\"\n",
        "  Preproccesses original dataset of images and extracts feature vectors for texture\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, target_size=224):\n",
        "    # Initialise and assign variables\n",
        "    self.target_size = target_size\n",
        "\n",
        "  def get_combined_features(self, img):\n",
        "    \"\"\"\n",
        "    Generates combined feature array\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # Preprocess image\n",
        "      pp_img = self.preprocess_image(img)\n",
        "\n",
        "      # Get feature vector from lbp_extractor\n",
        "      lbp_features = self.lbp_extractor(pp_img)\n",
        "      # Get feature vector from hog_extractor\n",
        "      hog_features = self.hog_extractor(pp_img)\n",
        "\n",
        "      return np.hstack([lbp_features, hog_features])\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"[Combined feature generator] Error occured:\\n\", e)\n",
        "\n",
        "\n",
        "  def resize_img(self, img):\n",
        "    \"\"\"\n",
        "    Resizes image to target size\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
        "      # (h, w) of original image\n",
        "      og_size = img.shape[:2]\n",
        "      # Find ratio to prevent distortion\n",
        "      ratio = float(self.target_size)/max(og_size)\n",
        "      # Ratio-ed (h, w) of desired size\n",
        "      new_size = tuple([int(x*ratio) for x in og_size])\n",
        "      # (w, h) of target resized image\n",
        "      img = cv2.resize(img, (new_size[1], new_size[0]))\n",
        "      # Find differences of desired size (512) and newly computed size\n",
        "      delta_w = self.target_size - new_size[1]\n",
        "      delta_h = self.target_size - new_size[0]\n",
        "      # Compute values for image padding to make black borders\n",
        "      top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "      left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "      # Generate final binary image according to target size\n",
        "      final_img = cv2.copyMakeBorder(img, top, bottom, left, right,\n",
        "                                    cv2.BORDER_CONSTANT,\n",
        "                                    value=[0, 0, 0])\n",
        "\n",
        "      return final_img\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"[Image resizing] Error occured:\\n\", e)\n",
        "\n",
        "  def lbp_extractor(self, img):\n",
        "    \"\"\"\n",
        "    Returns derived LBP bins as array\n",
        "    \"\"\"\n",
        "    # Define values for lbp\n",
        "    radius= 2\n",
        "    num_pts = 10\n",
        "    bin = 2**num_pts\n",
        "\n",
        "    # Compute lbp\n",
        "    lbp = local_binary_pattern(img, num_pts, radius, method='uniform')\n",
        "    # Compute lbp histograms by bins\n",
        "    (hist, hist_len) = np.histogram(lbp.ravel(), bins=np.arange(0, bin))\n",
        "    lbp_hist = hist.astype(\"float32\")\n",
        "\n",
        "    # Return array in float32 format\n",
        "    return np.array(lbp_hist, np.float32)\n",
        "\n",
        "  def hog_extractor(self, img):\n",
        "    \"\"\"\n",
        "    Returns derived HOG array\n",
        "    \"\"\"\n",
        "    fd, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                        cells_per_block=(2, 2),\n",
        "                        visualize=True, channel_axis=None)\n",
        "\n",
        "    # Rescale histogram for better display\n",
        "    fd = exposure.rescale_intensity(fd, in_range=(0, 10))\n",
        "\n",
        "    # Return array in float32 format\n",
        "    return np.array(fd, np.float32)\n",
        "\n",
        "  def preprocess_image(self, img):\n",
        "    \"\"\"\n",
        "    Image preprocessing before extracting shape and textural features\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # Make sure all images have the same size\n",
        "      img = self.resize_img(img)\n",
        "\n",
        "      # Convert image to greyscale\n",
        "      grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      # Apply gaussian filter\n",
        "      gaussian = cv2.GaussianBlur(grey_img, (3, 3) ,cv2.BORDER_DEFAULT)\n",
        "\n",
        "      return grey_img\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"[Image Preprocessing] Error occured:\\n\", e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def texture_preprocessing_pipeline(image_paths,\n",
        "                                   image_size=IMAGE_SIZE):\n",
        "  \"\"\"\n",
        "  Function to process pipeline for texture feature extracton\n",
        "  \"\"\"\n",
        "  #Instantiate class instance\n",
        "  texture_extractor = TextureDescriptor(target_size=image_size)\n",
        "\n",
        "  # Store preprocessed data\n",
        "  X = []\n",
        "\n",
        "  # Apply functions on each image and assign respective labels\n",
        "  for image_path in tqdm(image_paths):\n",
        "    # Read image\n",
        "    input_img = cv2.imread(image_path)\n",
        "    # Get feature vector\n",
        "    features = texture_extractor.get_combined_features(input_img)\n",
        "\n",
        "    # Append data to respective lists\n",
        "    X.append(features)\n",
        "\n",
        "  # Return preprocessed set of data\n",
        "  return np.array(X)"
      ],
      "metadata": {
        "id": "zwPyO3jT2y7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load final split dataset containing pre-processed images"
      ],
      "metadata": {
        "id": "KMeHnLpq1Tq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets from .npz files\n",
        "train_data = np.load(os.path.join(NPZ_DIR, 'full_trainset.npz'), allow_pickle=True)\n",
        "test_data = np.load(os.path.join(NPZ_DIR, 'full_testset.npz'), allow_pickle=True)"
      ],
      "metadata": {
        "id": "bSNF-zxe2zdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionaries to store datasets\n",
        "train_set = dict()\n",
        "test_set = dict()\n",
        "\n",
        "# Unload objects from .npy files into dictionary of key: np.array() items\n",
        "for key in train_data.files:\n",
        "  train_set[key] = train_data[key]\n",
        "for key in test_data.files:\n",
        "  test_set[key] = test_data[key]\n",
        "\n",
        "# Distinguish between train and validation sets\n",
        "val_set = {k: train_set[k][np.where(train_set['is_train']==False)] for k in train_set.keys()}\n",
        "train_set = {k: train_set[k][np.where(train_set['is_train']==True)] for k in train_set.keys()}"
      ],
      "metadata": {
        "id": "FLU_CiIg3LVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form dataframes for single-input model's ImageDataGenerator"
      ],
      "metadata": {
        "id": "5pxfBdDJ-Ts6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame.from_dict({key: value for key, value in train_set.items() if key in ['ndc9', 'original_filepath', 'colour_filepath'] })\n",
        "val_df = pd.DataFrame.from_dict({key: value for key, value in val_set.items() if key in ['ndc9', 'original_filepath', 'colour_filepath'] })\n",
        "test_df = pd.DataFrame.from_dict({key: value for key, value in test_set.items() if key in ['ndc9', 'original_filepath', 'colour_filepath'] })"
      ],
      "metadata": {
        "id": "AdvAZ0wn-av-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace absolute path with relative path\n",
        "for col in [\"original_filepath\", \"colour_filepath\"]:\n",
        "  train_df[col] = train_df[col].apply(lambda x: f\"{os.getcwd()}/{DATASET_DIR}/{x}\")\n",
        "  val_df[col] = val_df[col].apply(lambda x: f\"{os.getcwd()}/{DATASET_DIR}/{x}\")\n",
        "  test_df[col] = test_df[col].apply(lambda x: f\"{os.getcwd()}/{DATASET_DIR}/{x}\")"
      ],
      "metadata": {
        "id": "fzf2KfrbEYsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form dictionaries with keys as image paths for custom data generator class"
      ],
      "metadata": {
        "id": "n3M7QJpsvVTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filepath_data_dict(filepath_array, data_array, get_relative_path=True):\n",
        "  \"\"\"\n",
        "  Returns a dictionary with filepaths as keys for data generator\n",
        "  \"\"\"\n",
        "  # Store in a dictionary\n",
        "  output_dict = dict()\n",
        "\n",
        "  for filepath, data in zip(filepath_array, data_array):\n",
        "    if get_relative_path:\n",
        "      filepath = f\"{os.getcwd()}/{DATASET_DIR}/{filepath}\"\n",
        "\n",
        "    output_dict[filepath] = data\n",
        "\n",
        "  return output_dict\n",
        "\n",
        "def create_folder(file_path):\n",
        "  try:\n",
        "    os.makedirs(file_path)\n",
        "  except FileExistsError:\n",
        "    print('Folder exists')"
      ],
      "metadata": {
        "id": "EVl9nElVvNhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate respective dictionaries\n",
        "## X_TRAIN\n",
        "X_train_dict = filepath_data_dict(train_set['colour_filepath'],\n",
        "                                  train_set['feature'])\n",
        "\n",
        "## X_VAL\n",
        "X_val_dict = filepath_data_dict(val_set['colour_filepath'],\n",
        "                                val_set['feature'])\n",
        "\n",
        "## X_TEST\n",
        "X_test_dict = filepath_data_dict(test_set['colour_filepath'],\n",
        "                                 test_set['feature'])\n",
        "\n",
        "## Y_TRAIN\n",
        "y_train_dict = filepath_data_dict(train_set['colour_filepath'],\n",
        "                                  train_set['y_cat'])\n",
        "\n",
        "## Y_VAL\n",
        "y_val_dict = filepath_data_dict(val_set['colour_filepath'],\n",
        "                                val_set['y_cat'])\n",
        "\n",
        "## Y_TEST\n",
        "y_test_dict = filepath_data_dict(test_set['colour_filepath'],\n",
        "                                 test_set['y_cat'])"
      ],
      "metadata": {
        "id": "-Z9K7mjGvfSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All keys in X and y sets have to be identical: \")\n",
        "print(\"Train set\", set(X_train_dict.keys()) == set(train_df[\"colour_filepath\"].values))\n",
        "print(\"Val set\", set(X_val_dict.keys()) == set(val_df[\"colour_filepath\"].values))\n",
        "print(\"Test test\", set(X_test_dict.keys()) == set(test_df[\"colour_filepath\"].values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0BYjh1sE5Tl",
        "outputId": "d98aa2f1-846b-4195-f7f9-e5e67702f9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All keys in X and y sets have to be identical: \n",
            "Train set True\n",
            "Val set True\n",
            "Test test True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All keys in X and y sets have to be identical: \")\n",
        "print(\"Train set\", X_train_dict.keys() == y_train_dict.keys())\n",
        "print(\"Val set\", X_val_dict.keys() == y_val_dict.keys())\n",
        "print(\"Test test\", X_test_dict.keys() == y_test_dict.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDVJcKhjvh2b",
        "outputId": "2386db86-f837-45ba-ea09-c011d3acb7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All keys in X and y sets have to be identical: \n",
            "Train set True\n",
            "Val set True\n",
            "Test test True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjciMp0A3MhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of Functions for Training Models\n",
        "\n",
        "## Common Training Functions\n",
        "\n",
        "- `add_callbacks`: Adds callbacks to customise behaviour of model\n",
        "- `get_class_weights`: Computes class weights for sample balance\n",
        "- `compile_model`: Compiles model\n",
        "- `save_model_archi`: Saves model architecture as image with filename and model as inputs\n",
        "\n",
        "## Functions to train baseline mode\n",
        "\n",
        "- `add_callbacks`\n",
        "- `get_class_weights`\n",
        "- `compile_model`\n",
        "- `build_colour_baseline`: Builds and returns baseline CNN model\n",
        "- `train_baseline_model`: Trains model with inputs from ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "QQtqJvDUu2Hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "## Common Functions\n",
        "\n",
        "- add_callbacks\n",
        "- get_class_weights\n",
        "- save_model_archi\n",
        "- compile_model"
      ],
      "metadata": {
        "id": "sjWt7bj0LhVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorboard"
      ],
      "metadata": {
        "id": "ey0u0YL-LMbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tensorboard to view the network\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "TSU3iyClYN17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_callbacks(patience):\n",
        "  \"\"\"\n",
        "  Returns additional callbacks for model,\n",
        "  arg: patience value\n",
        "  \"\"\"\n",
        "  lr_reduce = ReduceLROnPlateau(monitor='val_loss', mode=\"min\",\n",
        "                                factor=0.2, min_lr=1e-5,\n",
        "                                patience=patience, verbose=1),\n",
        "  early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
        "\n",
        "  return [lr_reduce, early_stopping]\n",
        "\n",
        "\n",
        "def get_class_weights(y_train, is_encoded=False):\n",
        "  \"\"\"\n",
        "  Computes class weights to resolve imbalanced dataset\n",
        "  \"\"\"\n",
        "\n",
        "  #Cconvert from np.array of one-hot encoded labels to integers\n",
        "  if is_encoded:\n",
        "    y_train_encoded = y_train\n",
        "    y_train = [np.argmax(lbl) for lbl in y_train_encoded]\n",
        "\n",
        "  # Compute class weights\n",
        "  class_weights = class_weight.compute_class_weight(\n",
        "      'balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "  # Assign weights for model\n",
        "  class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "  return class_weights_dict\n",
        "\n",
        "def save_model_archi(model, filename):\n",
        "  tf.keras.utils.plot_model(\n",
        "      model,\n",
        "      to_file=\"{}.png\".format(filename),\n",
        "      show_shapes=True,\n",
        "      show_layer_names=True,\n",
        "      show_layer_activations=True\n",
        "  )\n",
        "\n",
        "  print(f\"Saved to {filename}\")"
      ],
      "metadata": {
        "id": "t_grQdkV3C9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to compile model"
      ],
      "metadata": {
        "id": "UgH75Iec2axs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzEDzrz5lQFz"
      },
      "outputs": [],
      "source": [
        "def compile_model(model,\n",
        "                  optimizer,\n",
        "                  loss='categorical_crossentropy'):\n",
        "    \"\"\"\n",
        "    Compiles a model with selected metrics,\n",
        "    args: optimizer, loss\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_k_categorical_accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.FalsePositives(name='false_positives')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Baseline model: Single-input CNN model from Scratch"
      ],
      "metadata": {
        "id": "D4YmP4ul9pac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "BHyzy5A-8vJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_colour_baseline(model_name,\n",
        "                          num_classes=NUM_CLASSES,\n",
        "                          img_size=IMAGE_SIZE):\n",
        "\n",
        "  \"\"\"\n",
        "  Builds and returns baseline CNN model for Colour stream\n",
        "  \"\"\"\n",
        "\n",
        "  print(model_name)\n",
        "\n",
        "  # Create a Sequential model\n",
        "  model= Sequential()\n",
        "  # Define inputs\n",
        "  input_image = Input(shape=(img_size, img_size, 3), name=\"coloured_image\")\n",
        "\n",
        "  # Stack conv layers\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Create fully connected layers\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Add Dense layer\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  # Add a Dropout layer\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  # Add Dense layer\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  # Add a Dropout layer\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Dense(num_classes, activation='softmax', name='predictions'))\n",
        "\n",
        "  # Print model summary\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(\n",
        "  model, model_name,\n",
        "  train_gen, val_gen,\n",
        "  X_train,\n",
        "  X_val,\n",
        "  batch_size,\n",
        "  class_weights,\n",
        "  result_dir,\n",
        "  log_dir,\n",
        "  version,\n",
        "  workers, use_multiprocessing,\n",
        "  epochs=20,\n",
        "  additional_callbacks=[]):\n",
        "  \"\"\"\n",
        "  Trains model with inputs from ImageDataGenerator\n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.exists(os.path.join(result_dir, model_name)):\n",
        "      os.makedirs(os.path.join(result_dir, model_name))\n",
        "\n",
        "  if not os.path.exists(os.path.join(log_dir, model_name)):\n",
        "      os.makedirs(os.path.join(log_dir, model_name))\n",
        "\n",
        "  model_log_dir = os.path.join(log_dir, model_name, f'{model_name}_{version}')\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(model_log_dir, histogram_freq=1)\n",
        "\n",
        "  history = model.fit(\n",
        "      train_gen,\n",
        "      steps_per_epoch=train_gen.n // train_gen.batch_size,\n",
        "      validation_data=val_gen,\n",
        "      validation_steps=val_gen.n // val_gen.batch_size,\n",
        "      class_weight=class_weights,\n",
        "      epochs=epochs,\n",
        "      workers=workers,\n",
        "      use_multiprocessing=use_multiprocessing,\n",
        "      verbose=1,\n",
        "      callbacks=[\n",
        "          tensorboard_callback,\n",
        "          tf.keras.callbacks.TerminateOnNaN(),\n",
        "          tf.keras.callbacks.CSVLogger(os.path.join(result_dir, model_name, f'{model_name}_{version}.log'), separator=',', append=True),\n",
        "          tf.keras.callbacks.ModelCheckpoint(os.path.join(result_dir, model_name, f'{model_name}_{version}'),\n",
        "                                monitor='val_loss', verbose=1,\n",
        "                                save_best_only=True, mode='min')\n",
        "      ] + additional_callbacks\n",
        "  )\n",
        "\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "nrT0slIp3HEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Data Generator"
      ],
      "metadata": {
        "id": "rWwMUsPJ6Jic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN\n",
        "base_train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "base_train_generator = base_train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='colour_filepath',\n",
        "    y_col='ndc9',\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# VAL and TEST\n",
        "base_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "base_val_generator = base_test_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='colour_filepath',\n",
        "    y_col='ndc9',\n",
        "    shuffle=False,\n",
        "    seed=SEED,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "base_test_generator = base_test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='colour_filepath',\n",
        "    y_col='ndc9',\n",
        "    shuffle=False,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPXjCerP9Mf8",
        "outputId": "8e4f92a7-c824-43e0-fac2-12a32635ac8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400 validated image filenames belonging to 40 classes.\n",
            "Found 100 validated image filenames belonging to 40 classes.\n",
            "Found 56 validated image filenames belonging to 40 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset generator if needed\n",
        "base_train_generator.reset()\n",
        "base_val_generator.reset()"
      ],
      "metadata": {
        "id": "_IEUccQJkGl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Train Model"
      ],
      "metadata": {
        "id": "7gOC6z046QHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of model to build\n",
        "model_name = \"Single_Baseline_PP\"\n",
        "\n",
        "# Build baseline model\n",
        "single_baseline_model = build_colour_baseline(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUA1-0kHB0Ar",
        "outputId": "61896a7f-75fb-4396-e78d-c64f111f07ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single_Baseline_PP\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 32)      9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 43264)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               5537920   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 40)                2600      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,577,416\n",
            "Trainable params: 5,577,416\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "single_baseline_model = compile_model(single_baseline_model, optimizer=Adam())\n",
        "\n",
        "# Train model\n",
        "single_baseline_model_history, single_baseline_model = train_model(\n",
        "    single_baseline_model, model_name,\n",
        "    train_gen=base_train_generator,\n",
        "    val_gen=base_val_generator,\n",
        "    X_train=train_df['colour_filepath'].values,\n",
        "    X_val=val_df['colour_filepath'].values,\n",
        "    class_weights=get_class_weights(base_train_generator.classes),\n",
        "    result_dir=BASELINE_CNN_RESULT_DIR,\n",
        "    log_dir=BASELINE_CNN_TB_DIR,\n",
        "    additional_callbacks=add_callbacks(4),\n",
        "    epochs=50,\n",
        "    workers=2,\n",
        "    use_multiprocessing=False,\n",
        "    version='coloured_base_v1',\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_62-uf0CCrss",
        "outputId": "e2f0056f-8df1-473a-ddb8-fd0df06d9f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.7758 - accuracy: 0.0217 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 3.66672, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 10s 852ms/step - loss: 3.7758 - accuracy: 0.0217 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6667 - val_accuracy: 0.1146 - val_top_k_categorical_accuracy: 0.2812 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.6266 - accuracy: 0.0653 - top_k_categorical_accuracy: 0.1932 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 3.66672 to 3.55154, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 784ms/step - loss: 3.6279 - accuracy: 0.0625 - top_k_categorical_accuracy: 0.1957 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5515 - val_accuracy: 0.1250 - val_top_k_categorical_accuracy: 0.3333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.4879 - accuracy: 0.0625 - top_k_categorical_accuracy: 0.2589 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 3.55154 to 3.33199, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 830ms/step - loss: 3.4919 - accuracy: 0.0625 - top_k_categorical_accuracy: 0.2582 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00 - val_loss: 3.3320 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.3958 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.3374 - accuracy: 0.1332 - top_k_categorical_accuracy: 0.3424 - precision: 0.7778 - recall: 0.0190 - false_positives: 2.0000\n",
            "Epoch 4: val_loss improved from 3.33199 to 3.11247, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 770ms/step - loss: 3.3374 - accuracy: 0.1332 - top_k_categorical_accuracy: 0.3424 - precision: 0.7778 - recall: 0.0190 - false_positives: 2.0000 - val_loss: 3.1125 - val_accuracy: 0.2083 - val_top_k_categorical_accuracy: 0.4271 - val_precision: 0.7500 - val_recall: 0.0625 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.2262 - accuracy: 0.1429 - top_k_categorical_accuracy: 0.3958 - precision: 0.5000 - recall: 0.0179 - false_positives: 6.0000\n",
            "Epoch 5: val_loss improved from 3.11247 to 3.01006, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 794ms/step - loss: 3.2234 - accuracy: 0.1440 - top_k_categorical_accuracy: 0.3995 - precision: 0.5714 - recall: 0.0217 - false_positives: 6.0000 - val_loss: 3.0101 - val_accuracy: 0.2083 - val_top_k_categorical_accuracy: 0.5104 - val_precision: 1.0000 - val_recall: 0.0417 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 2.9708 - accuracy: 0.1667 - top_k_categorical_accuracy: 0.4881 - precision: 0.7308 - recall: 0.0565 - false_positives: 7.0000\n",
            "Epoch 6: val_loss improved from 3.01006 to 2.74543, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 8s 756ms/step - loss: 2.9930 - accuracy: 0.1603 - top_k_categorical_accuracy: 0.4755 - precision: 0.7500 - recall: 0.0571 - false_positives: 7.0000 - val_loss: 2.7454 - val_accuracy: 0.2604 - val_top_k_categorical_accuracy: 0.5104 - val_precision: 1.0000 - val_recall: 0.1458 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.7057 - accuracy: 0.2473 - top_k_categorical_accuracy: 0.5897 - precision: 0.7358 - recall: 0.1060 - false_positives: 14.0000\n",
            "Epoch 7: val_loss improved from 2.74543 to 2.56837, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 775ms/step - loss: 2.7057 - accuracy: 0.2473 - top_k_categorical_accuracy: 0.5897 - precision: 0.7358 - recall: 0.1060 - false_positives: 14.0000 - val_loss: 2.5684 - val_accuracy: 0.2917 - val_top_k_categorical_accuracy: 0.5625 - val_precision: 0.8750 - val_recall: 0.1458 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6289 - accuracy: 0.2554 - top_k_categorical_accuracy: 0.5380 - precision: 0.6613 - recall: 0.1114 - false_positives: 21.0000\n",
            "Epoch 8: val_loss improved from 2.56837 to 2.51591, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 828ms/step - loss: 2.6289 - accuracy: 0.2554 - top_k_categorical_accuracy: 0.5380 - precision: 0.6613 - recall: 0.1114 - false_positives: 21.0000 - val_loss: 2.5159 - val_accuracy: 0.2917 - val_top_k_categorical_accuracy: 0.5938 - val_precision: 0.8750 - val_recall: 0.1458 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 2.5704 - accuracy: 0.2470 - top_k_categorical_accuracy: 0.5923 - precision: 0.6780 - recall: 0.1190 - false_positives: 19.0000\n",
            "Epoch 9: val_loss improved from 2.51591 to 2.38006, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 776ms/step - loss: 2.5630 - accuracy: 0.2500 - top_k_categorical_accuracy: 0.5951 - precision: 0.6875 - recall: 0.1196 - false_positives: 20.0000 - val_loss: 2.3801 - val_accuracy: 0.3125 - val_top_k_categorical_accuracy: 0.6042 - val_precision: 1.0000 - val_recall: 0.1667 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.4351 - accuracy: 0.2745 - top_k_categorical_accuracy: 0.6033 - precision: 0.6866 - recall: 0.1250 - false_positives: 21.0000\n",
            "Epoch 10: val_loss did not improve from 2.38006\n",
            "12/12 [==============================] - 7s 638ms/step - loss: 2.4351 - accuracy: 0.2745 - top_k_categorical_accuracy: 0.6033 - precision: 0.6866 - recall: 0.1250 - false_positives: 21.0000 - val_loss: 2.4263 - val_accuracy: 0.3125 - val_top_k_categorical_accuracy: 0.6667 - val_precision: 1.0000 - val_recall: 0.1458 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 2.1779 - accuracy: 0.3601 - top_k_categorical_accuracy: 0.7024 - precision: 0.6737 - recall: 0.1905 - false_positives: 31.0000\n",
            "Epoch 11: val_loss improved from 2.38006 to 2.29465, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 773ms/step - loss: 2.1823 - accuracy: 0.3505 - top_k_categorical_accuracy: 0.6875 - precision: 0.6863 - recall: 0.1902 - false_positives: 32.0000 - val_loss: 2.2946 - val_accuracy: 0.3021 - val_top_k_categorical_accuracy: 0.6771 - val_precision: 0.8500 - val_recall: 0.1771 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 2.0799 - accuracy: 0.3750 - top_k_categorical_accuracy: 0.7262 - precision: 0.6789 - recall: 0.2202 - false_positives: 35.0000\n",
            "Epoch 12: val_loss improved from 2.29465 to 2.18213, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 764ms/step - loss: 2.0522 - accuracy: 0.3750 - top_k_categorical_accuracy: 0.7364 - precision: 0.6942 - recall: 0.2283 - false_positives: 37.0000 - val_loss: 2.1821 - val_accuracy: 0.3854 - val_top_k_categorical_accuracy: 0.6667 - val_precision: 0.8333 - val_recall: 0.2083 - val_false_positives: 4.0000 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0082 - accuracy: 0.3859 - top_k_categorical_accuracy: 0.7255 - precision: 0.7642 - recall: 0.2201 - false_positives: 25.0000\n",
            "Epoch 13: val_loss improved from 2.18213 to 2.16680, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 4s 380ms/step - loss: 2.0082 - accuracy: 0.3859 - top_k_categorical_accuracy: 0.7255 - precision: 0.7642 - recall: 0.2201 - false_positives: 25.0000 - val_loss: 2.1668 - val_accuracy: 0.3750 - val_top_k_categorical_accuracy: 0.7188 - val_precision: 0.8462 - val_recall: 0.2292 - val_false_positives: 4.0000 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9264 - accuracy: 0.4049 - top_k_categorical_accuracy: 0.8098 - precision: 0.7049 - recall: 0.2337 - false_positives: 36.0000\n",
            "Epoch 14: val_loss improved from 2.16680 to 2.15210, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 807ms/step - loss: 1.9264 - accuracy: 0.4049 - top_k_categorical_accuracy: 0.8098 - precision: 0.7049 - recall: 0.2337 - false_positives: 36.0000 - val_loss: 2.1521 - val_accuracy: 0.4062 - val_top_k_categorical_accuracy: 0.7083 - val_precision: 0.7778 - val_recall: 0.2188 - val_false_positives: 6.0000 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.7713 - accuracy: 0.4484 - top_k_categorical_accuracy: 0.7772 - precision: 0.7537 - recall: 0.2745 - false_positives: 33.0000\n",
            "Epoch 15: val_loss did not improve from 2.15210\n",
            "12/12 [==============================] - 7s 647ms/step - loss: 1.7713 - accuracy: 0.4484 - top_k_categorical_accuracy: 0.7772 - precision: 0.7537 - recall: 0.2745 - false_positives: 33.0000 - val_loss: 2.2644 - val_accuracy: 0.3542 - val_top_k_categorical_accuracy: 0.6979 - val_precision: 0.6774 - val_recall: 0.2188 - val_false_positives: 10.0000 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8397 - accuracy: 0.4511 - top_k_categorical_accuracy: 0.7826 - precision: 0.7364 - recall: 0.2582 - false_positives: 34.0000\n",
            "Epoch 16: val_loss improved from 2.15210 to 2.09172, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 780ms/step - loss: 1.8397 - accuracy: 0.4511 - top_k_categorical_accuracy: 0.7826 - precision: 0.7364 - recall: 0.2582 - false_positives: 34.0000 - val_loss: 2.0917 - val_accuracy: 0.4062 - val_top_k_categorical_accuracy: 0.7083 - val_precision: 0.7917 - val_recall: 0.1979 - val_false_positives: 5.0000 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.5720 - accuracy: 0.5190 - top_k_categorical_accuracy: 0.8125 - precision: 0.7812 - recall: 0.3397 - false_positives: 35.0000\n",
            "Epoch 17: val_loss did not improve from 2.09172\n",
            "12/12 [==============================] - 7s 620ms/step - loss: 1.5720 - accuracy: 0.5190 - top_k_categorical_accuracy: 0.8125 - precision: 0.7812 - recall: 0.3397 - false_positives: 35.0000 - val_loss: 2.0947 - val_accuracy: 0.4062 - val_top_k_categorical_accuracy: 0.7396 - val_precision: 0.7419 - val_recall: 0.2396 - val_false_positives: 8.0000 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.4634 - accuracy: 0.5109 - top_k_categorical_accuracy: 0.8723 - precision: 0.7471 - recall: 0.3533 - false_positives: 44.0000\n",
            "Epoch 18: val_loss improved from 2.09172 to 2.05377, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 761ms/step - loss: 1.4634 - accuracy: 0.5109 - top_k_categorical_accuracy: 0.8723 - precision: 0.7471 - recall: 0.3533 - false_positives: 44.0000 - val_loss: 2.0538 - val_accuracy: 0.4271 - val_top_k_categorical_accuracy: 0.7396 - val_precision: 0.6970 - val_recall: 0.2396 - val_false_positives: 10.0000 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 1.5230 - accuracy: 0.5060 - top_k_categorical_accuracy: 0.8631 - precision: 0.7200 - recall: 0.3750 - false_positives: 49.0000\n",
            "Epoch 19: val_loss did not improve from 2.05377\n",
            "12/12 [==============================] - 7s 622ms/step - loss: 1.5034 - accuracy: 0.5245 - top_k_categorical_accuracy: 0.8641 - precision: 0.7382 - recall: 0.3832 - false_positives: 50.0000 - val_loss: 2.0867 - val_accuracy: 0.4062 - val_top_k_categorical_accuracy: 0.7396 - val_precision: 0.6250 - val_recall: 0.2604 - val_false_positives: 15.0000 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.3455 - accuracy: 0.5842 - top_k_categorical_accuracy: 0.8859 - precision: 0.7680 - recall: 0.4049 - false_positives: 45.0000\n",
            "Epoch 20: val_loss improved from 2.05377 to 2.01862, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 794ms/step - loss: 1.3455 - accuracy: 0.5842 - top_k_categorical_accuracy: 0.8859 - precision: 0.7680 - recall: 0.4049 - false_positives: 45.0000 - val_loss: 2.0186 - val_accuracy: 0.4375 - val_top_k_categorical_accuracy: 0.7812 - val_precision: 0.6818 - val_recall: 0.3125 - val_false_positives: 14.0000 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.2378 - accuracy: 0.6060 - top_k_categorical_accuracy: 0.9076 - precision: 0.8086 - recall: 0.4592 - false_positives: 40.0000\n",
            "Epoch 21: val_loss improved from 2.01862 to 1.98956, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/baseline-results-cnn/Single_Baseline_PP/Single_Baseline_PP_coloured_base_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 9s 781ms/step - loss: 1.2378 - accuracy: 0.6060 - top_k_categorical_accuracy: 0.9076 - precision: 0.8086 - recall: 0.4592 - false_positives: 40.0000 - val_loss: 1.9896 - val_accuracy: 0.4479 - val_top_k_categorical_accuracy: 0.7604 - val_precision: 0.6923 - val_recall: 0.2812 - val_false_positives: 12.0000 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 1.2188 - accuracy: 0.5714 - top_k_categorical_accuracy: 0.9167 - precision: 0.8261 - recall: 0.4524 - false_positives: 32.0000\n",
            "Epoch 22: val_loss did not improve from 1.98956\n",
            "12/12 [==============================] - 7s 613ms/step - loss: 1.1939 - accuracy: 0.5870 - top_k_categorical_accuracy: 0.9239 - precision: 0.8284 - recall: 0.4592 - false_positives: 35.0000 - val_loss: 2.1281 - val_accuracy: 0.4167 - val_top_k_categorical_accuracy: 0.7292 - val_precision: 0.6296 - val_recall: 0.3542 - val_false_positives: 20.0000 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.1301 - accuracy: 0.6198 - top_k_categorical_accuracy: 0.9245 - precision: 0.7628 - recall: 0.5026 - false_positives: 60.0000\n",
            "Epoch 23: val_loss did not improve from 1.98956\n",
            "12/12 [==============================] - 7s 631ms/step - loss: 1.1301 - accuracy: 0.6198 - top_k_categorical_accuracy: 0.9245 - precision: 0.7628 - recall: 0.5026 - false_positives: 60.0000 - val_loss: 2.0412 - val_accuracy: 0.3958 - val_top_k_categorical_accuracy: 0.8125 - val_precision: 0.6000 - val_recall: 0.3125 - val_false_positives: 20.0000 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9828 - accuracy: 0.6630 - top_k_categorical_accuracy: 0.9375 - precision: 0.8391 - recall: 0.5245 - false_positives: 37.0000\n",
            "Epoch 24: val_loss did not improve from 1.98956\n",
            "12/12 [==============================] - 2s 195ms/step - loss: 0.9828 - accuracy: 0.6630 - top_k_categorical_accuracy: 0.9375 - precision: 0.8391 - recall: 0.5245 - false_positives: 37.0000 - val_loss: 2.0666 - val_accuracy: 0.4583 - val_top_k_categorical_accuracy: 0.7812 - val_precision: 0.6230 - val_recall: 0.3958 - val_false_positives: 23.0000 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.0391 - accuracy: 0.6793 - top_k_categorical_accuracy: 0.9293 - precision: 0.8268 - recall: 0.5707 - false_positives: 44.0000\n",
            "Epoch 25: val_loss did not improve from 1.98956\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "12/12 [==============================] - 7s 619ms/step - loss: 1.0391 - accuracy: 0.6793 - top_k_categorical_accuracy: 0.9293 - precision: 0.8268 - recall: 0.5707 - false_positives: 44.0000 - val_loss: 2.0360 - val_accuracy: 0.4479 - val_top_k_categorical_accuracy: 0.7812 - val_precision: 0.6154 - val_recall: 0.3333 - val_false_positives: 20.0000 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u7x0VOHH6Gc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Single-input with Transfer Learning & Augmentation\n"
      ],
      "metadata": {
        "id": "4EESG8S2jAqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "S4giBO5O2TwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_model(base_model, model_name, is_trainable,\n",
        "                    drop_out=0.2,\n",
        "                    num_classes=NUM_CLASSES, img_size=IMAGE_SIZE):\n",
        "\n",
        "  print(model_name)\n",
        "\n",
        "  # Freeze convolutional base\n",
        "  if is_trainable == False:\n",
        "    base_model.trainable = False\n",
        "\n",
        "  # Define image inputs\n",
        "  inputs = tf.keras.Input(shape=(img_size, img_size, 3), name=\"coloured_image\")\n",
        "\n",
        "  # Instantiate pre-trained model\n",
        "  if 'MobileNet' in model_name:\n",
        "    # Pre-process input for model\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "\n",
        "  # Instantiate pre-trained model\n",
        "  elif 'ResNet50' in model_name:\n",
        "    # Pre-process input for model\n",
        "    x = tf.keras.applications.resnet_v2.preprocess_input(inputs)\n",
        "\n",
        "  else:\n",
        "    print('Backbone not found')\n",
        "\n",
        "  x = base_model(x, training=False)\n",
        "  # Add a classification layers\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # Add a Dropout layer\n",
        "  x = Dropout(drop_out)(x)\n",
        "  # Add Dense layers\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  # Add Dense layer to output predictions\n",
        "  predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "\n",
        "  # Create final model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
        "  # Print model summary\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "LItGR2E7VkfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: MobileNet V2"
      ],
      "metadata": {
        "id": "qeRoro2PNL6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear existing sessions\n",
        "# tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "QxgjyAV5rk4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset generator\n",
        "base_train_generator.reset()\n",
        "base_val_generator.reset()"
      ],
      "metadata": {
        "id": "F7xfJiwErk4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'MobileNet_Single_PP'\n",
        "\n",
        "# Load MobileNet V2 with ImageNet weights without last fully connected layers\n",
        "mobilenet_base_model = MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                                      weights='imagenet',\n",
        "                                      include_top=False)\n",
        "\n",
        "mobilenet_single_pp = build_cnn_model(mobilenet_base_model, model_name,\n",
        "                             is_trainable=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDgvFLQ6XtXY",
        "outputId": "27e81603-71e2-4d51-961c-a8fa52480dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNet_Single_PP\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " coloured_image (InputLayer)  [(None, 224, 224, 3)]    0         \n",
            "                                                                 \n",
            " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n",
            " a)                                                              \n",
            "                                                                 \n",
            " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1280)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               327936    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 40)                5160      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,623,976\n",
            "Trainable params: 365,992\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "mobilenet_single_pp = compile_model(mobilenet_single_pp, optimizer=Adam())\n",
        "\n",
        "# Train model\n",
        "mobilenet_single_pp_history, mobilenet_single_pp = train_model(\n",
        "    mobilenet_single_pp, model_name,\n",
        "    train_gen=base_train_generator,\n",
        "    val_gen=base_val_generator,\n",
        "    X_train=train_df['colour_filepath'].values,\n",
        "    X_val=val_df['colour_filepath'].values,\n",
        "    class_weights=get_class_weights(base_train_generator.classes),\n",
        "    result_dir=PP_SINGLE_RESULT_DIR,\n",
        "    log_dir=PP_SINGLE_TB_DIR,\n",
        "    additional_callbacks=add_callbacks(4),\n",
        "    epochs=50,\n",
        "    workers=2,\n",
        "    use_multiprocessing=False,\n",
        "    version='v1',\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOVb5fp7VkoE",
        "outputId": "b27f16f2-2ebf-4734-f890-7b6fdc885cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8333 - accuracy: 0.0272 - top_k_categorical_accuracy: 0.0951 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 3.71806, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 3.8333 - accuracy: 0.0272 - top_k_categorical_accuracy: 0.0951 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.7181 - val_accuracy: 0.0208 - val_top_k_categorical_accuracy: 0.1354 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.7372 - accuracy: 0.0149 - top_k_categorical_accuracy: 0.1071 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 3.71806 to 3.70090, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.7376 - accuracy: 0.0136 - top_k_categorical_accuracy: 0.1005 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.7009 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1146 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.7427 - accuracy: 0.0341 - top_k_categorical_accuracy: 0.0966 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 3: val_loss did not improve from 3.70090\n",
            "12/12 [==============================] - 8s 729ms/step - loss: 3.7375 - accuracy: 0.0312 - top_k_categorical_accuracy: 0.0911 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.7033 - val_accuracy: 0.0208 - val_top_k_categorical_accuracy: 0.1354 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.7059 - accuracy: 0.0179 - top_k_categorical_accuracy: 0.1190 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 3.70090 to 3.69515, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 30s 3s/step - loss: 3.7113 - accuracy: 0.0190 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6951 - val_accuracy: 0.0208 - val_top_k_categorical_accuracy: 0.1250 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.7023 - accuracy: 0.0208 - top_k_categorical_accuracy: 0.0982 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 5: val_loss did not improve from 3.69515\n",
            "12/12 [==============================] - 4s 334ms/step - loss: 3.7038 - accuracy: 0.0190 - top_k_categorical_accuracy: 0.0951 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.7021 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1354 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.7157 - accuracy: 0.0245 - top_k_categorical_accuracy: 0.1223 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 6: val_loss improved from 3.69515 to 3.69498, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.7157 - accuracy: 0.0245 - top_k_categorical_accuracy: 0.1223 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6950 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1250 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.7084 - accuracy: 0.0268 - top_k_categorical_accuracy: 0.0952 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 7: val_loss improved from 3.69498 to 3.69480, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 3.7066 - accuracy: 0.0245 - top_k_categorical_accuracy: 0.0951 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6948 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1146 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 3.6859 - accuracy: 0.0199 - top_k_categorical_accuracy: 0.1364 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 8: val_loss improved from 3.69480 to 3.69313, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 3.6930 - accuracy: 0.0182 - top_k_categorical_accuracy: 0.1302 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6931 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1458 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6885 - accuracy: 0.0272 - top_k_categorical_accuracy: 0.1332 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 9: val_loss improved from 3.69313 to 3.68361, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/MobileNet_Single_PP/MobileNet_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 3.6885 - accuracy: 0.0272 - top_k_categorical_accuracy: 0.1332 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6836 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1562 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6954 - accuracy: 0.0217 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 10: val_loss did not improve from 3.68361\n",
            "12/12 [==============================] - 8s 743ms/step - loss: 3.6954 - accuracy: 0.0217 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.7025 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1146 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.7039 - accuracy: 0.0245 - top_k_categorical_accuracy: 0.1033 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 11: val_loss did not improve from 3.68361\n",
            "12/12 [==============================] - 8s 724ms/step - loss: 3.7039 - accuracy: 0.0245 - top_k_categorical_accuracy: 0.1033 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6918 - val_accuracy: 0.0521 - val_top_k_categorical_accuracy: 0.1458 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.7017 - accuracy: 0.0353 - top_k_categorical_accuracy: 0.1440 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 12: val_loss did not improve from 3.68361\n",
            "12/12 [==============================] - 4s 342ms/step - loss: 3.7017 - accuracy: 0.0353 - top_k_categorical_accuracy: 0.1440 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6906 - val_accuracy: 0.0208 - val_top_k_categorical_accuracy: 0.0833 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6865 - accuracy: 0.0109 - top_k_categorical_accuracy: 0.1386 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 13: val_loss did not improve from 3.68361\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "12/12 [==============================] - 8s 713ms/step - loss: 3.6865 - accuracy: 0.0109 - top_k_categorical_accuracy: 0.1386 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6879 - val_accuracy: 0.0521 - val_top_k_categorical_accuracy: 0.1354 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: ResNet-50 V2\n"
      ],
      "metadata": {
        "id": "XGJGTTjlg-VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear existing sessions if needed\n",
        "# tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "HguQUG6evMNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset generator\n",
        "base_train_generator.reset()\n",
        "base_val_generator.reset()"
      ],
      "metadata": {
        "id": "kpqyZYkZvMNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ResNet50_Single_PP'\n",
        "\n",
        "# Load ResNet50 V2 with ImageNet weights without last fully connected layers\n",
        "resnet50_base_model = ResNet50V2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                                      weights='imagenet',\n",
        "                                      include_top=False)\n",
        "\n",
        "resnet50_single_pp = build_cnn_model(resnet50_base_model, model_name,\n",
        "                                     is_trainable=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd172f4-b5cf-4800-dbb4-5f62266a4aee",
        "id": "-C2yDMsJvMNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94668760/94668760 [==============================] - 1s 0us/step\n",
            "ResNet50_Single_PP\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " coloured_image (InputLayer)  [(None, 224, 224, 3)]    0         \n",
            "                                                                 \n",
            " tf.math.truediv_1 (TFOpLamb  (None, 224, 224, 3)      0         \n",
            " da)                                                             \n",
            "                                                                 \n",
            " tf.math.subtract_1 (TFOpLam  (None, 224, 224, 3)      0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 7, 7, 2048)        23564800  \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 40)                5160      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,127,400\n",
            "Trainable params: 562,600\n",
            "Non-trainable params: 23,564,800\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "resnet50_single_pp = compile_model(resnet50_single_pp, optimizer=Adam())\n",
        "\n",
        "# Train model\n",
        "resnet50_single_pp_history, resnet50_single_pp = train_model(\n",
        "    resnet50_single_pp, model_name,\n",
        "    train_gen=base_train_generator,\n",
        "    val_gen=base_val_generator,\n",
        "    X_train=train_df['colour_filepath'].values,\n",
        "    X_val=val_df['colour_filepath'].values,\n",
        "    class_weights=get_class_weights(base_train_generator.classes),\n",
        "    result_dir=PP_SINGLE_RESULT_DIR,\n",
        "    log_dir=PP_SINGLE_TB_DIR,\n",
        "    additional_callbacks=add_callbacks(4),\n",
        "    epochs=50,\n",
        "    workers=2,\n",
        "    use_multiprocessing=False,\n",
        "    version='v1',\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6a00db-0978-4199-cbb4-60e9cbcd0c5f",
        "id": "2rPVGeTAvMNG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8030 - accuracy: 0.0380 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 3.68472, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 39s 3s/step - loss: 3.8030 - accuracy: 0.0380 - top_k_categorical_accuracy: 0.1141 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6847 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1875 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6842 - accuracy: 0.0380 - top_k_categorical_accuracy: 0.1712 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 3.68472 to 3.65060, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.6842 - accuracy: 0.0380 - top_k_categorical_accuracy: 0.1712 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6506 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1979 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6364 - accuracy: 0.0462 - top_k_categorical_accuracy: 0.2092 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 3.65060 to 3.63268, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 36s 3s/step - loss: 3.6364 - accuracy: 0.0462 - top_k_categorical_accuracy: 0.2092 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.6327 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1979 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6178 - accuracy: 0.0462 - top_k_categorical_accuracy: 0.2120 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 3.63268 to 3.57354, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 3.6178 - accuracy: 0.0462 - top_k_categorical_accuracy: 0.2120 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5735 - val_accuracy: 0.0625 - val_top_k_categorical_accuracy: 0.3125 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.5709 - accuracy: 0.0734 - top_k_categorical_accuracy: 0.2337 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 5: val_loss improved from 3.57354 to 3.50925, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.5709 - accuracy: 0.0734 - top_k_categorical_accuracy: 0.2337 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5093 - val_accuracy: 0.0729 - val_top_k_categorical_accuracy: 0.2604 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.4372 - accuracy: 0.1005 - top_k_categorical_accuracy: 0.3179 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 6: val_loss improved from 3.50925 to 3.43225, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.4372 - accuracy: 0.1005 - top_k_categorical_accuracy: 0.3179 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.4323 - val_accuracy: 0.0625 - val_top_k_categorical_accuracy: 0.2917 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.3739 - accuracy: 0.0625 - top_k_categorical_accuracy: 0.3043 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 7: val_loss improved from 3.43225 to 3.33236, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 3.3739 - accuracy: 0.0625 - top_k_categorical_accuracy: 0.3043 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.3324 - val_accuracy: 0.0833 - val_top_k_categorical_accuracy: 0.3542 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.2234 - accuracy: 0.1196 - top_k_categorical_accuracy: 0.3832 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 8: val_loss improved from 3.33236 to 3.25758, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.2234 - accuracy: 0.1196 - top_k_categorical_accuracy: 0.3832 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.2576 - val_accuracy: 0.0625 - val_top_k_categorical_accuracy: 0.3438 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.1301 - accuracy: 0.1087 - top_k_categorical_accuracy: 0.4076 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 9: val_loss improved from 3.25758 to 3.18171, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.1301 - accuracy: 0.1087 - top_k_categorical_accuracy: 0.4076 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.1817 - val_accuracy: 0.1250 - val_top_k_categorical_accuracy: 0.3542 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.0483 - accuracy: 0.0978 - top_k_categorical_accuracy: 0.4158 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 10: val_loss improved from 3.18171 to 3.09543, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.0483 - accuracy: 0.0978 - top_k_categorical_accuracy: 0.4158 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.0954 - val_accuracy: 0.1458 - val_top_k_categorical_accuracy: 0.4062 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.0243 - accuracy: 0.1386 - top_k_categorical_accuracy: 0.4402 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00\n",
            "Epoch 11: val_loss did not improve from 3.09543\n",
            "12/12 [==============================] - 7s 615ms/step - loss: 3.0243 - accuracy: 0.1386 - top_k_categorical_accuracy: 0.4402 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00 - val_loss: 3.1219 - val_accuracy: 0.1146 - val_top_k_categorical_accuracy: 0.3333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.9018 - accuracy: 0.1576 - top_k_categorical_accuracy: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 12: val_loss improved from 3.09543 to 3.02248, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.9018 - accuracy: 0.1576 - top_k_categorical_accuracy: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.0225 - val_accuracy: 0.1771 - val_top_k_categorical_accuracy: 0.3958 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.8585 - accuracy: 0.1902 - top_k_categorical_accuracy: 0.4755 - precision: 0.5000 - recall: 0.0027 - false_positives: 1.0000    \n",
            "Epoch 13: val_loss improved from 3.02248 to 2.98683, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.8585 - accuracy: 0.1902 - top_k_categorical_accuracy: 0.4755 - precision: 0.5000 - recall: 0.0027 - false_positives: 1.0000 - val_loss: 2.9868 - val_accuracy: 0.1042 - val_top_k_categorical_accuracy: 0.4375 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.8096 - accuracy: 0.2065 - top_k_categorical_accuracy: 0.5000 - precision: 1.0000 - recall: 0.0054 - false_positives: 0.0000e+00\n",
            "Epoch 14: val_loss improved from 2.98683 to 2.94666, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.8096 - accuracy: 0.2065 - top_k_categorical_accuracy: 0.5000 - precision: 1.0000 - recall: 0.0054 - false_positives: 0.0000e+00 - val_loss: 2.9467 - val_accuracy: 0.1458 - val_top_k_categorical_accuracy: 0.3854 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.7513 - accuracy: 0.2391 - top_k_categorical_accuracy: 0.5326 - precision: 1.0000 - recall: 0.0109 - false_positives: 0.0000e+00\n",
            "Epoch 15: val_loss improved from 2.94666 to 2.94089, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.7513 - accuracy: 0.2391 - top_k_categorical_accuracy: 0.5326 - precision: 1.0000 - recall: 0.0109 - false_positives: 0.0000e+00 - val_loss: 2.9409 - val_accuracy: 0.1250 - val_top_k_categorical_accuracy: 0.4479 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.7339 - accuracy: 0.2031 - top_k_categorical_accuracy: 0.5573 - precision: 1.0000 - recall: 0.0078 - false_positives: 0.0000e+00\n",
            "Epoch 16: val_loss improved from 2.94089 to 2.89921, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.7339 - accuracy: 0.2031 - top_k_categorical_accuracy: 0.5573 - precision: 1.0000 - recall: 0.0078 - false_positives: 0.0000e+00 - val_loss: 2.8992 - val_accuracy: 0.1042 - val_top_k_categorical_accuracy: 0.5104 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.7233 - accuracy: 0.1848 - top_k_categorical_accuracy: 0.5516 - precision: 1.0000 - recall: 0.0163 - false_positives: 0.0000e+00\n",
            "Epoch 17: val_loss improved from 2.89921 to 2.89221, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.7233 - accuracy: 0.1848 - top_k_categorical_accuracy: 0.5516 - precision: 1.0000 - recall: 0.0163 - false_positives: 0.0000e+00 - val_loss: 2.8922 - val_accuracy: 0.1458 - val_top_k_categorical_accuracy: 0.4479 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6104 - accuracy: 0.2228 - top_k_categorical_accuracy: 0.5788 - precision: 1.0000 - recall: 0.0136 - false_positives: 0.0000e+00\n",
            "Epoch 18: val_loss improved from 2.89221 to 2.85963, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.6104 - accuracy: 0.2228 - top_k_categorical_accuracy: 0.5788 - precision: 1.0000 - recall: 0.0136 - false_positives: 0.0000e+00 - val_loss: 2.8596 - val_accuracy: 0.1458 - val_top_k_categorical_accuracy: 0.4688 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5996 - accuracy: 0.2337 - top_k_categorical_accuracy: 0.5679 - precision: 1.0000 - recall: 0.0272 - false_positives: 0.0000e+00\n",
            "Epoch 19: val_loss did not improve from 2.85963\n",
            "12/12 [==============================] - 7s 602ms/step - loss: 2.5996 - accuracy: 0.2337 - top_k_categorical_accuracy: 0.5679 - precision: 1.0000 - recall: 0.0272 - false_positives: 0.0000e+00 - val_loss: 2.8790 - val_accuracy: 0.1458 - val_top_k_categorical_accuracy: 0.4896 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5496 - accuracy: 0.2418 - top_k_categorical_accuracy: 0.6005 - precision: 0.9286 - recall: 0.0353 - false_positives: 1.0000\n",
            "Epoch 20: val_loss improved from 2.85963 to 2.79297, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.5496 - accuracy: 0.2418 - top_k_categorical_accuracy: 0.6005 - precision: 0.9286 - recall: 0.0353 - false_positives: 1.0000 - val_loss: 2.7930 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.5312 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.4939 - accuracy: 0.2717 - top_k_categorical_accuracy: 0.6277 - precision: 0.8235 - recall: 0.0380 - false_positives: 3.0000\n",
            "Epoch 21: val_loss did not improve from 2.79297\n",
            "12/12 [==============================] - 7s 586ms/step - loss: 2.4939 - accuracy: 0.2717 - top_k_categorical_accuracy: 0.6277 - precision: 0.8235 - recall: 0.0380 - false_positives: 3.0000 - val_loss: 2.8322 - val_accuracy: 0.1354 - val_top_k_categorical_accuracy: 0.4896 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5083 - accuracy: 0.2446 - top_k_categorical_accuracy: 0.6005 - precision: 0.9231 - recall: 0.0326 - false_positives: 1.0000\n",
            "Epoch 22: val_loss did not improve from 2.79297\n",
            "12/12 [==============================] - 7s 574ms/step - loss: 2.5083 - accuracy: 0.2446 - top_k_categorical_accuracy: 0.6005 - precision: 0.9231 - recall: 0.0326 - false_positives: 1.0000 - val_loss: 2.8059 - val_accuracy: 0.1875 - val_top_k_categorical_accuracy: 0.5104 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.4880 - accuracy: 0.2446 - top_k_categorical_accuracy: 0.6196 - precision: 0.8125 - recall: 0.0353 - false_positives: 3.0000\n",
            "Epoch 23: val_loss improved from 2.79297 to 2.74249, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.4880 - accuracy: 0.2446 - top_k_categorical_accuracy: 0.6196 - precision: 0.8125 - recall: 0.0353 - false_positives: 3.0000 - val_loss: 2.7425 - val_accuracy: 0.2188 - val_top_k_categorical_accuracy: 0.4792 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.4161 - accuracy: 0.2554 - top_k_categorical_accuracy: 0.6658 - precision: 0.8824 - recall: 0.0408 - false_positives: 2.0000\n",
            "Epoch 24: val_loss did not improve from 2.74249\n",
            "12/12 [==============================] - 11s 1s/step - loss: 2.4161 - accuracy: 0.2554 - top_k_categorical_accuracy: 0.6658 - precision: 0.8824 - recall: 0.0408 - false_positives: 2.0000 - val_loss: 2.7619 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.5521 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.3724 - accuracy: 0.2908 - top_k_categorical_accuracy: 0.6739 - precision: 0.9167 - recall: 0.0598 - false_positives: 2.0000\n",
            "Epoch 25: val_loss did not improve from 2.74249\n",
            "12/12 [==============================] - 7s 592ms/step - loss: 2.3724 - accuracy: 0.2908 - top_k_categorical_accuracy: 0.6739 - precision: 0.9167 - recall: 0.0598 - false_positives: 2.0000 - val_loss: 2.7591 - val_accuracy: 0.1562 - val_top_k_categorical_accuracy: 0.5312 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.3989 - accuracy: 0.2935 - top_k_categorical_accuracy: 0.6467 - precision: 0.9130 - recall: 0.0571 - false_positives: 2.0000\n",
            "Epoch 26: val_loss improved from 2.74249 to 2.68958, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.3989 - accuracy: 0.2935 - top_k_categorical_accuracy: 0.6467 - precision: 0.9130 - recall: 0.0571 - false_positives: 2.0000 - val_loss: 2.6896 - val_accuracy: 0.1979 - val_top_k_categorical_accuracy: 0.6146 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2991 - accuracy: 0.3179 - top_k_categorical_accuracy: 0.6739 - precision: 0.8400 - recall: 0.0571 - false_positives: 4.0000\n",
            "Epoch 27: val_loss did not improve from 2.68958\n",
            "12/12 [==============================] - 11s 1s/step - loss: 2.2991 - accuracy: 0.3179 - top_k_categorical_accuracy: 0.6739 - precision: 0.8400 - recall: 0.0571 - false_positives: 4.0000 - val_loss: 2.6931 - val_accuracy: 0.1875 - val_top_k_categorical_accuracy: 0.5312 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2290 - accuracy: 0.3315 - top_k_categorical_accuracy: 0.6929 - precision: 0.9286 - recall: 0.0707 - false_positives: 2.0000\n",
            "Epoch 28: val_loss improved from 2.68958 to 2.68128, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.2290 - accuracy: 0.3315 - top_k_categorical_accuracy: 0.6929 - precision: 0.9286 - recall: 0.0707 - false_positives: 2.0000 - val_loss: 2.6813 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.5312 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.3049 - accuracy: 0.3098 - top_k_categorical_accuracy: 0.6413 - precision: 0.9688 - recall: 0.0842 - false_positives: 1.0000\n",
            "Epoch 29: val_loss did not improve from 2.68128\n",
            "12/12 [==============================] - 11s 980ms/step - loss: 2.3049 - accuracy: 0.3098 - top_k_categorical_accuracy: 0.6413 - precision: 0.9688 - recall: 0.0842 - false_positives: 1.0000 - val_loss: 2.7982 - val_accuracy: 0.1771 - val_top_k_categorical_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 0.0104 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2976 - accuracy: 0.2962 - top_k_categorical_accuracy: 0.6739 - precision: 0.8889 - recall: 0.0652 - false_positives: 3.0000\n",
            "Epoch 30: val_loss improved from 2.68128 to 2.65399, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.2976 - accuracy: 0.2962 - top_k_categorical_accuracy: 0.6739 - precision: 0.8889 - recall: 0.0652 - false_positives: 3.0000 - val_loss: 2.6540 - val_accuracy: 0.2083 - val_top_k_categorical_accuracy: 0.5729 - val_precision: 0.6667 - val_recall: 0.0208 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2291 - accuracy: 0.3152 - top_k_categorical_accuracy: 0.6957 - precision: 0.8824 - recall: 0.0815 - false_positives: 4.0000\n",
            "Epoch 31: val_loss improved from 2.65399 to 2.64447, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.2291 - accuracy: 0.3152 - top_k_categorical_accuracy: 0.6957 - precision: 0.8824 - recall: 0.0815 - false_positives: 4.0000 - val_loss: 2.6445 - val_accuracy: 0.2188 - val_top_k_categorical_accuracy: 0.5729 - val_precision: 0.5000 - val_recall: 0.0104 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2532 - accuracy: 0.2935 - top_k_categorical_accuracy: 0.7092 - precision: 0.8750 - recall: 0.0951 - false_positives: 5.0000\n",
            "Epoch 32: val_loss did not improve from 2.64447\n",
            "12/12 [==============================] - 7s 592ms/step - loss: 2.2532 - accuracy: 0.2935 - top_k_categorical_accuracy: 0.7092 - precision: 0.8750 - recall: 0.0951 - false_positives: 5.0000 - val_loss: 2.6821 - val_accuracy: 0.2708 - val_top_k_categorical_accuracy: 0.5208 - val_precision: 1.0000 - val_recall: 0.0312 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.1744 - accuracy: 0.3397 - top_k_categorical_accuracy: 0.7337 - precision: 0.8182 - recall: 0.0978 - false_positives: 8.0000\n",
            "Epoch 33: val_loss did not improve from 2.64447\n",
            "12/12 [==============================] - 7s 589ms/step - loss: 2.1744 - accuracy: 0.3397 - top_k_categorical_accuracy: 0.7337 - precision: 0.8182 - recall: 0.0978 - false_positives: 8.0000 - val_loss: 2.7577 - val_accuracy: 0.1979 - val_top_k_categorical_accuracy: 0.5938 - val_precision: 0.7500 - val_recall: 0.0312 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2267 - accuracy: 0.3071 - top_k_categorical_accuracy: 0.6821 - precision: 0.7778 - recall: 0.0951 - false_positives: 10.0000\n",
            "Epoch 34: val_loss improved from 2.64447 to 2.62309, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.2267 - accuracy: 0.3071 - top_k_categorical_accuracy: 0.6821 - precision: 0.7778 - recall: 0.0951 - false_positives: 10.0000 - val_loss: 2.6231 - val_accuracy: 0.2604 - val_top_k_categorical_accuracy: 0.5625 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.1774 - accuracy: 0.3071 - top_k_categorical_accuracy: 0.6984 - precision: 0.8667 - recall: 0.0707 - false_positives: 4.0000\n",
            "Epoch 35: val_loss did not improve from 2.62309\n",
            "12/12 [==============================] - 6s 567ms/step - loss: 2.1774 - accuracy: 0.3071 - top_k_categorical_accuracy: 0.6984 - precision: 0.8667 - recall: 0.0707 - false_positives: 4.0000 - val_loss: 2.6494 - val_accuracy: 0.2188 - val_top_k_categorical_accuracy: 0.5938 - val_precision: 0.5000 - val_recall: 0.0104 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.1279 - accuracy: 0.3614 - top_k_categorical_accuracy: 0.6766 - precision: 0.8182 - recall: 0.0978 - false_positives: 8.0000\n",
            "Epoch 36: val_loss did not improve from 2.62309\n",
            "12/12 [==============================] - 7s 605ms/step - loss: 2.1279 - accuracy: 0.3614 - top_k_categorical_accuracy: 0.6766 - precision: 0.8182 - recall: 0.0978 - false_positives: 8.0000 - val_loss: 2.6262 - val_accuracy: 0.2500 - val_top_k_categorical_accuracy: 0.5729 - val_precision: 0.5000 - val_recall: 0.0208 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0458 - accuracy: 0.3587 - top_k_categorical_accuracy: 0.7554 - precision: 0.8043 - recall: 0.1005 - false_positives: 9.0000\n",
            "Epoch 37: val_loss improved from 2.62309 to 2.58251, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.0458 - accuracy: 0.3587 - top_k_categorical_accuracy: 0.7554 - precision: 0.8043 - recall: 0.1005 - false_positives: 9.0000 - val_loss: 2.5825 - val_accuracy: 0.1875 - val_top_k_categorical_accuracy: 0.6250 - val_precision: 0.8571 - val_recall: 0.0625 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0941 - accuracy: 0.3614 - top_k_categorical_accuracy: 0.7337 - precision: 0.8250 - recall: 0.0897 - false_positives: 7.0000\n",
            "Epoch 38: val_loss improved from 2.58251 to 2.57943, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.0941 - accuracy: 0.3614 - top_k_categorical_accuracy: 0.7337 - precision: 0.8250 - recall: 0.0897 - false_positives: 7.0000 - val_loss: 2.5794 - val_accuracy: 0.2500 - val_top_k_categorical_accuracy: 0.6562 - val_precision: 0.6667 - val_recall: 0.0208 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0234 - accuracy: 0.3967 - top_k_categorical_accuracy: 0.7717 - precision: 0.9074 - recall: 0.1332 - false_positives: 5.0000\n",
            "Epoch 39: val_loss improved from 2.57943 to 2.56909, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.0234 - accuracy: 0.3967 - top_k_categorical_accuracy: 0.7717 - precision: 0.9074 - recall: 0.1332 - false_positives: 5.0000 - val_loss: 2.5691 - val_accuracy: 0.2500 - val_top_k_categorical_accuracy: 0.6042 - val_precision: 0.6000 - val_recall: 0.0625 - val_false_positives: 4.0000 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0237 - accuracy: 0.3777 - top_k_categorical_accuracy: 0.7473 - precision: 0.8113 - recall: 0.1168 - false_positives: 10.0000\n",
            "Epoch 40: val_loss did not improve from 2.56909\n",
            "12/12 [==============================] - 7s 616ms/step - loss: 2.0237 - accuracy: 0.3777 - top_k_categorical_accuracy: 0.7473 - precision: 0.8113 - recall: 0.1168 - false_positives: 10.0000 - val_loss: 2.6038 - val_accuracy: 0.1979 - val_top_k_categorical_accuracy: 0.6354 - val_precision: 0.6250 - val_recall: 0.0521 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9870 - accuracy: 0.3646 - top_k_categorical_accuracy: 0.7865 - precision: 0.9184 - recall: 0.1172 - false_positives: 4.0000\n",
            "Epoch 41: val_loss improved from 2.56909 to 2.53526, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 1.9870 - accuracy: 0.3646 - top_k_categorical_accuracy: 0.7865 - precision: 0.9184 - recall: 0.1172 - false_positives: 4.0000 - val_loss: 2.5353 - val_accuracy: 0.2083 - val_top_k_categorical_accuracy: 0.5833 - val_precision: 0.7500 - val_recall: 0.0625 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0357 - accuracy: 0.3696 - top_k_categorical_accuracy: 0.7527 - precision: 0.8889 - recall: 0.1304 - false_positives: 6.0000\n",
            "Epoch 42: val_loss did not improve from 2.53526\n",
            "12/12 [==============================] - 11s 1s/step - loss: 2.0357 - accuracy: 0.3696 - top_k_categorical_accuracy: 0.7527 - precision: 0.8889 - recall: 0.1304 - false_positives: 6.0000 - val_loss: 2.5531 - val_accuracy: 0.2500 - val_top_k_categorical_accuracy: 0.6250 - val_precision: 0.7000 - val_recall: 0.0729 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0097 - accuracy: 0.3723 - top_k_categorical_accuracy: 0.7391 - precision: 0.9206 - recall: 0.1576 - false_positives: 5.0000\n",
            "Epoch 43: val_loss improved from 2.53526 to 2.51825, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 2.0097 - accuracy: 0.3723 - top_k_categorical_accuracy: 0.7391 - precision: 0.9206 - recall: 0.1576 - false_positives: 5.0000 - val_loss: 2.5183 - val_accuracy: 0.2917 - val_top_k_categorical_accuracy: 0.6354 - val_precision: 0.7500 - val_recall: 0.0625 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9395 - accuracy: 0.3995 - top_k_categorical_accuracy: 0.7636 - precision: 0.8906 - recall: 0.1549 - false_positives: 7.0000\n",
            "Epoch 44: val_loss did not improve from 2.51825\n",
            "12/12 [==============================] - 7s 585ms/step - loss: 1.9395 - accuracy: 0.3995 - top_k_categorical_accuracy: 0.7636 - precision: 0.8906 - recall: 0.1549 - false_positives: 7.0000 - val_loss: 2.5388 - val_accuracy: 0.2188 - val_top_k_categorical_accuracy: 0.5729 - val_precision: 0.7778 - val_recall: 0.0729 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9316 - accuracy: 0.4158 - top_k_categorical_accuracy: 0.7962 - precision: 0.8364 - recall: 0.1250 - false_positives: 9.0000\n",
            "Epoch 45: val_loss improved from 2.51825 to 2.49148, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 1.9316 - accuracy: 0.4158 - top_k_categorical_accuracy: 0.7962 - precision: 0.8364 - recall: 0.1250 - false_positives: 9.0000 - val_loss: 2.4915 - val_accuracy: 0.2500 - val_top_k_categorical_accuracy: 0.6771 - val_precision: 0.8571 - val_recall: 0.0625 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9244 - accuracy: 0.3995 - top_k_categorical_accuracy: 0.7391 - precision: 0.9508 - recall: 0.1576 - false_positives: 3.0000\n",
            "Epoch 46: val_loss did not improve from 2.49148\n",
            "12/12 [==============================] - 7s 624ms/step - loss: 1.9244 - accuracy: 0.3995 - top_k_categorical_accuracy: 0.7391 - precision: 0.9508 - recall: 0.1576 - false_positives: 3.0000 - val_loss: 2.4969 - val_accuracy: 0.2917 - val_top_k_categorical_accuracy: 0.6250 - val_precision: 0.7000 - val_recall: 0.0729 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8782 - accuracy: 0.4049 - top_k_categorical_accuracy: 0.7880 - precision: 0.8841 - recall: 0.1658 - false_positives: 8.0000\n",
            "Epoch 47: val_loss improved from 2.49148 to 2.48661, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 30s 3s/step - loss: 1.8782 - accuracy: 0.4049 - top_k_categorical_accuracy: 0.7880 - precision: 0.8841 - recall: 0.1658 - false_positives: 8.0000 - val_loss: 2.4866 - val_accuracy: 0.2292 - val_top_k_categorical_accuracy: 0.6250 - val_precision: 0.7778 - val_recall: 0.0729 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8140 - accuracy: 0.4429 - top_k_categorical_accuracy: 0.8043 - precision: 0.9403 - recall: 0.1712 - false_positives: 4.0000\n",
            "Epoch 48: val_loss improved from 2.48661 to 2.48005, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 1.8140 - accuracy: 0.4429 - top_k_categorical_accuracy: 0.8043 - precision: 0.9403 - recall: 0.1712 - false_positives: 4.0000 - val_loss: 2.4800 - val_accuracy: 0.2812 - val_top_k_categorical_accuracy: 0.6354 - val_precision: 0.8000 - val_recall: 0.0833 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8157 - accuracy: 0.4212 - top_k_categorical_accuracy: 0.7989 - precision: 0.8148 - recall: 0.1793 - false_positives: 15.0000\n",
            "Epoch 49: val_loss improved from 2.48005 to 2.47921, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 1.8157 - accuracy: 0.4212 - top_k_categorical_accuracy: 0.7989 - precision: 0.8148 - recall: 0.1793 - false_positives: 15.0000 - val_loss: 2.4792 - val_accuracy: 0.3021 - val_top_k_categorical_accuracy: 0.6875 - val_precision: 0.6250 - val_recall: 0.0521 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8044 - accuracy: 0.4647 - top_k_categorical_accuracy: 0.8098 - precision: 0.8732 - recall: 0.1685 - false_positives: 9.0000\n",
            "Epoch 50: val_loss improved from 2.47921 to 2.46342, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/single-results-pp/ResNet50_Single_PP/ResNet50_Single_PP_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 33s 3s/step - loss: 1.8044 - accuracy: 0.4647 - top_k_categorical_accuracy: 0.8098 - precision: 0.8732 - recall: 0.1685 - false_positives: 9.0000 - val_loss: 2.4634 - val_accuracy: 0.2396 - val_top_k_categorical_accuracy: 0.6562 - val_precision: 0.7692 - val_recall: 0.1042 - val_false_positives: 3.0000 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (3) Multi-input with Transfer Learning, Augmentation, & Descriptors\n",
        "\n",
        "Two inputs:\n",
        "1. Preprocessed image for Colour stream\n",
        "2. Feature vector for Texture stream"
      ],
      "metadata": {
        "id": "Kxp97POF0HWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Mixed-type Data Generator"
      ],
      "metadata": {
        "id": "ZzgBZ5Rm3Avg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self,\n",
        "               image_filepaths, feature_vectors, labels,\n",
        "               batch_size=BATCH_SIZE, num_classes=NUM_CLASSES,\n",
        "               image_dim=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "               is_train=True, is_shuffle=False, is_augment=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Custom data generator that gives [image array, feature vectors], label as output\n",
        "\n",
        "    Args:\n",
        "    image_filepaths: path to images (string)\n",
        "    feature_vectors: LBP and HOG feature vector (numpy array)\n",
        "    labels: labels or class (integer)\n",
        "    is_train: True if input data is a train set\n",
        "    is_shuffle: True to shuffle input data and if it is a train set\n",
        "    is_augment: True to perform data augmentation and if it is a train set\n",
        "    \"\"\"\n",
        "\n",
        "    # Define variables\n",
        "    self.image_filepaths = image_filepaths\n",
        "    self.feature_vectors = feature_vectors\n",
        "    self.labels = labels\n",
        "\n",
        "    self.image_dim = image_dim\n",
        "    self.batch_size = batch_size\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.is_train = is_train\n",
        "    self.is_augment = is_augment\n",
        "    self.is_shuffle = is_shuffle\n",
        "\n",
        "    self.indices = np.arange(len(image_filepaths))\n",
        "\n",
        "    # Prevent shuffling and augmentation for generation of validation set\n",
        "    if is_train == False:\n",
        "      self.is_shuffle = False\n",
        "      self.is_augment = False\n",
        "\n",
        "    if self.is_train:\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    # Define augmentations to perform on images\n",
        "    self.iaa_augs = iaa.Sequential(\n",
        "        [\n",
        "            # Rotate the Images\n",
        "            iaa.Affine(rotate=(-15, 15)),\n",
        "            # Scale the Images\n",
        "            iaa.Affine(scale=(0.5, 1.5)),\n",
        "            # Flip the Images\n",
        "            iaa.Fliplr(0.5),\n",
        "            # Add Gaussian Blur\n",
        "            iaa.GaussianBlur((1.0, 2.0)),\n",
        "            # Add Gaussian Noise\n",
        "            iaa.AdditiveGaussianNoise(scale=0.01*255, per_channel=True),\n",
        "            # Multiply all pixels with a specific value\n",
        "            iaa.Multiply((0.8, 1.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"Found {len(self.image_filepaths)} images belonging to {self.num_classes} classes\")\n",
        "    print(f\"Found {len(self.feature_vectors)} feature vectors belonging to {self.num_classes} classes\")\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Returns number of input data per batch\n",
        "    \"\"\"\n",
        "    return int(np.floor(len(self.image_filepaths) / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Generates a batch of data in the form of X, y\n",
        "    \"\"\"\n",
        "    # Get indices of the batch\n",
        "    indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "    # Get list of IDs\n",
        "    img_files_temp = [self.image_filepaths[i] for i in indices]\n",
        "\n",
        "    # Generate data\n",
        "    X, y = self.__data_generate(img_files_temp)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "  def next(self):\n",
        "    return self.__next__()\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    \"\"\"\n",
        "    Shuffle batch of data at every epoch\n",
        "    \"\"\"\n",
        "    self.indices = np.arange(len(self.image_filepaths))\n",
        "    if(self.is_train):\n",
        "      np.random.shuffle(self.indices)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  def shuffle_data(self, x, y):\n",
        "    \"\"\"\n",
        "    Shuffle data set\n",
        "\n",
        "    x: a list of two numpy arrays [images, vectors],\n",
        "    where images = x[0], vectors=x[1],\n",
        "    images and vectors each is an array of size = self.batch_size\n",
        "    \"\"\"\n",
        "    lam = np.random.beta(0.2, 0.4)\n",
        "    ori_index = np.arange(int(len(x[0])))\n",
        "    index_array = np.arange(int(len(x[0])))\n",
        "    np.random.shuffle(index_array)\n",
        "\n",
        "    shuffled_x = [lam * x[0][ori_index] + (1 - lam) * x[0][index_array],\n",
        "                  lam * x[1][ori_index] + (1 - lam) * x[1][index_array]]\n",
        "    shuffled_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
        "\n",
        "    return shuffled_x, shuffled_y\n",
        "\n",
        "  def __data_generate(self, img_files_temp):\n",
        "    \"\"\"\n",
        "    Generates batches of data\n",
        "    \"\"\"\n",
        "    X_img = []\n",
        "    X_features = []\n",
        "    y = [0] * self.batch_size\n",
        "\n",
        "    # Generate data\n",
        "    for i, img_file in enumerate(img_files_temp):\n",
        "      # Read image\n",
        "      img = cv2.imread(img_file)\n",
        "\n",
        "      # Apply image augmentation\n",
        "      if self.is_augment:\n",
        "          img = self.iaa_augs.augment_image(img)\n",
        "\n",
        "      X_img.append(img)\n",
        "      X_features.append(self.feature_vectors[img_file])\n",
        "\n",
        "      y[i] = self.labels[img_file]\n",
        "\n",
        "    X = [np.array(X_img, np.float32), np.array(X_features, np.float32)]\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    # y = tf.keras.utils.to_categorical(y, num_classes=self.num_classes)\n",
        "    # print(y)\n",
        "\n",
        "    if self.is_shuffle:\n",
        "        X, y = self.shuffle_data(X, y)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "2bOOmU8X6Zik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_train_datagen = CustomDataGenerator(image_filepaths=list(X_train_dict.keys()),\n",
        "                                           feature_vectors=X_train_dict,\n",
        "                                           labels=y_train_dict,\n",
        "                                           is_train=True, is_augment=True, is_shuffle=True)\n",
        "custom_val_datagen = CustomDataGenerator(image_filepaths=list(X_val_dict.keys()),\n",
        "                                         feature_vectors=X_val_dict,\n",
        "                                         labels=y_val_dict,\n",
        "                                         is_train=False, is_augment=False, is_shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM8VQv_V6K7q",
        "outputId": "c0e12ef9-c275-4188-85d8-5a87be73f6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400 images belonging to 40 classes\n",
            "Found 400 feature vectors belonging to 40 classes\n",
            "Found 100 images belonging to 40 classes\n",
            "Found 100 feature vectors belonging to 40 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Functions"
      ],
      "metadata": {
        "id": "tOxe87zz642z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fusion_model(\n",
        "  model, model_name,\n",
        "  train_gen, val_gen, batch_size,\n",
        "  X_train,\n",
        "  X_val,\n",
        "  class_weights,\n",
        "  result_dir,\n",
        "  log_dir,\n",
        "  version,\n",
        "  workers, use_multiprocessing,\n",
        "  initial_epoch=0, epochs=20,\n",
        "  initial_value_threshold=None,\n",
        "  additional_callbacks=[]):\n",
        "  \"\"\"\n",
        "  Trains fusion model with inputs from custom data generator (CustomDataGenerator)\n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.exists(os.path.join(result_dir, model_name)):\n",
        "      os.makedirs(os.path.join(result_dir, model_name))\n",
        "\n",
        "  if not os.path.exists(os.path.join(log_dir, model_name)):\n",
        "      os.makedirs(os.path.join(log_dir, model_name))\n",
        "\n",
        "  model_log_dir = os.path.join(log_dir, model_name, f'{model_name}_{version}')\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(model_log_dir, histogram_freq=1)\n",
        "\n",
        "  history = model.fit(\n",
        "      train_gen,\n",
        "      steps_per_epoch=len(train_gen.image_filepaths) // custom_train_datagen.batch_size,\n",
        "      validation_data=val_gen,\n",
        "      validation_steps=len(val_gen.image_filepaths) // val_gen.batch_size,\n",
        "      class_weight=class_weights,\n",
        "      epochs=epochs,\n",
        "      initial_epoch=initial_epoch,\n",
        "      workers=workers,\n",
        "      use_multiprocessing=use_multiprocessing,\n",
        "      verbose=1,\n",
        "      callbacks=[\n",
        "          tensorboard_callback,\n",
        "          tf.keras.callbacks.TerminateOnNaN(),\n",
        "          tf.keras.callbacks.CSVLogger(os.path.join(result_dir, model_name, f'{model_name}_{version}.log'), separator=',', append=True),\n",
        "          tf.keras.callbacks.ModelCheckpoint(os.path.join(result_dir, model_name, f'{model_name}_{version}'),\n",
        "                                monitor='val_loss', verbose=1,\n",
        "                                save_best_only=True, mode='min', initial_value_threshold=initial_value_threshold)\n",
        "      ] + additional_callbacks\n",
        "  )\n",
        "\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "EJ0gDj4gt6Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_texture_mlp():\n",
        "  \"\"\"\n",
        "  Builds and returns feed-forward model with pre-trained weights for Texture stream\n",
        "  \"\"\"\n",
        "\n",
        "  # Define inputs\n",
        "  input_texture = Input(shape=(27267,), name=\"texture_vector\")\n",
        "\n",
        "  y = Dense(64, activation='relu')(input_texture)\n",
        "  y = Dense(28, activation='relu')(y)\n",
        "  y = Dense(8, activation='relu')(y)\n",
        "  y = Model(inputs=input_texture, outputs=y)\n",
        "\n",
        "  return y"
      ],
      "metadata": {
        "id": "5XS-rlp53ayx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_colour_cnn(base_model, model_name, is_trainable,\n",
        "                     drop_out=0.25,\n",
        "                     img_size=IMAGE_SIZE):\n",
        "\n",
        "  \"\"\"\n",
        "  Builds and returns CNN model with pre-trained weights for Colour stream\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Backbone model: \", model_name)\n",
        "\n",
        "  # Define inputs\n",
        "  input_image = Input(shape=(img_size, img_size, 3), name=\"coloured_image\")\n",
        "\n",
        "  # Freeze convolutional base\n",
        "  if is_trainable == False:\n",
        "    base_model.trainable = False\n",
        "\n",
        "  # Instantiate pre-trained model\n",
        "  if 'MobileNet' in model_name:\n",
        "    # Pre-process input for model\n",
        "    x = Lambda(tf.keras.applications.mobilenet_v2.preprocess_input)(input_image)\n",
        "\n",
        "  # Instantiate pre-trained model\n",
        "  elif 'ResNet50' in model_name:\n",
        "    # Pre-process input for model\n",
        "    x = Lambda(tf.keras.applications.resnet_v2.preprocess_input)(input_image)\n",
        "\n",
        "  else:\n",
        "    print('Backbone model not found')\n",
        "\n",
        "  x = base_model(x, training=False)\n",
        "\n",
        "  # Create fully connected layers\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # Add a Dropout layer\n",
        "  x = Dropout(drop_out)(x)\n",
        "  # Add Dense layers\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  # Add a Dropout layer\n",
        "  x = Dropout(drop_out)(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "\n",
        "  # Create CNN model\n",
        "  x = Model(inputs=input_image, outputs=x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "ZrG4J48GbsPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fusion_model(base_model, model_name, is_trainable,\n",
        "                       cnn_drop_out=0.25,\n",
        "                       num_classes=NUM_CLASSES,\n",
        "                       img_size=IMAGE_SIZE):\n",
        "\n",
        "  \"\"\"\n",
        "  Build fusion CNN-MLP model for mixed-data inputs and multi-output\n",
        "  \"\"\"\n",
        "\n",
        "  # Instantiate model for Colour Stream\n",
        "  x = build_colour_cnn(base_model, model_name, is_trainable,\n",
        "                   drop_out=0.25,\n",
        "                   img_size=IMAGE_SIZE)\n",
        "\n",
        "  # Instantiate model for Texture Stream\n",
        "  y = build_texture_mlp()\n",
        "\n",
        "  # Concatenate two streams together\n",
        "  combined = layers.concatenate([x.output, y.output])\n",
        "\n",
        "  # Define joined Layer\n",
        "  z = Dense(64, activation=\"relu\")(combined)\n",
        "\n",
        "  # Add Dense layer to output predictions\n",
        "  z = Dense(num_classes, activation='softmax', name='predictions')(z)\n",
        "\n",
        "  # Define the final model\n",
        "  model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "\n",
        "  # Print model summary\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "P5dKBGrt_-9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet V2"
      ],
      "metadata": {
        "id": "NSvwQQPaNjqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "K.clear_session()"
      ],
      "metadata": {
        "id": "EHuhQo4YXnyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'MobileNet_Multi'\n",
        "\n",
        "# Load MobileNet V2 with ImageNet weights without last fully connected layers\n",
        "mobilenet_base_model = MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                                      weights='imagenet',\n",
        "                                      include_top=False)\n",
        "\n",
        "mobilenet_multi_1 = build_fusion_model(mobilenet_base_model, model_name,\n",
        "                             is_trainable=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a220baa7-9b7c-47ca-a612-17a26a639c81",
        "id": "2AbIXtcaXnyN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone model:  MobileNet_Multi\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " coloured_image (InputLayer)    [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 224, 224, 3)  0           ['coloured_image[0][0]']         \n",
            "                                                                                                  \n",
            " mobilenetv2_1.00_224 (Function  (None, 7, 7, 1280)  2257984     ['lambda[0][0]']                 \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 1280)        0           ['mobilenetv2_1.00_224[0][0]']   \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1280)         0           ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " texture_vector (InputLayer)    [(None, 27267)]      0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          163968      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           1745152     ['texture_vector[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 28)           1820        ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 8)            232         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 72)           0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           4672        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " predictions (Dense)            (None, 40)           2600        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,184,684\n",
            "Trainable params: 1,926,700\n",
            "Non-trainable params: 2,257,984\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "mobilenet_multi_1 = compile_model(mobilenet_multi_1, optimizer=Adam())\n",
        "\n",
        "# Train model\n",
        "mobilenet_multi_1_history, mobilenet_multi_1 = train_fusion_model(\n",
        "    mobilenet_multi_1, model_name,\n",
        "    train_gen=custom_train_datagen,\n",
        "    val_gen=custom_val_datagen,\n",
        "    X_train=list(X_train_dict.keys()),\n",
        "    X_val=list(X_val_dict.keys()),\n",
        "    class_weights=get_class_weights(list(y_train_dict.values()), is_encoded=True),\n",
        "    result_dir=PP_MULTI_RESULT_DIR,\n",
        "    log_dir=PP_MULTI_TB_DIR,\n",
        "    additional_callbacks=add_callbacks(4),\n",
        "    epochs=50,\n",
        "    workers=2,\n",
        "    use_multiprocessing=False,\n",
        "    version='v1',\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af26455f-a2ed-4067-94b4-3b017194c04c",
        "id": "a-AIEykIXnyO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 15.2667 - accuracy: 0.0156 - top_k_categorical_accuracy: 0.1198 - precision: 0.0252 - recall: 0.0054 - false_positives: 155.0000\n",
            "Epoch 1: val_loss improved from inf to 4.01321, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 37s 3s/step - loss: 15.2667 - accuracy: 0.0156 - top_k_categorical_accuracy: 0.1198 - precision: 0.0252 - recall: 0.0054 - false_positives: 155.0000 - val_loss: 4.0132 - val_accuracy: 0.0312 - val_top_k_categorical_accuracy: 0.1458 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.8957 - accuracy: 0.0339 - top_k_categorical_accuracy: 0.1667 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 1.0000\n",
            "Epoch 2: val_loss improved from 4.01321 to 3.65910, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.8957 - accuracy: 0.0339 - top_k_categorical_accuracy: 0.1667 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 1.0000 - val_loss: 3.6591 - val_accuracy: 0.0521 - val_top_k_categorical_accuracy: 0.1875 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6678 - accuracy: 0.0312 - top_k_categorical_accuracy: 0.1979 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 3.65910 to 3.59491, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 3.6678 - accuracy: 0.0312 - top_k_categorical_accuracy: 0.1979 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5949 - val_accuracy: 0.1042 - val_top_k_categorical_accuracy: 0.2708 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6161 - accuracy: 0.0573 - top_k_categorical_accuracy: 0.2474 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 3.59491 to 3.55947, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.6161 - accuracy: 0.0573 - top_k_categorical_accuracy: 0.2474 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5595 - val_accuracy: 0.0729 - val_top_k_categorical_accuracy: 0.3125 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.5670 - accuracy: 0.0729 - top_k_categorical_accuracy: 0.2526 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 5: val_loss improved from 3.55947 to 3.50029, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 31s 3s/step - loss: 3.5670 - accuracy: 0.0729 - top_k_categorical_accuracy: 0.2526 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5003 - val_accuracy: 0.0938 - val_top_k_categorical_accuracy: 0.2812 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.4587 - accuracy: 0.0885 - top_k_categorical_accuracy: 0.3620 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 6: val_loss improved from 3.50029 to 3.43304, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 33s 3s/step - loss: 3.4587 - accuracy: 0.0885 - top_k_categorical_accuracy: 0.3620 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.4330 - val_accuracy: 0.1354 - val_top_k_categorical_accuracy: 0.3854 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.4151 - accuracy: 0.1120 - top_k_categorical_accuracy: 0.3620 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 7: val_loss improved from 3.43304 to 3.34789, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.4151 - accuracy: 0.1120 - top_k_categorical_accuracy: 0.3620 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.3479 - val_accuracy: 0.0938 - val_top_k_categorical_accuracy: 0.3958 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.3865 - accuracy: 0.1120 - top_k_categorical_accuracy: 0.3542 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 8: val_loss improved from 3.34789 to 3.29952, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.3865 - accuracy: 0.1120 - top_k_categorical_accuracy: 0.3542 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.2995 - val_accuracy: 0.1250 - val_top_k_categorical_accuracy: 0.3750 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.3691 - accuracy: 0.1094 - top_k_categorical_accuracy: 0.3490 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 9: val_loss improved from 3.29952 to 3.23255, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 30s 3s/step - loss: 3.3691 - accuracy: 0.1094 - top_k_categorical_accuracy: 0.3490 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.2326 - val_accuracy: 0.1042 - val_top_k_categorical_accuracy: 0.4583 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.1747 - accuracy: 0.1224 - top_k_categorical_accuracy: 0.4531 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 10: val_loss improved from 3.23255 to 3.15471, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.1747 - accuracy: 0.1224 - top_k_categorical_accuracy: 0.4531 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.1547 - val_accuracy: 0.1250 - val_top_k_categorical_accuracy: 0.4167 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.0519 - accuracy: 0.1901 - top_k_categorical_accuracy: 0.5052 - precision: 0.5000 - recall: 0.0013 - false_positives: 1.0000\n",
            "Epoch 11: val_loss improved from 3.15471 to 3.01164, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 29s 3s/step - loss: 3.0519 - accuracy: 0.1901 - top_k_categorical_accuracy: 0.5052 - precision: 0.5000 - recall: 0.0013 - false_positives: 1.0000 - val_loss: 3.0116 - val_accuracy: 0.1458 - val_top_k_categorical_accuracy: 0.4792 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.1247 - accuracy: 0.1693 - top_k_categorical_accuracy: 0.4948 - precision: 1.0000 - recall: 0.0053 - false_positives: 0.0000e+00\n",
            "Epoch 12: val_loss improved from 3.01164 to 2.95613, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.1247 - accuracy: 0.1693 - top_k_categorical_accuracy: 0.4948 - precision: 1.0000 - recall: 0.0053 - false_positives: 0.0000e+00 - val_loss: 2.9561 - val_accuracy: 0.1771 - val_top_k_categorical_accuracy: 0.4896 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.8709 - accuracy: 0.2552 - top_k_categorical_accuracy: 0.5625 - precision: 1.0000 - recall: 0.0161 - false_positives: 0.0000e+00\n",
            "Epoch 13: val_loss improved from 2.95613 to 2.94278, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.8709 - accuracy: 0.2552 - top_k_categorical_accuracy: 0.5625 - precision: 1.0000 - recall: 0.0161 - false_positives: 0.0000e+00 - val_loss: 2.9428 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.4583 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.0215 - accuracy: 0.2214 - top_k_categorical_accuracy: 0.5651 - precision: 0.7857 - recall: 0.0147 - false_positives: 3.0000\n",
            "Epoch 14: val_loss improved from 2.94278 to 2.79465, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 3.0215 - accuracy: 0.2214 - top_k_categorical_accuracy: 0.5651 - precision: 0.7857 - recall: 0.0147 - false_positives: 3.0000 - val_loss: 2.7947 - val_accuracy: 0.1979 - val_top_k_categorical_accuracy: 0.5417 - val_precision: 1.0000 - val_recall: 0.0104 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.9210 - accuracy: 0.2031 - top_k_categorical_accuracy: 0.5729 - precision: 0.8750 - recall: 0.0187 - false_positives: 2.0000\n",
            "Epoch 15: val_loss did not improve from 2.79465\n",
            "12/12 [==============================] - 10s 895ms/step - loss: 2.9210 - accuracy: 0.2031 - top_k_categorical_accuracy: 0.5729 - precision: 0.8750 - recall: 0.0187 - false_positives: 2.0000 - val_loss: 2.8686 - val_accuracy: 0.1979 - val_top_k_categorical_accuracy: 0.4792 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.8474 - accuracy: 0.2500 - top_k_categorical_accuracy: 0.6172 - precision: 0.8667 - recall: 0.0174 - false_positives: 2.0000\n",
            "Epoch 16: val_loss improved from 2.79465 to 2.74635, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.8474 - accuracy: 0.2500 - top_k_categorical_accuracy: 0.6172 - precision: 0.8667 - recall: 0.0174 - false_positives: 2.0000 - val_loss: 2.7464 - val_accuracy: 0.2500 - val_top_k_categorical_accuracy: 0.5417 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5852 - accuracy: 0.3073 - top_k_categorical_accuracy: 0.7005 - precision: 0.8571 - recall: 0.0242 - false_positives: 3.0000\n",
            "Epoch 17: val_loss did not improve from 2.74635\n",
            "12/12 [==============================] - 8s 662ms/step - loss: 2.5852 - accuracy: 0.3073 - top_k_categorical_accuracy: 0.7005 - precision: 0.8571 - recall: 0.0242 - false_positives: 3.0000 - val_loss: 2.7712 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.5000 - val_precision: 1.0000 - val_recall: 0.0417 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.4913 - accuracy: 0.3229 - top_k_categorical_accuracy: 0.6745 - precision: 0.8750 - recall: 0.0467 - false_positives: 5.0000\n",
            "Epoch 18: val_loss improved from 2.74635 to 2.53792, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.4913 - accuracy: 0.3229 - top_k_categorical_accuracy: 0.6745 - precision: 0.8750 - recall: 0.0467 - false_positives: 5.0000 - val_loss: 2.5379 - val_accuracy: 0.2812 - val_top_k_categorical_accuracy: 0.6146 - val_precision: 0.8571 - val_recall: 0.0625 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6835 - accuracy: 0.2500 - top_k_categorical_accuracy: 0.6328 - precision: 0.7083 - recall: 0.0225 - false_positives: 7.0000\n",
            "Epoch 19: val_loss did not improve from 2.53792\n",
            "12/12 [==============================] - 10s 884ms/step - loss: 2.6835 - accuracy: 0.2500 - top_k_categorical_accuracy: 0.6328 - precision: 0.7083 - recall: 0.0225 - false_positives: 7.0000 - val_loss: 2.6062 - val_accuracy: 0.2604 - val_top_k_categorical_accuracy: 0.5729 - val_precision: 1.0000 - val_recall: 0.0521 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6209 - accuracy: 0.3047 - top_k_categorical_accuracy: 0.6901 - precision: 0.8537 - recall: 0.0469 - false_positives: 6.0000\n",
            "Epoch 20: val_loss improved from 2.53792 to 2.52068, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.6209 - accuracy: 0.3047 - top_k_categorical_accuracy: 0.6901 - precision: 0.8537 - recall: 0.0469 - false_positives: 6.0000 - val_loss: 2.5207 - val_accuracy: 0.2708 - val_top_k_categorical_accuracy: 0.5833 - val_precision: 0.8750 - val_recall: 0.0729 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6263 - accuracy: 0.2995 - top_k_categorical_accuracy: 0.6589 - precision: 0.8788 - recall: 0.0390 - false_positives: 4.0000\n",
            "Epoch 21: val_loss did not improve from 2.52068\n",
            "12/12 [==============================] - 10s 898ms/step - loss: 2.6263 - accuracy: 0.2995 - top_k_categorical_accuracy: 0.6589 - precision: 0.8788 - recall: 0.0390 - false_positives: 4.0000 - val_loss: 2.5282 - val_accuracy: 0.2708 - val_top_k_categorical_accuracy: 0.5938 - val_precision: 0.9000 - val_recall: 0.0938 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.4931 - accuracy: 0.3880 - top_k_categorical_accuracy: 0.7318 - precision: 0.7955 - recall: 0.0470 - false_positives: 9.0000\n",
            "Epoch 22: val_loss improved from 2.52068 to 2.50313, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.4931 - accuracy: 0.3880 - top_k_categorical_accuracy: 0.7318 - precision: 0.7955 - recall: 0.0470 - false_positives: 9.0000 - val_loss: 2.5031 - val_accuracy: 0.2917 - val_top_k_categorical_accuracy: 0.5625 - val_precision: 0.8750 - val_recall: 0.0729 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6206 - accuracy: 0.3594 - top_k_categorical_accuracy: 0.6589 - precision: 0.9062 - recall: 0.0388 - false_positives: 3.0000\n",
            "Epoch 23: val_loss did not improve from 2.50313\n",
            "12/12 [==============================] - 10s 905ms/step - loss: 2.6206 - accuracy: 0.3594 - top_k_categorical_accuracy: 0.6589 - precision: 0.9062 - recall: 0.0388 - false_positives: 3.0000 - val_loss: 2.5547 - val_accuracy: 0.2083 - val_top_k_categorical_accuracy: 0.5833 - val_precision: 1.0000 - val_recall: 0.0729 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2386 - accuracy: 0.4010 - top_k_categorical_accuracy: 0.7474 - precision: 0.8491 - recall: 0.0602 - false_positives: 8.0000\n",
            "Epoch 24: val_loss improved from 2.50313 to 2.48103, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.2386 - accuracy: 0.4010 - top_k_categorical_accuracy: 0.7474 - precision: 0.8491 - recall: 0.0602 - false_positives: 8.0000 - val_loss: 2.4810 - val_accuracy: 0.3125 - val_top_k_categorical_accuracy: 0.5938 - val_precision: 0.7500 - val_recall: 0.0938 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2764 - accuracy: 0.4245 - top_k_categorical_accuracy: 0.7682 - precision: 0.8462 - recall: 0.0887 - false_positives: 12.0000\n",
            "Epoch 25: val_loss did not improve from 2.48103\n",
            "12/12 [==============================] - 10s 893ms/step - loss: 2.2764 - accuracy: 0.4245 - top_k_categorical_accuracy: 0.7682 - precision: 0.8462 - recall: 0.0887 - false_positives: 12.0000 - val_loss: 2.5075 - val_accuracy: 0.3021 - val_top_k_categorical_accuracy: 0.6146 - val_precision: 0.7857 - val_recall: 0.1146 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.3378 - accuracy: 0.3594 - top_k_categorical_accuracy: 0.7526 - precision: 0.8167 - recall: 0.0663 - false_positives: 11.0000\n",
            "Epoch 26: val_loss improved from 2.48103 to 2.33323, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 2.3378 - accuracy: 0.3594 - top_k_categorical_accuracy: 0.7526 - precision: 0.8167 - recall: 0.0663 - false_positives: 11.0000 - val_loss: 2.3332 - val_accuracy: 0.3021 - val_top_k_categorical_accuracy: 0.6979 - val_precision: 0.8000 - val_recall: 0.1250 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6375 - accuracy: 0.3255 - top_k_categorical_accuracy: 0.6849 - precision: 0.8393 - recall: 0.0626 - false_positives: 9.0000\n",
            "Epoch 27: val_loss did not improve from 2.33323\n",
            "12/12 [==============================] - 10s 883ms/step - loss: 2.6375 - accuracy: 0.3255 - top_k_categorical_accuracy: 0.6849 - precision: 0.8393 - recall: 0.0626 - false_positives: 9.0000 - val_loss: 2.3920 - val_accuracy: 0.2708 - val_top_k_categorical_accuracy: 0.6875 - val_precision: 0.8571 - val_recall: 0.1250 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6090 - accuracy: 0.3385 - top_k_categorical_accuracy: 0.7109 - precision: 0.8400 - recall: 0.0563 - false_positives: 8.0000\n",
            "Epoch 28: val_loss did not improve from 2.33323\n",
            "12/12 [==============================] - 10s 904ms/step - loss: 2.6090 - accuracy: 0.3385 - top_k_categorical_accuracy: 0.7109 - precision: 0.8400 - recall: 0.0563 - false_positives: 8.0000 - val_loss: 2.3349 - val_accuracy: 0.3229 - val_top_k_categorical_accuracy: 0.7083 - val_precision: 0.7333 - val_recall: 0.1146 - val_false_positives: 4.0000 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2623 - accuracy: 0.4010 - top_k_categorical_accuracy: 0.7526 - precision: 0.8485 - recall: 0.0759 - false_positives: 10.0000\n",
            "Epoch 29: val_loss did not improve from 2.33323\n",
            "12/12 [==============================] - 11s 906ms/step - loss: 2.2623 - accuracy: 0.4010 - top_k_categorical_accuracy: 0.7526 - precision: 0.8485 - recall: 0.0759 - false_positives: 10.0000 - val_loss: 2.4232 - val_accuracy: 0.3438 - val_top_k_categorical_accuracy: 0.6354 - val_precision: 0.6667 - val_recall: 0.1042 - val_false_positives: 5.0000 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2968 - accuracy: 0.4089 - top_k_categorical_accuracy: 0.7552 - precision: 0.7778 - recall: 0.0745 - false_positives: 16.0000\n",
            "Epoch 30: val_loss improved from 2.33323 to 2.26808, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 29s 3s/step - loss: 2.2968 - accuracy: 0.4089 - top_k_categorical_accuracy: 0.7552 - precision: 0.7778 - recall: 0.0745 - false_positives: 16.0000 - val_loss: 2.2681 - val_accuracy: 0.3021 - val_top_k_categorical_accuracy: 0.6979 - val_precision: 0.8750 - val_recall: 0.1458 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9823 - accuracy: 0.4896 - top_k_categorical_accuracy: 0.8281 - precision: 0.8974 - recall: 0.0942 - false_positives: 8.0000\n",
            "Epoch 31: val_loss improved from 2.26808 to 2.23451, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 1.9823 - accuracy: 0.4896 - top_k_categorical_accuracy: 0.8281 - precision: 0.8974 - recall: 0.0942 - false_positives: 8.0000 - val_loss: 2.2345 - val_accuracy: 0.3854 - val_top_k_categorical_accuracy: 0.7292 - val_precision: 0.6842 - val_recall: 0.1354 - val_false_positives: 6.0000 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8924 - accuracy: 0.4922 - top_k_categorical_accuracy: 0.8411 - precision: 0.8298 - recall: 0.1051 - false_positives: 16.0000\n",
            "Epoch 32: val_loss did not improve from 2.23451\n",
            "12/12 [==============================] - 10s 893ms/step - loss: 1.8924 - accuracy: 0.4922 - top_k_categorical_accuracy: 0.8411 - precision: 0.8298 - recall: 0.1051 - false_positives: 16.0000 - val_loss: 2.3622 - val_accuracy: 0.3542 - val_top_k_categorical_accuracy: 0.6458 - val_precision: 0.6667 - val_recall: 0.1667 - val_false_positives: 8.0000 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9780 - accuracy: 0.4922 - top_k_categorical_accuracy: 0.8385 - precision: 0.8049 - recall: 0.1318 - false_positives: 24.0000\n",
            "Epoch 33: val_loss did not improve from 2.23451\n",
            "12/12 [==============================] - 10s 898ms/step - loss: 1.9780 - accuracy: 0.4922 - top_k_categorical_accuracy: 0.8385 - precision: 0.8049 - recall: 0.1318 - false_positives: 24.0000 - val_loss: 2.3291 - val_accuracy: 0.3125 - val_top_k_categorical_accuracy: 0.6771 - val_precision: 0.8000 - val_recall: 0.1667 - val_false_positives: 4.0000 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9452 - accuracy: 0.4870 - top_k_categorical_accuracy: 0.8438 - precision: 0.8224 - recall: 0.1176 - false_positives: 19.0000\n",
            "Epoch 34: val_loss improved from 2.23451 to 2.18790, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 1.9452 - accuracy: 0.4870 - top_k_categorical_accuracy: 0.8438 - precision: 0.8224 - recall: 0.1176 - false_positives: 19.0000 - val_loss: 2.1879 - val_accuracy: 0.3229 - val_top_k_categorical_accuracy: 0.7604 - val_precision: 0.6957 - val_recall: 0.1667 - val_false_positives: 7.0000 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8387 - accuracy: 0.4922 - top_k_categorical_accuracy: 0.8438 - precision: 0.8627 - recall: 0.1180 - false_positives: 14.0000\n",
            "Epoch 35: val_loss improved from 2.18790 to 2.08261, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 32s 3s/step - loss: 1.8387 - accuracy: 0.4922 - top_k_categorical_accuracy: 0.8438 - precision: 0.8627 - recall: 0.1180 - false_positives: 14.0000 - val_loss: 2.0826 - val_accuracy: 0.3021 - val_top_k_categorical_accuracy: 0.7188 - val_precision: 0.7727 - val_recall: 0.1771 - val_false_positives: 5.0000 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.7510 - accuracy: 0.4896 - top_k_categorical_accuracy: 0.8385 - precision: 0.8000 - recall: 0.1132 - false_positives: 21.0000\n",
            "Epoch 36: val_loss did not improve from 2.08261\n",
            "12/12 [==============================] - 10s 895ms/step - loss: 1.7510 - accuracy: 0.4896 - top_k_categorical_accuracy: 0.8385 - precision: 0.8000 - recall: 0.1132 - false_positives: 21.0000 - val_loss: 2.2262 - val_accuracy: 0.3125 - val_top_k_categorical_accuracy: 0.7292 - val_precision: 0.7500 - val_recall: 0.1875 - val_false_positives: 6.0000 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.1568 - accuracy: 0.4193 - top_k_categorical_accuracy: 0.7917 - precision: 0.7407 - recall: 0.1071 - false_positives: 28.0000\n",
            "Epoch 37: val_loss improved from 2.08261 to 2.06452, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/MobileNet_Multi/MobileNet_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 30s 3s/step - loss: 2.1568 - accuracy: 0.4193 - top_k_categorical_accuracy: 0.7917 - precision: 0.7407 - recall: 0.1071 - false_positives: 28.0000 - val_loss: 2.0645 - val_accuracy: 0.3125 - val_top_k_categorical_accuracy: 0.7500 - val_precision: 0.7200 - val_recall: 0.1875 - val_false_positives: 7.0000 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.7630 - accuracy: 0.4896 - top_k_categorical_accuracy: 0.8672 - precision: 0.8661 - recall: 0.1309 - false_positives: 15.0000\n",
            "Epoch 38: val_loss did not improve from 2.06452\n",
            "12/12 [==============================] - 10s 884ms/step - loss: 1.7630 - accuracy: 0.4896 - top_k_categorical_accuracy: 0.8672 - precision: 0.8661 - recall: 0.1309 - false_positives: 15.0000 - val_loss: 2.1177 - val_accuracy: 0.3646 - val_top_k_categorical_accuracy: 0.7500 - val_precision: 0.6800 - val_recall: 0.1771 - val_false_positives: 8.0000 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0080 - accuracy: 0.4870 - top_k_categorical_accuracy: 0.8438 - precision: 0.8319 - recall: 0.1252 - false_positives: 19.0000\n",
            "Epoch 39: val_loss did not improve from 2.06452\n",
            "12/12 [==============================] - 8s 653ms/step - loss: 2.0080 - accuracy: 0.4870 - top_k_categorical_accuracy: 0.8438 - precision: 0.8319 - recall: 0.1252 - false_positives: 19.0000 - val_loss: 2.1474 - val_accuracy: 0.3646 - val_top_k_categorical_accuracy: 0.7500 - val_precision: 0.6800 - val_recall: 0.1771 - val_false_positives: 8.0000 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.5949 - accuracy: 0.5781 - top_k_categorical_accuracy: 0.9036 - precision: 0.8519 - recall: 0.1535 - false_positives: 20.0000\n",
            "Epoch 40: val_loss did not improve from 2.06452\n",
            "12/12 [==============================] - 10s 887ms/step - loss: 1.5949 - accuracy: 0.5781 - top_k_categorical_accuracy: 0.9036 - precision: 0.8519 - recall: 0.1535 - false_positives: 20.0000 - val_loss: 2.2354 - val_accuracy: 0.3229 - val_top_k_categorical_accuracy: 0.7292 - val_precision: 0.6333 - val_recall: 0.1979 - val_false_positives: 11.0000 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.7208 - accuracy: 0.5260 - top_k_categorical_accuracy: 0.8802 - precision: 0.7803 - recall: 0.1377 - false_positives: 29.0000\n",
            "Epoch 41: val_loss did not improve from 2.06452\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "12/12 [==============================] - 10s 904ms/step - loss: 1.7208 - accuracy: 0.5260 - top_k_categorical_accuracy: 0.8802 - precision: 0.7803 - recall: 0.1377 - false_positives: 29.0000 - val_loss: 2.0855 - val_accuracy: 0.3542 - val_top_k_categorical_accuracy: 0.7708 - val_precision: 0.7143 - val_recall: 0.2083 - val_false_positives: 8.0000 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50 V2"
      ],
      "metadata": {
        "id": "pkmadYtP0g1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ResNet50_Multi'\n",
        "\n",
        "# Load ResNet50 V2 with ImageNet weights without last fully connected layers\n",
        "resnet50_base_model = ResNet50V2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                                      weights='imagenet',\n",
        "                                      include_top=False)\n",
        "\n",
        "resnet50_multi_1 = build_fusion_model(resnet50_base_model, model_name,\n",
        "                             is_trainable=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhweRok26Au",
        "outputId": "2ae22c92-ba5c-4787-9d2e-ea1745f4900c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone model:  ResNet50_Multi\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " coloured_image (InputLayer)    [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 224, 224, 3)  0           ['coloured_image[0][0]']         \n",
            "                                                                                                  \n",
            " resnet50v2 (Functional)        (None, 7, 7, 2048)   23564800    ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['resnet50v2[0][0]']             \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 2048)         0           ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " texture_vector (InputLayer)    [(None, 27267)]      0           []                               \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          262272      ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 64)           1745152     ['texture_vector[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 28)           1820        ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 64)           8256        ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 8)            232         ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 72)           0           ['dense_7[0][0]',                \n",
            "                                                                  'dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 64)           4672        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " predictions (Dense)            (None, 40)           2600        ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,589,804\n",
            "Trainable params: 2,025,004\n",
            "Non-trainable params: 23,564,800\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "resnet50_multi_1 = compile_model(resnet50_multi_1, optimizer=Adam())\n",
        "\n",
        "# Train model\n",
        "resnet50_multi_1_history, resnet50_multi_1 = train_fusion_model(\n",
        "    resnet50_multi_1, model_name,\n",
        "    train_gen=custom_train_datagen,\n",
        "    val_gen=custom_val_datagen,\n",
        "    X_train=list(X_train_dict.keys()),\n",
        "    X_val=list(X_val_dict.keys()),\n",
        "    class_weights=get_class_weights(list(y_train_dict.values()), is_encoded=True),\n",
        "    result_dir=PP_MULTI_RESULT_DIR,\n",
        "    log_dir=PP_MULTI_TB_DIR,\n",
        "    additional_callbacks=add_callbacks(4),\n",
        "    epochs=50,\n",
        "    workers=2,\n",
        "    use_multiprocessing=False,\n",
        "    version='v1',\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trE4YUDk26Au",
        "outputId": "3d67cf93-0918-4721-9b3d-68be60da7ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 12.2700 - accuracy: 0.0391 - top_k_categorical_accuracy: 0.1198 - precision: 0.0559 - recall: 0.0107 - false_positives: 135.0000\n",
            "Epoch 1: val_loss improved from inf to 3.92586, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 39s 3s/step - loss: 12.2700 - accuracy: 0.0391 - top_k_categorical_accuracy: 0.1198 - precision: 0.0559 - recall: 0.0107 - false_positives: 135.0000 - val_loss: 3.9259 - val_accuracy: 0.0417 - val_top_k_categorical_accuracy: 0.1562 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.7927 - accuracy: 0.0339 - top_k_categorical_accuracy: 0.1615 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 3.92586 to 3.59278, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.7927 - accuracy: 0.0339 - top_k_categorical_accuracy: 0.1615 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5928 - val_accuracy: 0.0833 - val_top_k_categorical_accuracy: 0.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.6687 - accuracy: 0.0312 - top_k_categorical_accuracy: 0.1849 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 3.59278 to 3.57045, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.6687 - accuracy: 0.0312 - top_k_categorical_accuracy: 0.1849 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5705 - val_accuracy: 0.0521 - val_top_k_categorical_accuracy: 0.2812 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.5855 - accuracy: 0.0755 - top_k_categorical_accuracy: 0.2188 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 3.57045 to 3.50434, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.5855 - accuracy: 0.0755 - top_k_categorical_accuracy: 0.2188 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.5043 - val_accuracy: 0.1042 - val_top_k_categorical_accuracy: 0.3229 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.4812 - accuracy: 0.0938 - top_k_categorical_accuracy: 0.3021 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 5: val_loss improved from 3.50434 to 3.41385, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 3.4812 - accuracy: 0.0938 - top_k_categorical_accuracy: 0.3021 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.4138 - val_accuracy: 0.1042 - val_top_k_categorical_accuracy: 0.3021 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.4806 - accuracy: 0.0885 - top_k_categorical_accuracy: 0.3125 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00\n",
            "Epoch 6: val_loss improved from 3.41385 to 3.34800, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.4806 - accuracy: 0.0885 - top_k_categorical_accuracy: 0.3125 - precision: 0.0000e+00 - recall: 0.0000e+00 - false_positives: 0.0000e+00 - val_loss: 3.3480 - val_accuracy: 0.0729 - val_top_k_categorical_accuracy: 0.3750 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.3058 - accuracy: 0.1406 - top_k_categorical_accuracy: 0.4141 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00\n",
            "Epoch 7: val_loss improved from 3.34800 to 3.19984, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.3058 - accuracy: 0.1406 - top_k_categorical_accuracy: 0.4141 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00 - val_loss: 3.1998 - val_accuracy: 0.1250 - val_top_k_categorical_accuracy: 0.4271 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.1989 - accuracy: 0.1719 - top_k_categorical_accuracy: 0.4323 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00\n",
            "Epoch 8: val_loss improved from 3.19984 to 3.02507, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.1989 - accuracy: 0.1719 - top_k_categorical_accuracy: 0.4323 - precision: 1.0000 - recall: 0.0027 - false_positives: 0.0000e+00 - val_loss: 3.0251 - val_accuracy: 0.1667 - val_top_k_categorical_accuracy: 0.5833 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.1979 - accuracy: 0.1745 - top_k_categorical_accuracy: 0.4661 - precision: 1.0000 - recall: 0.0081 - false_positives: 0.0000e+00\n",
            "Epoch 9: val_loss improved from 3.02507 to 2.89391, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 3.1979 - accuracy: 0.1745 - top_k_categorical_accuracy: 0.4661 - precision: 1.0000 - recall: 0.0081 - false_positives: 0.0000e+00 - val_loss: 2.8939 - val_accuracy: 0.1979 - val_top_k_categorical_accuracy: 0.6771 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.1147 - accuracy: 0.1901 - top_k_categorical_accuracy: 0.4766 - precision: 1.0000 - recall: 0.0053 - false_positives: 0.0000e+00\n",
            "Epoch 10: val_loss improved from 2.89391 to 2.75807, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 3.1147 - accuracy: 0.1901 - top_k_categorical_accuracy: 0.4766 - precision: 1.0000 - recall: 0.0053 - false_positives: 0.0000e+00 - val_loss: 2.7581 - val_accuracy: 0.2396 - val_top_k_categorical_accuracy: 0.6562 - val_precision: 1.0000 - val_recall: 0.0208 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.9753 - accuracy: 0.2135 - top_k_categorical_accuracy: 0.5391 - precision: 1.0000 - recall: 0.0173 - false_positives: 0.0000e+00\n",
            "Epoch 11: val_loss improved from 2.75807 to 2.66391, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 2.9753 - accuracy: 0.2135 - top_k_categorical_accuracy: 0.5391 - precision: 1.0000 - recall: 0.0173 - false_positives: 0.0000e+00 - val_loss: 2.6639 - val_accuracy: 0.2812 - val_top_k_categorical_accuracy: 0.6562 - val_precision: 1.0000 - val_recall: 0.0312 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.7870 - accuracy: 0.2552 - top_k_categorical_accuracy: 0.6250 - precision: 0.8095 - recall: 0.0226 - false_positives: 4.0000\n",
            "Epoch 12: val_loss improved from 2.66391 to 2.52595, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 37s 3s/step - loss: 2.7870 - accuracy: 0.2552 - top_k_categorical_accuracy: 0.6250 - precision: 0.8095 - recall: 0.0226 - false_positives: 4.0000 - val_loss: 2.5260 - val_accuracy: 0.2812 - val_top_k_categorical_accuracy: 0.7188 - val_precision: 0.7500 - val_recall: 0.0312 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.6039 - accuracy: 0.2839 - top_k_categorical_accuracy: 0.6849 - precision: 0.8000 - recall: 0.0266 - false_positives: 5.0000\n",
            "Epoch 13: val_loss improved from 2.52595 to 2.39857, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 2.6039 - accuracy: 0.2839 - top_k_categorical_accuracy: 0.6849 - precision: 0.8000 - recall: 0.0266 - false_positives: 5.0000 - val_loss: 2.3986 - val_accuracy: 0.3229 - val_top_k_categorical_accuracy: 0.7500 - val_precision: 1.0000 - val_recall: 0.0625 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5391 - accuracy: 0.2839 - top_k_categorical_accuracy: 0.6745 - precision: 0.8378 - recall: 0.0410 - false_positives: 6.0000\n",
            "Epoch 14: val_loss improved from 2.39857 to 2.23847, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 2.5391 - accuracy: 0.2839 - top_k_categorical_accuracy: 0.6745 - precision: 0.8378 - recall: 0.0410 - false_positives: 6.0000 - val_loss: 2.2385 - val_accuracy: 0.3333 - val_top_k_categorical_accuracy: 0.7604 - val_precision: 1.0000 - val_recall: 0.0833 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5135 - accuracy: 0.3099 - top_k_categorical_accuracy: 0.6667 - precision: 0.8947 - recall: 0.0457 - false_positives: 4.0000\n",
            "Epoch 15: val_loss did not improve from 2.23847\n",
            "12/12 [==============================] - 10s 865ms/step - loss: 2.5135 - accuracy: 0.3099 - top_k_categorical_accuracy: 0.6667 - precision: 0.8947 - recall: 0.0457 - false_positives: 4.0000 - val_loss: 2.2776 - val_accuracy: 0.3750 - val_top_k_categorical_accuracy: 0.7292 - val_precision: 0.8182 - val_recall: 0.0938 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.3738 - accuracy: 0.3359 - top_k_categorical_accuracy: 0.7057 - precision: 0.8696 - recall: 0.0529 - false_positives: 6.0000\n",
            "Epoch 16: val_loss improved from 2.23847 to 2.22838, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 2.3738 - accuracy: 0.3359 - top_k_categorical_accuracy: 0.7057 - precision: 0.8696 - recall: 0.0529 - false_positives: 6.0000 - val_loss: 2.2284 - val_accuracy: 0.3646 - val_top_k_categorical_accuracy: 0.6979 - val_precision: 1.0000 - val_recall: 0.1042 - val_false_positives: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0912 - accuracy: 0.4141 - top_k_categorical_accuracy: 0.7917 - precision: 0.8814 - recall: 0.0692 - false_positives: 7.0000\n",
            "Epoch 17: val_loss improved from 2.22838 to 1.97984, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 2.0912 - accuracy: 0.4141 - top_k_categorical_accuracy: 0.7917 - precision: 0.8814 - recall: 0.0692 - false_positives: 7.0000 - val_loss: 1.9798 - val_accuracy: 0.3854 - val_top_k_categorical_accuracy: 0.7500 - val_precision: 0.9286 - val_recall: 0.1354 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.3210 - accuracy: 0.3698 - top_k_categorical_accuracy: 0.7500 - precision: 0.7917 - recall: 0.0761 - false_positives: 15.0000\n",
            "Epoch 18: val_loss did not improve from 1.97984\n",
            "12/12 [==============================] - 13s 1s/step - loss: 2.3210 - accuracy: 0.3698 - top_k_categorical_accuracy: 0.7500 - precision: 0.7917 - recall: 0.0761 - false_positives: 15.0000 - val_loss: 2.0388 - val_accuracy: 0.3646 - val_top_k_categorical_accuracy: 0.7708 - val_precision: 0.9167 - val_recall: 0.1146 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.5910 - accuracy: 0.2917 - top_k_categorical_accuracy: 0.7005 - precision: 0.7755 - recall: 0.0512 - false_positives: 11.0000\n",
            "Epoch 19: val_loss did not improve from 1.97984\n",
            "12/12 [==============================] - 10s 895ms/step - loss: 2.5910 - accuracy: 0.2917 - top_k_categorical_accuracy: 0.7005 - precision: 0.7755 - recall: 0.0512 - false_positives: 11.0000 - val_loss: 2.0347 - val_accuracy: 0.4271 - val_top_k_categorical_accuracy: 0.8229 - val_precision: 0.9167 - val_recall: 0.1146 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0729 - accuracy: 0.4323 - top_k_categorical_accuracy: 0.8255 - precision: 0.8154 - recall: 0.0710 - false_positives: 12.0000\n",
            "Epoch 20: val_loss did not improve from 1.97984\n",
            "12/12 [==============================] - 11s 905ms/step - loss: 2.0729 - accuracy: 0.4323 - top_k_categorical_accuracy: 0.8255 - precision: 0.8154 - recall: 0.0710 - false_positives: 12.0000 - val_loss: 2.0234 - val_accuracy: 0.3646 - val_top_k_categorical_accuracy: 0.8125 - val_precision: 0.8750 - val_recall: 0.1458 - val_false_positives: 2.0000 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2753 - accuracy: 0.4089 - top_k_categorical_accuracy: 0.7682 - precision: 0.8108 - recall: 0.0799 - false_positives: 14.0000\n",
            "Epoch 21: val_loss improved from 1.97984 to 1.91434, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 36s 3s/step - loss: 2.2753 - accuracy: 0.4089 - top_k_categorical_accuracy: 0.7682 - precision: 0.8108 - recall: 0.0799 - false_positives: 14.0000 - val_loss: 1.9143 - val_accuracy: 0.3750 - val_top_k_categorical_accuracy: 0.8125 - val_precision: 0.7500 - val_recall: 0.1562 - val_false_positives: 5.0000 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2041 - accuracy: 0.4427 - top_k_categorical_accuracy: 0.7891 - precision: 0.8000 - recall: 0.0694 - false_positives: 13.0000\n",
            "Epoch 22: val_loss improved from 1.91434 to 1.87861, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 2.2041 - accuracy: 0.4427 - top_k_categorical_accuracy: 0.7891 - precision: 0.8000 - recall: 0.0694 - false_positives: 13.0000 - val_loss: 1.8786 - val_accuracy: 0.3854 - val_top_k_categorical_accuracy: 0.8125 - val_precision: 0.9444 - val_recall: 0.1771 - val_false_positives: 1.0000 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0564 - accuracy: 0.4453 - top_k_categorical_accuracy: 0.8307 - precision: 0.8252 - recall: 0.1127 - false_positives: 18.0000\n",
            "Epoch 23: val_loss improved from 1.87861 to 1.76519, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 2.0564 - accuracy: 0.4453 - top_k_categorical_accuracy: 0.8307 - precision: 0.8252 - recall: 0.1127 - false_positives: 18.0000 - val_loss: 1.7652 - val_accuracy: 0.4583 - val_top_k_categorical_accuracy: 0.8646 - val_precision: 0.7619 - val_recall: 0.1667 - val_false_positives: 5.0000 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.7891 - accuracy: 0.4714 - top_k_categorical_accuracy: 0.8802 - precision: 0.7917 - recall: 0.1017 - false_positives: 20.0000\n",
            "Epoch 24: val_loss did not improve from 1.76519\n",
            "12/12 [==============================] - 11s 915ms/step - loss: 1.7891 - accuracy: 0.4714 - top_k_categorical_accuracy: 0.8802 - precision: 0.7917 - recall: 0.1017 - false_positives: 20.0000 - val_loss: 1.7766 - val_accuracy: 0.4271 - val_top_k_categorical_accuracy: 0.8229 - val_precision: 0.7143 - val_recall: 0.2083 - val_false_positives: 8.0000 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9937 - accuracy: 0.4453 - top_k_categorical_accuracy: 0.8750 - precision: 0.8230 - recall: 0.1250 - false_positives: 20.0000\n",
            "Epoch 25: val_loss improved from 1.76519 to 1.70488, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 1.9937 - accuracy: 0.4453 - top_k_categorical_accuracy: 0.8750 - precision: 0.8230 - recall: 0.1250 - false_positives: 20.0000 - val_loss: 1.7049 - val_accuracy: 0.4479 - val_top_k_categorical_accuracy: 0.8333 - val_precision: 0.8750 - val_recall: 0.2188 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8648 - accuracy: 0.5234 - top_k_categorical_accuracy: 0.8594 - precision: 0.8718 - recall: 0.1360 - false_positives: 15.0000\n",
            "Epoch 26: val_loss did not improve from 1.70488\n",
            "12/12 [==============================] - 10s 851ms/step - loss: 1.8648 - accuracy: 0.5234 - top_k_categorical_accuracy: 0.8594 - precision: 0.8718 - recall: 0.1360 - false_positives: 15.0000 - val_loss: 1.7566 - val_accuracy: 0.5000 - val_top_k_categorical_accuracy: 0.8333 - val_precision: 0.8846 - val_recall: 0.2396 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.2695 - accuracy: 0.4375 - top_k_categorical_accuracy: 0.7839 - precision: 0.8242 - recall: 0.1011 - false_positives: 16.0000\n",
            "Epoch 27: val_loss improved from 1.70488 to 1.70420, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 2.2695 - accuracy: 0.4375 - top_k_categorical_accuracy: 0.7839 - precision: 0.8242 - recall: 0.1011 - false_positives: 16.0000 - val_loss: 1.7042 - val_accuracy: 0.4792 - val_top_k_categorical_accuracy: 0.8125 - val_precision: 0.8571 - val_recall: 0.1875 - val_false_positives: 3.0000 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.7005 - accuracy: 0.5260 - top_k_categorical_accuracy: 0.8750 - precision: 0.8318 - recall: 0.1195 - false_positives: 18.0000\n",
            "Epoch 28: val_loss did not improve from 1.70420\n",
            "12/12 [==============================] - 10s 847ms/step - loss: 1.7005 - accuracy: 0.5260 - top_k_categorical_accuracy: 0.8750 - precision: 0.8318 - recall: 0.1195 - false_positives: 18.0000 - val_loss: 1.7232 - val_accuracy: 0.4271 - val_top_k_categorical_accuracy: 0.8542 - val_precision: 0.6452 - val_recall: 0.2083 - val_false_positives: 11.0000 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0897 - accuracy: 0.4714 - top_k_categorical_accuracy: 0.8464 - precision: 0.8083 - recall: 0.1295 - false_positives: 23.0000\n",
            "Epoch 29: val_loss improved from 1.70420 to 1.60232, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 34s 3s/step - loss: 2.0897 - accuracy: 0.4714 - top_k_categorical_accuracy: 0.8464 - precision: 0.8083 - recall: 0.1295 - false_positives: 23.0000 - val_loss: 1.6023 - val_accuracy: 0.4688 - val_top_k_categorical_accuracy: 0.8438 - val_precision: 0.8621 - val_recall: 0.2604 - val_false_positives: 4.0000 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8448 - accuracy: 0.5234 - top_k_categorical_accuracy: 0.8802 - precision: 0.8899 - recall: 0.1292 - false_positives: 12.0000\n",
            "Epoch 30: val_loss did not improve from 1.60232\n",
            "12/12 [==============================] - 11s 912ms/step - loss: 1.8448 - accuracy: 0.5234 - top_k_categorical_accuracy: 0.8802 - precision: 0.8899 - recall: 0.1292 - false_positives: 12.0000 - val_loss: 1.6665 - val_accuracy: 0.4583 - val_top_k_categorical_accuracy: 0.8750 - val_precision: 0.8148 - val_recall: 0.2292 - val_false_positives: 5.0000 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0719 - accuracy: 0.4583 - top_k_categorical_accuracy: 0.7865 - precision: 0.8962 - recall: 0.1272 - false_positives: 11.0000\n",
            "Epoch 31: val_loss improved from 1.60232 to 1.56626, saving model to /content/drive/MyDrive/uploaded_datasets/pill-recognition/all-results/multi-results-pp/ResNet50_Multi/ResNet50_Multi_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 35s 3s/step - loss: 2.0719 - accuracy: 0.4583 - top_k_categorical_accuracy: 0.7865 - precision: 0.8962 - recall: 0.1272 - false_positives: 11.0000 - val_loss: 1.5663 - val_accuracy: 0.5104 - val_top_k_categorical_accuracy: 0.8854 - val_precision: 0.7586 - val_recall: 0.2292 - val_false_positives: 7.0000 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.8736 - accuracy: 0.5000 - top_k_categorical_accuracy: 0.8620 - precision: 0.8889 - recall: 0.1283 - false_positives: 12.0000\n",
            "Epoch 32: val_loss did not improve from 1.56626\n",
            "12/12 [==============================] - 10s 856ms/step - loss: 1.8736 - accuracy: 0.5000 - top_k_categorical_accuracy: 0.8620 - precision: 0.8889 - recall: 0.1283 - false_positives: 12.0000 - val_loss: 1.6277 - val_accuracy: 0.4688 - val_top_k_categorical_accuracy: 0.8646 - val_precision: 0.7647 - val_recall: 0.2708 - val_false_positives: 8.0000 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0098 - accuracy: 0.4844 - top_k_categorical_accuracy: 0.8385 - precision: 0.8689 - recall: 0.1429 - false_positives: 16.0000\n",
            "Epoch 33: val_loss did not improve from 1.56626\n",
            "12/12 [==============================] - 10s 841ms/step - loss: 2.0098 - accuracy: 0.4844 - top_k_categorical_accuracy: 0.8385 - precision: 0.8689 - recall: 0.1429 - false_positives: 16.0000 - val_loss: 1.5776 - val_accuracy: 0.4688 - val_top_k_categorical_accuracy: 0.8646 - val_precision: 0.7667 - val_recall: 0.2396 - val_false_positives: 7.0000 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 2.0769 - accuracy: 0.4818 - top_k_categorical_accuracy: 0.8490 - precision: 0.7615 - recall: 0.1318 - false_positives: 31.0000\n",
            "Epoch 34: val_loss did not improve from 1.56626\n",
            "12/12 [==============================] - 10s 870ms/step - loss: 2.0769 - accuracy: 0.4818 - top_k_categorical_accuracy: 0.8490 - precision: 0.7615 - recall: 0.1318 - false_positives: 31.0000 - val_loss: 1.6605 - val_accuracy: 0.4375 - val_top_k_categorical_accuracy: 0.8646 - val_precision: 0.7188 - val_recall: 0.2396 - val_false_positives: 9.0000 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.9846 - accuracy: 0.5312 - top_k_categorical_accuracy: 0.8724 - precision: 0.8496 - recall: 0.1289 - false_positives: 17.0000\n",
            "Epoch 35: val_loss did not improve from 1.56626\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "12/12 [==============================] - 10s 874ms/step - loss: 1.9846 - accuracy: 0.5312 - top_k_categorical_accuracy: 0.8724 - precision: 0.8496 - recall: 0.1289 - false_positives: 17.0000 - val_loss: 1.5951 - val_accuracy: 0.4375 - val_top_k_categorical_accuracy: 0.8646 - val_precision: 0.7857 - val_recall: 0.2292 - val_false_positives: 6.0000 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curves"
      ],
      "metadata": {
        "id": "vTKEPCBW7gsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %tensorboard --logdir /content/drive/MyDrive/uploaded_datasets/pill-classification/all-results/multi-tb-pp/MobileNet_Multi/MobileNet_Multi_bs_16\n",
        "\n",
        "# %tensorboard --logdir /content/drive/MyDrive/uploaded_datasets/pill-classification/all-results/multi-tb-pp/ResNet50_Multi/ResNet50_Multi_bs_16"
      ],
      "metadata": {
        "id": "cFS7LvUX7gFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on Test Set"
      ],
      "metadata": {
        "id": "KnwviYICNy6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator for Test Set"
      ],
      "metadata": {
        "id": "FFZUv4KuRPdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_test_datagen = CustomDataGenerator(image_filepaths=list(X_test_dict.keys()),\n",
        "                                         feature_vectors=X_test_dict,\n",
        "                                         labels=y_test_dict,\n",
        "                                         is_train=False, is_augment=False, is_shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOE5zFegRKJj",
        "outputId": "c2c78e24-2570-4e16-8353-f8241efdbe51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 56 images belonging to 40 classes\n",
            "Found 56 feature vectors belonging to 40 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics = mobilenet_multi_1.metrics_names\n",
        "print(evaluation_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBx696PBSfCh",
        "outputId": "9d161a4f-c2db-4b9a-f780-7cdc6d3c640a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss', 'accuracy', 'top_k_categorical_accuracy', 'precision', 'recall', 'false_positives']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_eval_results = mobilenet_multi_1.evaluate(custom_test_datagen, return_dict=True, verbose=1)\n",
        "resnet50_eval_results = resnet50_multi_1.evaluate(custom_test_datagen, return_dict=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nodI33AHOPej",
        "outputId": "1377d7c2-964e-4415-8ab6-e1f6efb3dbef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 202ms/step - loss: 2.2270 - accuracy: 0.2812 - top_k_categorical_accuracy: 0.7500 - precision: 0.6250 - recall: 0.1562 - false_positives: 3.0000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 1.7049 - accuracy: 0.4688 - top_k_categorical_accuracy: 0.8125 - precision: 0.8571 - recall: 0.1875 - false_positives: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_results(model_names, result_array):\n",
        "  print(\"Model Performance\\n\")\n",
        "\n",
        "  for model_name, results in zip(model_names, result_array):\n",
        "    print(\"======================== {} ========================\".format(model_name))\n",
        "    print(pd.DataFrame(results.items(), columns=['metric', 'score']), \"\\n\\n\")"
      ],
      "metadata": {
        "id": "Kox7rDPySl1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_results(['MobileNet', 'ResNet50'], [mobilenet_eval_results, resnet50_eval_results])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XSapcrRUmNd",
        "outputId": "795ba5d9-fe16-4614-cfd6-52dcecb0c7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance\n",
            "\n",
            "======================== MobileNet ========================\n",
            "                       metric     score\n",
            "0                        loss  2.226971\n",
            "1                    accuracy  0.281250\n",
            "2  top_k_categorical_accuracy  0.750000\n",
            "3                   precision  0.625000\n",
            "4                      recall  0.156250\n",
            "5             false_positives  3.000000 \n",
            "\n",
            "\n",
            "======================== ResNet50 ========================\n",
            "                       metric     score\n",
            "0                        loss  1.704935\n",
            "1                    accuracy  0.468750\n",
            "2  top_k_categorical_accuracy  0.812500\n",
            "3                   precision  0.857143\n",
            "4                      recall  0.187500\n",
            "5             false_positives  1.000000 \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}